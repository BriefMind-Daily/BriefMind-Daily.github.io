标题,中文标题,领域分类,研究机构,PDF链接,论文链接,简明摘要,Upvote数
"Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",观看、推理与搜索：面向Agentic视频推理的开放网络视频深度研究基准,Agent,Other,https://arxiv.org/pdf/2601.06943,https://huggingface.co/papers/2601.06943,本文提出了首个面向开放网络环境的视频深度研究基准VideoDR，针对视频问答任务设计，要求模型从视频中提取跨帧线索，结合网络检索进行多步推理验证。通过严格人工标注，VideoDR涵盖六大语义领域，提供高质量样本。实验评估了多种多模态大模型，发现基于Agentic的推理方式并非始终优于传统流程，模型在长链检索中保持视频线索的能力是关键瓶颈。该基准系统揭示了视频推理中目标偏移和长程一致性等核心挑战，为下一代视频智能代理研究提供了重要参考。,172
DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI,DataFlow：面向数据中心AI时代的统一数据准备与工作流自动化的LLM驱动框架,LLM,"PKU, Shanghai AI Lab",https://arxiv.org/pdf/2512.16676,https://huggingface.co/papers/2512.16676,本文提出了DataFlow，一种基于大型语言模型驱动的统一数据准备框架，旨在解决当前数据处理流程中缺乏规范性和复用性的问题。DataFlow通过模块化设计和丰富的操作算子，支持多领域数据转换，提供易用的流水线构建接口，并能自动将自然语言需求转化为可执行流程。实验证明，DataFlow在数学推理、代码生成、文本理解等多个任务中显著提升了模型性能，且生成的数据集能以更少样本超越传统大规模训练数据，展示了其在高效、可靠和可复现的数据驱动AI开发中的重要价值。,157
STEP3-VL-10B Technical Report,STEP3-VL-10B 技术报告,Multimodal LLM,Other,https://arxiv.org/pdf/2601.09668,https://huggingface.co/papers/2601.09668,本文介绍了STEP3-VL-10B，一款轻量级开源多模态基础模型，通过统一预训练和强化学习后训练，实现了视觉与语言的深度融合。该模型引入了并行协调推理技术，有效提升了推理效率和性能。尽管参数规模仅为100亿，STEP3-VL-10B在多项复杂视觉语言任务中表现优异，超越了许多参数规模远大的先进模型，展现出高效且强大的多模态智能能力。论文发布了完整模型，推动社区在多模态领域的研究和应用。,155
BabyVision: Visual Reasoning Beyond Language,BabyVision：超越语言的视觉推理,Multimodal LLM,"PKU, Alibaba, THU",https://arxiv.org/pdf/2601.06521,https://huggingface.co/papers/2601.06521,本文提出了BabyVision基准，用于评估多模态大语言模型（MLLMs）在不依赖语言知识的基础视觉理解能力。研究发现，当前最先进的MLLMs在基本视觉任务上的表现远逊于人类儿童，甚至三岁儿童都能轻松完成。通过388个任务覆盖四大类别，实验结果显示模型在视觉推理方面存在显著不足。BabyVision为推动机器视觉理解向人类水平迈进提供了新的评测工具和方向，同时还提出了基于生成模型的解决方案及自动评估工具，促进相关研究的复现与发展。,147
Urban Socio-Semantic Segmentation with Vision-Language Reasoning,基于视觉-语言推理的城市社会语义分割,Multimodal LLM,Alibaba,https://arxiv.org/pdf/2601.10477,https://huggingface.co/papers/2601.10477,本文提出了一种结合视觉与语言推理的新方法，实现了对城市卫星影像中社会语义实体（如学校、公园等）的精确分割。为此，作者构建了包含卫星图像、数字地图及分层标注的SocioSeg数据集，并设计了模拟人类识别过程的SocioReasoner框架，通过跨模态识别和多阶段推理提升分割效果。利用强化学习优化非可微过程，显著优于现有模型并具备良好的零样本泛化能力。该研究为城市社会语义分析提供了有效工具，推动了相关应用的发展。,144
GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization,GDPO：面向多奖励强化学习优化的组奖励解耦归一化策略优化,Agent,Other,https://arxiv.org/pdf/2601.05242,https://huggingface.co/papers/2601.05242,本文针对多重奖励强化学习中常用的GRPO方法存在的奖励归一化塌陷问题，提出了GDPO方法，通过对各个奖励单独归一化，避免了奖励信号的混淆和训练信号分辨率下降。GDPO显著提升了训练的稳定性和多奖励优化的效果。在工具调用、数学推理和代码推理三项任务中，GDPO在准确性和约束遵守等指标上均优于GRPO，展示了其在多奖励强化学习中的广泛适用性和优越性能。,137
Kling-Omni Technical Report,Kling-Omni 技术报告,Multimodal LLM,Other,https://arxiv.org/pdf/2512.16776,https://huggingface.co/papers/2512.16776,本文介绍了Kling-Omni，一种通用的视频生成框架，能够基于文本、图片和视频等多模态输入，直接合成高质量视频。该系统融合了视频生成、编辑和智能推理功能，打破了传统分离的处理流程，实现了统一的多模态表示和高效的内容创作。通过构建完善的数据体系和采用大规模预训练策略，Kling-Omni在多任务场景下表现出色，展现了强大的生成与编辑能力。该框架不仅提升了视频内容创作的智能化水平，也推动了多模态智能系统的发展。,136
Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization,基于地图思维的强化并行地图增强智能体用于图像地理定位,Agent,Alibaba,https://arxiv.org/pdf/2601.05432,https://huggingface.co/papers/2601.05432,本文提出了一种基于地图辅助的图像地理定位方法，通过将地图信息融入大规模视觉语言模型，模拟人类利用地图推理的策略。该方法设计了两阶段优化流程：先通过强化学习提升模型的决策能力，再采用并行推理探索多条路径以提高定位准确率。为验证效果，作者构建了包含真实场景图像的地理定位基准MAPBench。实验表明，该方法在多个指标上显著优于现有公开和闭源模型，定位精度提升明显，展示了地图辅助推理在图像地理定位中的潜力和优势。,131
Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs,奖励稀有策略：面向大语言模型创造性问题解决的唯一性感知强化学习,LLM,Other,https://arxiv.org/pdf/2601.08763,https://huggingface.co/papers/2601.08763,本文提出了一种面向大语言模型的强化学习新方法——“独特性感知强化学习”，通过对同一问题的不同解答策略进行聚类，并对罕见且正确的高层次推理策略给予更高奖励，促进模型探索更多多样化的解决方案。该方法有效缓解了传统强化学习中模型过早集中于少数主流策略的问题，提升了多样性和整体表现，在数学、物理和医学推理任务中均取得了更优的多样性和准确率表现，推动了大语言模型在复杂问题解决中的创新能力。,122
Controlled Self-Evolution for Algorithmic Code Optimization,用于算法代码优化的受控自我进化方法,LLM,PKU,https://arxiv.org/pdf/2601.07348,https://huggingface.co/papers/2601.07348,本文提出了一种名为“受控自我进化”（CSE）的方法，用于提升算法代码生成的效果。该方法通过多样化初始化、基于反馈的遗传进化和分层记忆机制，有效扩大了解空间，避免陷入低质量解，提升了探索效率和解的质量。实验证明，CSE在多个大型语言模型基础上均优于现有方法，且从早期迭代即展现出更高效率，持续改进性能。该研究为自动代码优化提供了更高效、稳健的解决方案。,97
Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance,Wan-Move：通过潜在轨迹引导实现运动可控的视频生成,Diffusion Model,"Alibaba, THU",https://arxiv.org/pdf/2512.08765,https://huggingface.co/papers/2512.08765,本文提出了Wan-Move，一种简洁且易扩展的视频生成框架，实现了对视频中运动的精细控制。该方法通过将物体的密集运动轨迹映射到潜在空间，并沿轨迹传播首帧特征，生成对运动变化敏感的特征图，作为视频生成的条件输入，无需修改模型结构或额外编码器。Wan-Move支持高质量、细粒度的运动引导，生成的视频在运动控制效果上接近商业产品。为全面评估，作者还构建了包含多样内容和高质量标注的MoveBench基准，实验结果表明该方法在运动质量上具有显著优势。,94
MMFormalizer: Multimodal Autoformalization in the Wild,MMFormalizer：野外多模态自动形式化,Multimodal LLM,Other,https://arxiv.org/pdf/2601.03017,https://huggingface.co/papers/2601.03017,本文提出了MMFormalizer，一种结合视觉感知与形式化数学推理的多模态自动形式化方法，能够将自然语言和视觉信息中的物理与数学知识转化为形式化表达。该方法通过递归地将感知到的基础元素与公理组合，确保每个抽象均有视觉证据支持，并在经典力学、相对论、量子力学等复杂物理领域表现出色。通过新构建的多模态基准测试，MMFormalizer展示了其在统一多模态形式化推理上的有效性，推动了机器理解和推理物理世界的能力。,94
DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation,DeepResearchEval：一个用于深度研究任务构建与智能体评估的自动化框架,Agent,Other,https://arxiv.org/pdf/2601.09688,https://huggingface.co/papers/2601.09688,本文提出了DeepResearchEval，一个自动化框架，用于构建复杂的深度研究任务并进行智能化评估。该框架通过基于用户角色生成真实多样的研究任务，并筛选出需要多源信息整合的任务，实现任务构建自动化。在评估方面，DeepResearchEval设计了动态调整评价标准的机制，同时具备主动事实核查能力，能够在缺乏引用的情况下通过网络搜索验证报告内容的真实性。该方法有效解决了现有评测依赖人工标注和固定指标、难以准确验证事实的问题，推动了深度研究系统的性能评估与发展。,92
Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization,Youtu-Agent：通过自动生成与混合策略优化提升智能体生产力,Agent,Other,https://arxiv.org/pdf/2512.24615,https://huggingface.co/papers/2512.24615,本文提出了Youtu-Agent，一种模块化框架，旨在自动生成并持续优化大型语言模型代理。该框架通过结构化配置系统实现工具和环境的灵活复用，支持标准任务和复杂需求的两种生成模式。其混合策略优化包括基于上下文的经验积累和结合分布式训练的强化学习，提升代理适应性和性能。实验表明，Youtu-Agent在多个问答和推理任务中取得领先表现，显著降低了配置成本并加速了训练过程，展示了提升智能代理生产力的有效路径。,88
NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos,NeoVerse：利用野外单目视频增强4D世界模型,Other,Other,https://arxiv.org/pdf/2601.00393,https://huggingface.co/papers/2601.00393,本文提出了NeoVerse，一种能够从单目视频中高效构建四维（4D）世界模型的方法。NeoVerse克服了现有4D建模方法对多视角数据和复杂训练流程的依赖，实现了无需位姿信息的快速前向重建和在线单目退化模拟。该方法不仅具备良好的通用性和适应性，还在标准重建与视频生成任务中取得了领先性能，推动了4D世界模型在多样化实际场景中的应用。,85
Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling,基于超图记忆的多步RAG改进方法用于长上下文复杂关系建模,LLM,Tencent,https://arxiv.org/pdf/2512.23959,https://huggingface.co/papers/2512.23959,本文提出了一种基于超图的动态记忆机制HGMem，用于提升多步检索增强生成（RAG）模型在处理长文本复杂推理任务中的表现。与传统被动存储孤立事实的记忆不同，HGMem通过超图结构捕捉事实间的高阶关联，构建连贯的知识网络，增强了模型的全局理解和多步推理能力。实验结果表明，该方法在多个复杂数据集上显著优于现有强基线，展示了其在长上下文复杂关系建模中的有效性和广泛适用性。,83
Adaptation of Agentic AI,Agentic AI的适应性研究,Agent,Other,https://arxiv.org/pdf/2512.16301,https://huggingface.co/papers/2512.16301,本文提出了一个系统框架，统一了智能代理系统中“代理模型”和“辅助工具”的适应方法，明确了不同适应策略的分类及其设计权衡。通过将适应过程细分为基于工具执行信号和代理输出信号的代理适应，以及不依赖或依赖代理监督的工具适应，作者梳理了当前研究进展，分析了各类方法的优势与局限，并指出未来的挑战与机遇。该框架为构建更高效、可靠且具备复杂任务处理能力的智能代理系统提供了理论基础和实践指导。,81
MAXS: Meta-Adaptive Exploration with LLM Agents,MAXS：基于大语言模型智能体的元自适应探索,Agent,Other,https://arxiv.org/pdf/2601.09259,https://huggingface.co/papers/2601.09259,本文提出了MAXS，一种基于大语言模型（LLM）代理的元自适应推理框架，旨在提升多工具协同推理的效果和效率。通过引入前瞻策略，MAXS能够提前评估工具调用的价值，避免局部短视的决策；同时设计轨迹收敛机制，及时终止推理路径，降低计算成本。实验证明，MAXS在多种模型和数据集上均优于现有方法，显著提升了推理性能和资源利用效率，展示了其在复杂任务中灵活高效协同多工具推理的潜力。,81
MMGR: Multi-Modal Generative Reasoning,MMGR：多模态生成推理,Multimodal LLM,Microsoft,https://arxiv.org/pdf/2512.14691,https://huggingface.co/papers/2512.14691,本文提出了MMGR，一种多模态生成推理的评估框架，涵盖物理、逻辑、空间和时间五大推理能力，针对抽象推理、具身导航和物理常识三个领域，对视频与图像生成模型进行全面测试。实验结果显示，现有模型在物理常识任务上表现尚可，但在抽象推理和长程空间规划上存在显著不足。研究揭示了当前模型过度依赖感知数据、缺乏全局一致性及因优化目标偏重视觉效果而忽视因果正确性的问题。MMGR为推动具备推理能力的生成模型发展提供了统一的诊断工具和明确方向。,79
Step-GUI Technical Report,Step-GUI技术报告,Agent,Other,https://arxiv.org/pdf/2512.15431,https://huggingface.co/papers/2512.15431,本文提出了一种自我进化的训练流程，通过校准奖励系统高效生成高质量标注数据，显著降低训练成本并提升准确率。基于此，开发了Step-GUI系列模型，在多项GUI自动化任务中表现优异，达到行业领先水平。为适应多设备环境并保障用户隐私，论文设计了GUI-MCP协议，实现任务分层处理和本地数据保护。最后，结合真实使用场景构建了AndroidDaily基准，验证模型在日常操作中的实用性。该研究推动了实用GUI智能代理的发展，具备广泛应用潜力。,78
Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows,利用科学家对齐工作流程探测大语言模型的科学通用智能,AI4Science,Shanghai AI Lab,https://arxiv.org/pdf/2512.16969,https://huggingface.co/papers/2512.16969,本文提出了科学通用智能（SGI）的操作性定义，基于实用探究模型，将科学探索过程划分为深度研究、创意生成、实验执行和实验推理四个关键任务。作者构建了包含1000多个跨学科样本的SGI-Bench基准，用以系统评估大型语言模型在科学问题上的表现，揭示了现有模型在细节准确性、实验结果和多模态推理等方面的不足。论文还引入了一种测试时强化学习方法，提升了模型生成新颖假设的能力。该工作为开发能够真正参与科学发现的AI系统奠定了基础。,78
SemanticGen: Video Generation in Semantic Space,SemanticGen：语义空间中的视频生成,Diffusion Model,Other,https://arxiv.org/pdf/2512.20619,https://huggingface.co/papers/2512.20619,本文提出了SemanticGen，一种基于语义空间的视频生成方法，通过两阶段扩散模型先生成紧凑的语义特征，再生成视频潜变量，从而实现视频的全局布局规划和细节补充。该方法相比传统直接在潜变量空间建模，收敛更快且计算效率更高，尤其适合长视频生成。实验结果显示，SemanticGen在生成质量和效率上均优于现有先进方法，展示了在高质量长视频生成领域的显著优势。,77
T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground,T-pro 2.0：一种高效的俄语混合推理模型及其应用平台,LLM,Other,https://arxiv.org/pdf/2512.10430,https://huggingface.co/papers/2512.10430,本文介绍了T-pro 2.0，一款开源的俄语大型语言模型，专为混合推理和高效推断设计。该模型采用了针对西里尔字母优化的分词器和改进的推断加速技术，支持直接回答和推理过程展示。作者同时发布了模型权重、包含约50万条指令的训练数据集及推理评测基准，方便社区复现和扩展研究。此外，公开的在线演示平台展示了模型在不同模式下的性能和速度提升。T-pro 2.0为俄语语言理解与推理提供了实用且开放的工具，推动相关领域的发展。,75
ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding,ReFusion：一种具有并行自回归解码的扩散大语言模型,Diffusion Model,Other,https://arxiv.org/pdf/2512.13586,https://huggingface.co/papers/2512.13586,本文提出了ReFusion，一种创新的掩码扩散模型，通过将并行解码提升到固定长度的“槽”级别，实现了更高效且性能优越的文本生成。该方法结合了基于扩散的规划与自回归填充，既支持键值缓存以降低计算开销，又简化了学习复杂度。实验证明，ReFusion在多个基准测试上较传统掩码扩散模型性能提升34%，速度提升超过18倍，同时在保持2.3倍加速的情况下，性能接近甚至超越强大的自回归模型，显著提升了生成模型的效率和效果。,74
InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion,InsertAnywhere：融合4D场景几何与扩散模型实现真实感视频对象插入,Diffusion Model,Other,https://arxiv.org/pdf/2512.17504,https://huggingface.co/papers/2512.17504,本文提出了InsertAnywhere框架，实现了在视频中自然且逼真地插入新对象。该方法通过结合四维场景几何理解与基于扩散模型的视频生成，确保插入对象在空间位置、遮挡关系和光照效果上的一致性与连贯性。为支持监督训练，作者构建了包含光照信息的合成数据集ROSE++。大量实验表明，InsertAnywhere在多样的真实场景中显著优于现有方法，具备商业广告和影视后期制作的应用潜力。,74
A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation,A^3-Bench：基于锚点与吸引子激活的记忆驱动科学推理基准测试,AI4Science,Other,https://arxiv.org/pdf/2601.09274,https://huggingface.co/papers/2601.09274,本文提出了A³-Bench，一个专注于记忆驱动科学推理能力评估的新基准。该基准通过标注涵盖多个领域的2198个科学推理问题，构建了基于“锚点”和“吸引子”激活的双尺度记忆评估框架，并设计了评测记忆激活率的指标AAUI。实验验证表明，记忆激活机制显著提升了模型的推理表现，揭示了记忆在科学推理中的关键作用。该工作为深入理解和提升人工智能系统的科学推理能力提供了新的视角和工具。,74
InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields,InfiniDepth：基于神经隐式场的任意分辨率与细粒度深度估计,Other,Other,https://arxiv.org/pdf/2601.03252,https://huggingface.co/papers/2601.03252,本文提出了InfiniDepth，一种基于神经隐式场的新型深度表示方法。通过局部隐式解码器，InfiniDepth能够在连续二维坐标上查询深度，实现任意分辨率和细节丰富的深度估计。该方法突破了传统基于离散图像网格的限制，显著提升了细节恢复能力。作者还构建了包含多样场景和丰富细节的高质量4K合成数据集，实验证明InfiniDepth在合成和真实数据上的深度估计任务中均表现优异，尤其在细节区域效果突出。此外，该方法在大视角变化的视点合成任务中也表现出更少的孔洞和伪影，展现了广泛应用潜力。,73
Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss,通过辅助损失实现Mixture-of-Experts中专家与路由器的耦合,LLM,ByteDance,https://arxiv.org/pdf/2512.23447,https://huggingface.co/papers/2512.23447,本文提出了一种名为专家-路由器耦合（ERC）损失的辅助方法，用于改进混合专家模型中路由器与专家能力的一致性。该方法通过引入代理标记和对内部激活的约束，使路由决策更准确地反映专家的特长，提升模型性能和计算效率。ERC损失计算成本固定且低于传统方法，适用于大规模语言模型的预训练。实验表明，ERC不仅有效增强了专家的专业化，还提供了训练过程中专家能力的可量化监控，为混合专家模型的优化提供了新思路。,72
mHC: Manifold-Constrained Hyper-Connections,mHC：流形约束超连接,Other,Other,https://arxiv.org/pdf/2512.24880,https://huggingface.co/papers/2512.24880,本文提出了一种名为Manifold-Constrained Hyper-Connections（mHC）的新型残差连接框架，旨在解决现有超连接方法中训练不稳定、扩展受限及内存开销大的问题。mHC通过将残差连接映射到特定流形上，恢复了关键的恒等映射特性，同时结合基础设施优化提升计算效率。实验证明，mHC在大规模训练中表现出更好的稳定性和性能提升，展现出优越的扩展能力。该方法为深度网络结构设计提供了新的思路，有助于推动基础模型的发展。,72
Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning,协作多智能体测试时强化学习用于推理,Agent,Microsoft,https://arxiv.org/pdf/2601.09667,https://huggingface.co/papers/2601.09667,本文提出了一种名为多智能体测试时强化学习（MATTRL）的新框架，旨在提升多智能体系统在推理任务中的表现。该方法通过在推理阶段注入结构化的文本经验，促进多专家团队的多轮讨论与共识决策，有效解决了传统多智能体强化学习训练资源消耗大且不稳定的问题。实验证明，MATTRL在医学、数学和教育等领域的多个基准测试中显著提升了准确率，且无需额外调优。该研究为实现稳定、高效且具鲁棒性的多智能体协同推理提供了新的思路。,71
Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding,Mindscape-Aware检索增强生成用于改进长上下文理解,LLM,Tencent,https://arxiv.org/pdf/2512.17220,https://huggingface.co/papers/2512.17220,本文提出了Mindscape-Aware Retrieval-Augmented Generation（MiA-RAG）方法，通过构建层级摘要形成整体语义视图，赋予基于大语言模型的检索增强生成系统全局上下文感知能力。该方法使检索和生成过程均依赖于统一的全局语义表示，从而提升了对长文本的理解和基于证据的推理能力。实验结果显示，MiA-RAG在多种长文本和双语任务中均优于现有方法，实现了更符合人类思维方式的长上下文信息检索与推理。,69
Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform,Visionary：基于WebGPU驱动的高斯点渲染平台构建的世界模型承载器,Other,Shanghai AI Lab,https://arxiv.org/pdf/2512.08478,https://huggingface.co/papers/2512.08478,本文提出了Visionary，一种基于WebGPU的开放式网页平台，实现了高效的3D高斯点渲染和网格显示。该平台集成了实时神经网络推理，支持动态内容生成和多种3D高斯点变体，提供轻量级、即点即用的浏览器体验。通过统一推理与渲染流程，Visionary显著提升了渲染效率，简化了3D世界模型的部署与应用，促进了重建和生成模型的广泛使用。该平台兼容现有网页应用，降低了技术门槛，推动了3D神经渲染技术的普及和发展。,66
Towards Scalable Pre-training of Visual Tokenizers for Generation,面向生成的视觉标记器可扩展预训练方法,Other,Other,https://arxiv.org/pdf/2512.13687,https://huggingface.co/papers/2512.13687,本文提出了一种统一的视觉标记器预训练框架（VTP），通过联合优化图像-文本对比、自监督和重建损失，解决了传统重建训练偏向低层信息、难以提升生成质量的问题。研究表明，高层语义理解是提升生成效果的关键，且该方法在大规模预训练下具备良好的扩展性，显著提升了生成模型的零样本准确率和收敛速度。相比传统方法，VTP在不改变生成模型训练配置的情况下，通过增加预训练计算资源，实现了生成质量的大幅提升，展示了视觉标记器预训练的新方向和潜力。,66
PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence,PhysBrain：以人体自我中心数据为桥梁，连接视觉语言模型与物理智能,Embodied AI,Other,https://arxiv.org/pdf/2512.16793,https://huggingface.co/papers/2512.16793,本文提出了PhysBrain，一种利用大规模人类第一视角视频数据，桥接视觉语言模型与机器人物理智能的方法。通过Egocentric2Embodiment转换流程，将原始第一视角视频转化为结构化、多层次的训练数据，构建了规模化的E2E-3M数据集。基于该数据训练的PhysBrain显著提升了机器人对自身视角下环境的理解和长远规划能力，实现了更高效的模型微调和任务成功率，展示了人类第一视角监督对机器人控制的有效迁移潜力。,64
Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting,熵自适应微调：解决自信冲突以缓解遗忘,LLM,Other,https://arxiv.org/pdf/2601.02151,https://huggingface.co/papers/2601.02151,本文针对监督微调中常见的灾难性遗忘问题，提出了一种基于令牌熵的自适应微调方法（EAFT）。该方法通过利用令牌级别的熵值区分模型的不确定性与知识冲突，有效避免了因模型自信预测与外部监督标签不一致而导致的破坏性更新。实验表明，EAFT在多个大规模语言模型和不同领域任务中，既保持了下游任务的性能，又显著减缓了模型泛化能力的退化，提升了微调的稳定性和效果。,64
Memory in the Age of AI Agents,AI智能体时代的记忆研究综述,Agent,PKU,https://arxiv.org/pdf/2512.13564,https://huggingface.co/papers/2512.13564,本文系统梳理了人工智能代理中的记忆研究，明确界定了记忆的范围并区分了相关概念。作者从记忆的形式、功能和动态三个角度出发，提出了更细致的分类方法，涵盖了多种记忆类型及其演变过程。文章还总结了现有的评测基准和开源工具，揭示了该领域的分散现状，并展望了自动化设计、多模态记忆和多智能体记忆等未来研究方向。该综述为理解和设计智能代理的记忆机制提供了全面的理论基础和实践指导。,63
PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning,PaCoRe：通过并行协调推理学习扩展测试时计算能力,LLM,"THU, PKU",https://arxiv.org/pdf/2601.05593,https://huggingface.co/papers/2601.05593,本文提出了Parallel Coordinated Reasoning（PaCoRe）框架，通过多轮并行推理和信息协调机制，突破了传统语言模型在固定上下文窗口内测试时计算能力的限制。PaCoRe利用大规模并行探索和消息传递，有效整合推理结果，实现了千万级别的测试时计算扩展而不超出上下文限制。该方法在多个领域表现优异，尤其在数学推理任务中，8亿参数模型以94.5%的成绩超越了GPT-5，展示了显著的性能提升。论文还开源了相关模型和数据，促进后续研究发展。,62
Next-Embedding Prediction Makes Strong Vision Learners,Next-Embedding Prediction 使视觉学习者更强大,Other,Other,https://arxiv.org/pdf/2512.16922,https://huggingface.co/papers/2512.16922,本论文提出了一种基于“下一个嵌入预测”的视觉自监督学习方法，借鉴了自然语言处理中生成式预训练的成功经验。该方法通过训练模型预测图像中未来的局部特征嵌入，有效避免了传统方法中对像素重建或对比损失的依赖，保持了模型结构的简洁性和可扩展性。实验表明，该方法在ImageNet图像分类和ADE20K语义分割任务中均取得了优异表现，展示了其作为一种简单且通用的视觉预训练手段的潜力。,59
MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences,MemGovern：通过学习受控的人类经验提升代码智能体能力,Agent,PKU,https://arxiv.org/pdf/2601.06789,https://huggingface.co/papers/2601.06789,本文提出了MemGovern框架，旨在将GitHub上分散且无结构的历史软件开发经验转化为结构化的“经验卡”，供自动化软件工程代理使用。通过引入经验治理和基于逻辑的检索策略，MemGovern显著提升了代理利用人类经验解决软件缺陷的能力。在生成13.5万条经验卡后，系统在SWE-bench测试中解决率提升了4.65%。作为一种插件式方案，MemGovern为构建面向代码代理的记忆基础设施提供了有效路径，推动了自动化软件开发的智能化进程。,59
LLaDA2.0: Scaling Up Diffusion Language Models to 100B,LLaDA2.0：将扩散语言模型规模扩大至1000亿参数,LLM,Other,https://arxiv.org/pdf/2512.15745,https://huggingface.co/papers/2512.15745,本文提出了LLaDA2.0，一种通过将预训练的自回归语言模型系统性转换为离散扩散模型的方法，实现了参数规模高达1000亿的大型语言模型。该方法采用分阶段的块级训练策略，兼顾了知识传承与训练效率，避免了从零训练的高昂成本。基于此，作者开发了两款指令调优的专家混合模型，兼具高效的并行解码能力和优异的性能表现，适合实际部署。相关模型已开源，推动了大规模扩散语言模型的应用与研究。,58
QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management,QwenLong-L1.5：面向长上下文推理与记忆管理的后训练方案,LLM,Alibaba,https://arxiv.org/pdf/2512.12967,https://huggingface.co/papers/2512.12967,本文提出了QwenLong-L1.5，一种通过系统化后训练方法显著提升长文本推理能力的模型。其核心创新包括：构建高质量的多跳推理训练数据；引入稳定的强化学习策略以应对长上下文训练的不稳定性；设计支持超长文本处理的记忆增强架构，实现对超过400万字令牌的有效推理。基于Qwen3-30B模型，QwenLong-L1.5在多项长文本推理基准测试中表现优异，超越多个领先模型，且其长上下文能力在科学推理和多轮对话等实际应用中同样表现出色。,55
Evolving Programmatic Skill Networks,进化的程序化技能网络,Agent,Microsoft,https://arxiv.org/pdf/2601.03509,https://huggingface.co/papers/2601.03509,本文提出了程序化技能网络（PSN）框架，旨在支持智能体在开放环境中持续构建、优化和复用技能库。PSN将技能表示为可执行的符号程序，形成一个可演化的组合网络，通过三个机制实现持续改进：结构化的错误定位以精确分配责任，基于技能成熟度的渐进优化保障稳定性与灵活性，以及结构重构保持网络简洁。实验证明，PSN在复杂任务中表现出强大的技能复用能力、快速适应性和良好的泛化性能，展示了其在持续学习和技能进化方面的潜力。,54
Latent Implicit Visual Reasoning,潜隐隐式视觉推理,Multimodal LLM,Other,https://arxiv.org/pdf/2512.21218,https://huggingface.co/papers/2512.21218,本文提出了一种无需显式监督的视觉推理机制，旨在提升大型多模态模型在视觉推理任务中的表现。通过引入可自适应捕捉图像关键信息的隐含视觉推理标记，模型能够灵活地重编码视觉输入，避免了传统方法中对中间视觉步骤的人工设计和高昂标注成本。该方法在多种视觉任务上实现了领先性能，尤其适用于难以明确中间抽象的复杂任务，并具备良好的多任务泛化能力，显著推动了视觉推理能力的发展。,53
VIBE: Visual Instruction Based Editor,VIBE：基于视觉指令的编辑器,Multimodal LLM,Other,https://arxiv.org/pdf/2601.02242,https://huggingface.co/papers/2601.02242,本文提出了VIBE，一种基于视觉指令的紧凑型图像编辑系统。该系统结合了参数量较小的2B参数语言视觉模型和1.6B参数的扩散模型，实现了高质量且计算成本低的图像编辑。VIBE在保持输入图像一致性的同时，支持多种编辑任务，如属性调整、物体移除和背景替换。实验表明，该方法在多个公开基准上性能优于参数更多、计算更昂贵的模型，且能在普通GPU环境下快速生成高分辨率编辑图像，展现出良好的实用价值和推广潜力。,53
LongVie 2: Multimodal Controllable Ultra-Long Video World Model,LongVie 2：多模态可控超长视频世界模型,Multimodal LLM,"Shanghai AI Lab, THU",https://arxiv.org/pdf/2512.13604,https://huggingface.co/papers/2512.13604,本文提出了LongVie 2，一种端到端的自回归视频世界模型，能够生成时长达三至五分钟的超长视频。该模型通过三阶段训练策略提升了视频的可控性、视觉质量和时间一致性：融合多模态控制信号实现精细引导，采用退化感知训练缩小训练与推理差距，结合历史上下文保证连续片段间的连贯性。作者还构建了包含多样高分辨率视频的LongVGenBench基准，实验证明LongVie 2在长时视频生成的控制能力、画面连贯性和视觉效果方面均达到领先水平，推动了统一视频世界建模的发展。,52
The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding,棱镜假说：通过统一自编码实现语义与像素表示的协调,Other,Other,https://arxiv.org/pdf/2512.19693,https://huggingface.co/papers/2512.19693,本文提出了“棱镜假说”，揭示了语义编码器主要捕捉低频信息以表达抽象意义，而像素编码器则同时保留高频细节，从频谱视角统一理解两者的功能差异。基于此，作者设计了统一自编码模型（UAE），通过频段调制器有效融合语义结构与像素细节，实现了语义抽象与视觉保真度的协调统一。大量实验证明，UAE在多个图像理解任务中表现优异，展示了该方法在跨模态表示学习中的潜力和实用价值。,52
Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning,Native Parallel Reasoner：通过自蒸馏强化学习实现并行推理,Agent,Other,https://arxiv.org/pdf/2512.07461,https://huggingface.co/papers/2512.07461,本文提出了Native Parallel Reasoner（NPR），一种无需教师模型指导的训练框架，使大型语言模型具备真正的并行推理能力。通过自蒸馏的渐进式训练、并行感知策略优化算法以及改进的执行引擎，NPR实现了从传统的顺序推理向原生并行推理的转变。在多个推理任务中，NPR显著提升了推理准确率（最高提升24.5%）和推理速度（最高加速4.6倍），并首次实现了完全的并行执行，为高效、可扩展的智能推理树立了新标准。,51
TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times,TurboDiffusion：通过100-200倍加速视频扩散模型,Diffusion Model,THU,https://arxiv.org/pdf/2512.16093,https://huggingface.co/papers/2512.16093,本文提出了TurboDiffusion，一种显著加速视频生成的框架，能够在保持视频质量的同时，将生成速度提升100到200倍。该方法通过优化注意力计算、采用高效的步骤蒸馏技术以及将模型参数和激活量量化为8位，极大地提升了推理效率。实验在多种大型视频生成模型上验证，结果显示TurboDiffusion在单张RTX 5090 GPU上实现了显著的加速效果。此工作为高效视频生成提供了实用且有效的解决方案。,51
LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation,LiveTalk：通过改进的在线策略蒸馏实现实时多模态交互视频扩散,Diffusion Model,Other,https://arxiv.org/pdf/2512.23576,https://huggingface.co/papers/2512.23576,本文提出了一种改进的实时多模态视频扩散生成方法，通过优化条件输入质量和蒸馏训练策略，大幅降低推理延迟（约20倍），同时保持高质量视频输出。该方法支持文本、图像和音频的多模态交互，克服了传统扩散模型在实时生成中的视觉伪影和质量下降问题。基于此，作者构建了LiveTalk系统，实现了多轮对话中流畅且高质量的虚拟头像视频生成，显著优于现有技术，推动了人机多模态实时交互的发展。,51
Solar Open Technical Report,Solar Open技术报告,LLM,Other,https://arxiv.org/pdf/2601.07022,https://huggingface.co/papers/2601.07022,本文介绍了Solar Open，一款拥有1020亿参数的双语专家混合语言模型，专门针对资源匮乏语言进行设计。为解决数据不足问题，团队合成了大量高质量、领域特定的训练数据，并通过逐步优化的数据课程提升模型表现。此外，采用了一种高效的强化学习框架以增强模型推理能力。实验结果表明，Solar Open在英语和韩语等语言的多项基准测试中表现出色，展示了其在支持欠缺语言的人工智能发展中的潜力和有效性。,51
Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?,视频真实性测试：AI生成的ASMR视频能否欺骗视觉语言模型和人类？,Multimodal LLM,Other,https://arxiv.org/pdf/2512.13281,https://huggingface.co/papers/2512.13281,本文提出了Video Reality Test，一个基于ASMR视频的评测基准，用于检测AI生成的视频及其音频的真实性。通过设计创作者-评审对抗机制，评估了先进视频生成模型在产生细致音视频同步内容时欺骗人类和视觉语言模型（VLMs）的能力。实验结果表明，最优模型能显著迷惑VLMs，导致其识别准确率仅略高于随机水平，而人类专家识别准确率明显更高。研究揭示了当前视频生成技术在感知真实度和音视频一致性方面的局限，强调了发展更有效检测方法的必要性。,50
Yume-1.5: A Text-Controlled Interactive World Generation Model,Yume-1.5：一种文本控制的交互式世界生成模型,Diffusion Model,Shanghai AI Lab,https://arxiv.org/pdf/2512.22096,https://huggingface.co/papers/2512.22096,本文提出了Yume-1.5，一种基于文本或单张图像生成交互式虚拟世界的新框架。该方法通过整合上下文压缩与线性注意力机制，实现了长视频的高效生成；结合双向注意力蒸馏和改进的文本嵌入技术，提升了实时流式生成速度；并支持基于文本的事件控制，使用户可通过键盘实时探索和编辑持续变化的虚拟环境。该框架有效解决了参数规模大、推理耗时长和历史信息增长快等问题，显著增强了生成世界的交互性和实时性，推动了沉浸式虚拟内容的应用发展。,50
Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies,自底向上的策略优化：你的语言模型策略中隐藏的内部策略,LLM,Other,https://arxiv.org/pdf/2512.19673,https://huggingface.co/papers/2512.19673,本文提出了一种将大型语言模型的策略分解为内部层级和模块策略的新方法，揭示了模型在不同层次上的推理模式差异。通过分析内部策略的熵变化，发现早期层保持高探索性，顶层则趋于稳定，且不同模型展现出不同的收敛行为。基于此，作者设计了“自底向上策略优化”（BuPO）方法，直接优化早期层策略以重构基础推理能力。实验证明，BuPO在复杂推理任务中显著提升了模型性能，展示了对语言模型内部机制理解与利用的新方向。,49
Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning,自回归模型中涌现的时间抽象促进层次化强化学习,Agent,Other,https://arxiv.org/pdf/2512.20605,https://huggingface.co/papers/2512.20605,本文提出了一种在自回归模型内部学习和执行时间抽象动作的新方法，通过引入一个高阶非因果序列模型来控制基础模型的内部激活，从而实现行为的层次化表达。该方法在具有层次结构的任务中表现出能够压缩长时间序列并生成具有终止条件的行为控制器，显著提升了在稀疏奖励环境下的探索效率。实验结果表明，直接强化内部控制器（“内部强化学习”）在传统强化学习难以奏效的情况下仍能有效学习，展示了在基础模型中实现层次化强化学习的潜力。,49
WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling,WorldPlay：面向实时交互式世界建模的长期几何一致性,Diffusion Model,Tencent,https://arxiv.org/pdf/2512.14614,https://huggingface.co/papers/2512.14614,本文提出了WorldPlay，一种支持实时交互的流式视频生成模型，能够在用户操作下保持长时间的几何一致性。通过引入双重动作表示增强控制响应，重构上下文记忆以缓解记忆衰减，并采用上下文强制蒸馏方法保持模型对长距离信息的利用，WorldPlay有效解决了现有方法在速度与记忆之间的权衡问题。该模型能以24帧/秒生成720p高质量视频，并在多样场景中表现出良好的泛化能力，推动了实时动态三维环境建模的发展。,48
When Reasoning Meets Its Laws,当推理遇上其定律,LLM,Other,https://arxiv.org/pdf/2512.17901,https://huggingface.co/papers/2512.17901,本文提出了“推理定律”（LoRe）框架，旨在理论上定义大规模推理模型（LRMs）应具备的推理行为，重点关注计算资源与准确性的关系。作者提出计算定律，假设推理所需计算量应与问题复杂度线性相关，并引入准确性定律作为补充。为验证这些定律，设计了LoRe-Bench基准测试，系统评估推理模型的单调性和组合性两个关键属性。实验发现大多数模型虽具备合理的单调性，但缺乏组合性。基于此，论文提出一种有效的微调方法以增强组合性，显著提升模型推理性能。该研究为理解和优化大规模推理模型提供了理论基础和实用工具。,48
TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows,TwinFlow：利用自对抗流实现大模型的一步生成,Multimodal LLM,Other,https://arxiv.org/pdf/2512.05150,https://huggingface.co/papers/2512.05150,本文提出了TwinFlow，一种无需依赖预训练教师模型和传统对抗训练的单步生成框架，显著提升了大型多模态生成模型的推理效率。该方法在文本到图像生成任务中表现优异，1步推理即可达到与传统多步方法（需40-100步）相当的效果，且计算成本降低约100倍。TwinFlow具备良好的扩展性，成功应用于大规模模型Qwen-Image-20B，实现高效的少步生成，推动了大模型高效生成技术的发展。,47
Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits,LLM能否预测自身失败？通过内部电路实现自我意识,LLM,Other,https://arxiv.org/pdf/2512.20578,https://huggingface.co/papers/2512.20578,本文提出了一种名为Gnosis的轻量级机制，使大语言模型（LLMs）能够通过分析其推理过程中的内部状态，自主预测生成结果的正确性。该方法无需额外计算资源，仅增加少量参数，且适用于不同规模的冻结模型。实验表明，Gnosis在数学推理、问答及学术知识等多项任务中，准确性和校准效果均优于现有内部和外部评估手段，并能零样本泛化至部分生成阶段，实现对失败路径的早期识别和计算资源的智能调控。研究表明，模型生成过程本身蕴含可靠的正确性线索，可高效提取以提升模型自我认知能力。,47
Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows,Finch：基于电子表格的企业财务与会计工作流基准测试,Agent,Other,https://arxiv.org/pdf/2512.13168,https://huggingface.co/papers/2512.13168,本文提出了FINCH，一个针对企业财务与会计领域复杂工作流程的AI评测基准。该基准基于真实企业环境中的多模态数据（如电子表格、邮件、图表等），涵盖预算、交易和资产管理等多样化任务。通过结合大型语言模型辅助发现与专家精细标注，构建了包含172个复合工作流程和384个具体任务的数据集。实测多款先进AI系统在该基准上的表现有限，揭示了真实企业财务工作流程对AI智能化的巨大挑战。FINCH为推动AI在财务会计领域的应用与发展提供了重要工具和参考。,46
KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions,KnowMe-Bench：面向终身数字伴侣的人物理解基准测试,Agent,"THU, PKU",https://arxiv.org/pdf/2601.04745,https://huggingface.co/papers/2601.04745,本文提出了KnowMe-Bench，一个基于长篇自传叙事构建的公开基准，用于评估人工智能模型对个体长期理解的能力。该基准通过时间标注和情境细节重构叙事，设计涵盖事实回忆、主观状态推断及原则层面推理的问题，真实反映个体动机和决策原则。实验发现，现有基于检索的方法虽提升了事实准确性，但在时间关联解释和高层推理上仍存在不足，表明理解个体需要超越简单记忆检索的机制。该工作推动了面向终身数字伴侣的个性化理解研究。,46
Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning,面向卓越长链思维推理的分布对齐序列蒸馏,LLM,Alibaba,https://arxiv.org/pdf/2601.09088,https://huggingface.co/papers/2601.09088,本文提出了DASD-4B-Thinking，一款轻量且高效的开源推理模型，在数学、科学推理及代码生成等多个复杂任务上表现优异，超越了部分更大规模模型。作者重新审视了现有的教师-学生知识蒸馏方法，指出其在传递教师模型输出分布、适应学生模型能力和训练推理方式一致性方面存在不足。为此，论文设计了一套改进的序列级蒸馏训练流程，有效提升了学生模型的泛化能力。该方法显著减少了训练数据需求，并公开了模型与数据，促进社区研究发展。,46
StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation,StereoWorld：基于几何感知的单目到立体视频生成,Other,Other,https://arxiv.org/pdf/2512.09363,https://huggingface.co/papers/2512.09363,本文提出了StereoWorld，一种将单目视频转换为高质量立体视频的端到端框架。该方法利用预训练的视频生成模型，结合几何感知的正则化确保生成视频的三维结构一致性，并通过时空分块技术实现高效高分辨率合成。为支持大规模训练，作者构建了包含超过1100万帧、符合人眼自然瞳距的高清立体视频数据集。实验结果表明，StereoWorld在视觉效果和几何准确性上显著优于现有方法，能为XR设备提供更真实的立体视频内容。,45
Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models,[翻译]Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models,Other,Other,https://arxiv.org/pdf/2512.24618,https://huggingface.co/papers/2512.24618,本文提出了Youtu-LLM，一种轻量级但功能强大的语言模型，专为提升计算效率和内在智能设计。通过创新的紧凑架构支持超长上下文处理，结合从常识到复杂理工科及智能任务的分阶段训练策略，模型系统地培养了推理和规划能力。中期训练利用多样化高质量数据强化其计划与反思能力。实验表明，Youtu-LLM在小于20亿参数的模型中表现领先，既在通用任务上与大型模型竞争，又在智能代理任务上显著超越现有同类，展示了轻量模型具备强大内在智能潜力。,45
Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space,动态大型概念模型：自适应语义空间中的潜在推理,LLM,"ByteDance, THU",https://arxiv.org/pdf/2512.24617,https://huggingface.co/papers/2512.24617,本文提出了动态大概念模型（DLCM），通过从潜在表示中自动学习语义边界，将计算从逐词处理转移到压缩的概念空间，以提高推理效率。该方法无需预定义语言单位，支持变长概念的端到端发现，显著优化了计算资源分配。作者还引入了首个考虑压缩效应的模型扩展规律，指导在固定计算预算下合理分配资源，并设计了稳定训练该架构的参数化方法。实验证明，在相同计算量下，DLCM在多项零样本任务中平均提升性能2.69%，展现了其在语言理解和推理上的潜力。,45
NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation,NextFlow：统一的序列建模激活多模态理解与生成,Multimodal LLM,"ByteDance, THU",https://arxiv.org/pdf/2601.02204,https://huggingface.co/papers/2601.02204,本文提出了NextFlow，一种统一的自回归变换器模型，能够同时处理交错的文本和图像信息，实现高效的多模态理解与生成。通过结合文本的顺序预测和图像的多尺度预测策略，NextFlow显著提升了图像生成速度，能够在5秒内生成高质量的1024×1024图像，远快于传统方法。该模型支持图像编辑、交织内容生成和视频创作，且在视觉质量上达到领先水平，展示了统一模型在多模态任务中的强大潜力和广泛应用价值。,45
MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization,MOSS Transcribe Diarize：具备说话人分离功能的精准转录,Multimodal LLM,Other,https://arxiv.org/pdf/2601.01554,https://huggingface.co/papers/2601.01554,本文提出了MOSS Transcribe Diarize，一种统一的多模态大语言模型，能够端到端地完成多说话人语音的准确转录、说话人标注及时间戳定位。该模型支持最长90分钟的输入，具备超长上下文处理能力，显著提升了说话人记忆和时间精度。通过在大规模真实数据上训练，MOSS Transcribe Diarize在多项公开和内部测试中表现优于现有商业系统，解决了传统多模块系统中误差累积和上下文利用不足的问题，极大增强了会议记录等多说话人场景的转录质量和实用性。,45
CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature,CaricatureGS：基于高斯曲率的3D高斯点渲染人脸夸张方法,Other,Other,https://arxiv.org/pdf/2601.03319,https://huggingface.co/papers/2601.03319,本文提出了一种基于高斯曲率的三维面部夸张方法，结合了3D高斯点绘制技术，实现了可控且逼真的三维头像变形。通过解决加权泊松方程获得夸张形状，并设计了交替使用真实与合成图像的训练方案，使单一高斯集合同时表达自然与夸张面部形态。该方法支持局部编辑和连续调节夸张程度，且通过高效插值实现实时变形。实验结果表明，该方法在保留几何细节和视觉真实感方面优于现有技术，推动了高质量三维卡通化头像的生成。,45
Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation,Avatar Forcing：用于自然对话的实时交互式头部头像生成,Embodied AI,Other,https://arxiv.org/pdf/2601.00664,https://huggingface.co/papers/2601.00664,本文提出了Avatar Forcing，一种实时生成互动头部虚拟形象的框架，能够基于用户的音频和动作输入，低延迟地生成自然且富有表现力的头像反应。该方法通过扩散强制和无标签的偏好优化，实现了无需额外标注数据的生动交互，显著提升了头像的响应速度和情感表达能力。实验表明，Avatar Forcing在实时性和互动性上均优于现有方法，增强了虚拟交流中的沉浸感和自然感，推动了虚拟头像在内容创作和人机交互中的应用。,44
K-EXAONE Technical Report,K-EXAONE 技术报告,LLM,Other,https://arxiv.org/pdf/2601.01739,https://huggingface.co/papers/2601.01739,本文介绍了由LG AI Research开发的多语言大规模语言模型K-EXAONE。该模型采用专家混合架构，拥有2360亿参数，推理时激活230亿参数，支持长达25.6万词的上下文窗口，涵盖韩语、英语、西班牙语、德语、日语和越南语六种语言。通过多项综合评测，K-EXAONE在推理、编程、语言理解等任务中表现出与同规模开源模型相当的竞争力。该模型旨在推动人工智能技术的发展，具备广泛的工业和科研应用潜力。,44
LTX-2: Efficient Joint Audio-Visual Foundation Model,LTX-2：高效联合音视频基础模型,Diffusion Model,Other,https://arxiv.org/pdf/2601.03233,https://huggingface.co/papers/2601.03233,本文提出了LTX-2，一种开源的联合音视频生成基础模型，能够同步生成高质量的视频和音频内容。该模型采用双流变换器结构，分别处理视频和音频信息，并通过跨模态注意力机制实现二者的紧密结合。LTX-2支持多语言文本输入，具备良好的音视频对齐和控制能力，生成的音频不仅包含语音，还包括丰富的环境和情感声音元素。实验结果表明，LTX-2在开源系统中实现了领先的视听质量和文本匹配效果，且计算效率高于多数同类专有模型。所有代码和模型权重均已公开。,43
Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs,超越实数：面向长上下文大语言模型的旋转位置编码的虚数扩展,LLM,Shanghai AI Lab,https://arxiv.org/pdf/2512.07525,https://huggingface.co/papers/2512.07525,本文提出了一种改进的旋转位置编码方法，通过同时利用复数点积中的实部和虚部信息，增强了大型语言模型对长文本上下文的建模能力。传统方法仅使用实部，忽略了包含重要位置信息的虚部，导致对长距离依赖的捕捉不足。新方法充分利用复数表示，构建双分量注意力分数，理论和实验均表明其在多项长文本语言建模任务中表现优于标准旋转位置编码，且随着上下文长度增加，性能提升更为显著。相关代码已开源。,42
Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models,4D推理学习：面向视觉语言模型的动态空间理解,Multimodal LLM,Tencent,https://arxiv.org/pdf/2512.20557,https://huggingface.co/papers/2512.20557,本文提出了DSR Suite，一种提升视觉语言模型动态空间推理能力的系统。通过自动化流程从真实视频中生成带有视角变化和多物体交互的问答数据，构建了用于训练和评测的动态空间推理数据集。结合轻量级几何选择模块，有效整合几何先验信息，增强模型对物体在三维空间随时间变化的理解能力。实验证明，该方法显著提升了模型在动态空间推理任务上的表现，同时保持了对一般视频理解的准确性，推动了视觉语言模型在四维动态场景中的应用发展。,42
Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving,面向奥林匹克级数学问题求解的长时域推理智能体,Agent,Shanghai AI Lab,https://arxiv.org/pdf/2512.10739,https://huggingface.co/papers/2512.10739,本文提出了Intern-S1-MO，一种面向奥林匹克级数学难题的长程推理智能体，采用多轮层级推理和紧凑记忆结构，有效突破了大型推理模型的上下文限制。结合OREAL-H强化学习框架，提升模型的推理能力和整体表现。实验证明，该智能体在IMO2025非几何题目中表现达到银牌水平，并在多项数学推理竞赛中超越现有先进模型，展现出解决超长复杂推理任务的潜力。,41
DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models,DiffThinker：基于扩散模型的生成式多模态推理探索,Diffusion Model,Other,https://arxiv.org/pdf/2512.24165,https://huggingface.co/papers/2512.24165,本文提出了DiffThinker，一种基于扩散模型的生成式多模态推理框架，创新性地将多模态推理转化为图像到图像的生成任务。相比传统以文本为主的多模态大模型，DiffThinker在处理复杂的视觉推理任务时表现出更高的逻辑一致性和空间精度。系统性分析揭示了该方法在效率、可控性、并行性和协作性上的优势。大量实验覆盖规划、优化、约束满足和空间配置等领域，结果显示DiffThinker显著超越了包括GPT-5在内的多款先进模型，展示了生成式多模态推理在视觉中心任务中的巨大潜力。,41
Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning,超越静态工具：面向科学推理的测试时工具进化,AI4Science,"Shanghai AI Lab, THU",https://arxiv.org/pdf/2601.07641,https://huggingface.co/papers/2601.07641,本论文提出了一种名为“测试时工具进化”（TTE）的新方法，使人工智能系统能够在推理过程中动态生成、验证并改进计算工具，突破了传统静态工具库的局限。该方法将工具从固定资源转变为针对具体问题自动演化的产物，有效解决了科学领域工具稀缺、多样且不完整的问题。为评估该方法，作者构建了包含1590个科学推理任务和925个自动演化工具的基准测试。实验结果表明，TTE在准确性和工具使用效率上均达到领先水平，并实现了跨领域的适应能力。,41
Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality,保持源视频真实感：用于电影质量的高保真换脸技术,Other,Other,https://arxiv.org/pdf/2512.07951,https://huggingface.co/papers/2512.07951,本文提出了LivingSwap，一种基于视频参考引导的高保真视频换脸方法，专为电影级别的长镜头视频设计。该方法通过关键帧注入目标身份，结合视频参考信息，实现了在复杂光照、夸张表情及遮挡等挑战下的稳定身份保持和细节还原。为解决训练数据不足问题，作者构建了配对换脸数据集Face2Face，并采用数据反转增强监督效果。实验表明，LivingSwap在保持源视频真实感和时间一致性方面表现优异，显著提升了换脸质量并减少了人工调节工作量，具备广泛的影视制作应用价值。,40
Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation,我们准备好在文本到3D生成中应用强化学习了吗？一项渐进式调查,Agent,"Shanghai AI Lab, PKU",https://arxiv.org/pdf/2512.10949,https://huggingface.co/papers/2512.10949,本文首次系统性探讨了强化学习在文本驱动的三维生成中的应用，重点研究了奖励设计、强化学习算法、评测基准和分层优化策略。作者发现，奖励信号需与人类偏好高度一致，多模态模型能有效提供三维属性信息；基于令牌级优化的算法表现优越；现有评测不足以衡量三维生成的推理能力，因而提出了新基准MME-3DR；并设计了分层强化学习框架Hi-GRPO以优化全局到局部的生成过程。基于此，开发了首个强化学习增强的文本到三维生成模型AR3D-R1，实现从粗糙形状到细节纹理的高质量生成。该研究为三维生成中的强化学习提供了重要方法和基准支持。,40
Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience,Seed-Prover 1.5：通过经验学习掌握本科水平的定理证明,Agent,ByteDance,https://arxiv.org/pdf/2512.17260,https://huggingface.co/papers/2512.17260,本文介绍了Seed-Prover 1.5，一种基于大规模强化学习训练的形式化定理证明模型，结合高效的测试时扩展流程，有效提升了数学定理证明的能力和效率。该模型通过持续与Lean等工具交互积累经验，显著缩小了自然语言与形式语言证明之间的差距。Seed-Prover 1.5在本科及更高难度的数学问题上表现优异，解决了88%的本科级PutnamBench问题，并在有限计算资源下超越了现有最先进方法。研究表明，基于高质量反馈的经验学习对未来形式数学推理具有重要潜力。,40
User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale,面向用户的多轮对话生成及大规模工具使用,Agent,Other,https://arxiv.org/pdf/2601.08225,https://huggingface.co/papers/2601.08225,本文提出了一种基于大规模推理模型的多轮对话生成框架，旨在通过动态生成领域相关工具，实现复杂任务的自动化解决。相比传统以任务为中心的方法，该框架引入了模拟真实用户行为的用户导向策略，促进更自然、持续的多轮互动，反映现实中问题解决的迭代过程。该方法支持从任意对话状态启动，具备高度扩展性，能够在单条对话中完成多个任务，生成丰富且贴近真实人机交互需求的数据集，推动多轮工具使用对话研究的发展。,40
EgoX: Egocentric Video Generation from a Single Exocentric Video,EgoX：基于单个外视角视频的第一视角视频生成,Diffusion Model,Other,https://arxiv.org/pdf/2512.08269,https://huggingface.co/papers/2512.08269,本文提出了EgoX框架，实现了从单个第三人称（外视角）视频生成第一人称（自视角）视频。该方法通过轻量化调整预训练的视频扩散模型，结合外视角与自视角信息，并引入几何引导的注意力机制，确保生成视频在视觉效果和空间一致性上的高质量表现。EgoX不仅能够真实地还原场景内容，还能合理合成视野外区域，展现出良好的泛化能力和鲁棒性。此技术为沉浸式体验、影视制作及机器人感知等领域提供了新的可能。,39
NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents,NL2Repo-Bench：面向长时序仓库生成的编码智能体评测,Agent,"ByteDance, PKU",https://arxiv.org/pdf/2512.12730,https://huggingface.co/papers/2512.12730,本文提出了NL2Repo Bench，一个专门用于评估编码智能体在长时间跨度内独立构建完整Python软件库能力的新基准。该基准通过仅给出自然语言需求，要求智能体自主设计架构、管理依赖、实现多模块逻辑并生成可安装的软件库。实验结果显示，即使是最先进的模型也难以达到较高的完成率，暴露出长期推理、全局一致性维护和跨文件依赖管理等核心挑战。NL2Repo Bench为衡量和推动编码智能体的持续智能能力提供了重要工具，强调了长远规划能力在自动化软件开发中的关键作用。,38
Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling,Scone：通过统一的理解-生成建模桥接主体驱动图像生成中的组合与区分,Diffusion Model,PKU,https://arxiv.org/pdf/2512.12675,https://huggingface.co/papers/2512.12675,本文提出了Scone，一种将图像内容组合与对象区分能力统一集成的主体驱动图像生成方法。通过两阶段训练策略，Scone首先学习多主体的组合关系，随后利用语义对齐和注意力掩码技术提升对目标主体的准确识别与生成能力。为全面评估模型性能，作者还设计了SconeEval基准测试。实验结果表明，Scone在多主体图像生成任务中显著优于现有开源模型，提升了复杂场景下的图像生成质量和主体区分效果。模型与数据已开源，推动该领域进一步发展。,38
Region-Constraint In-Context Generation for Instructional Video Editing,基于区域约束的上下文生成用于教学视频编辑,Diffusion Model,Other,https://arxiv.org/pdf/2512.17650,https://huggingface.co/papers/2512.17650,本文提出了ReCo，一种针对教学视频编辑的新方法，通过在生成过程中引入对编辑区域与非编辑区域的约束，有效提升了编辑的准确性并减少了区域间的干扰。ReCo通过联合处理源视频与目标视频，利用两种正则化策略强化对编辑区域的关注，同时抑制非编辑区域的影响，保证了视频内容的精细修改。此外，作者构建了包含50万条指令-视频对的大规模数据集ReCo-Data，支持模型训练。大量实验表明，ReCo在多种指令驱动的视频编辑任务中表现优越，展示了其在教学视频编辑领域的潜力和实用价值。,38
LongVideoAgent: Multi-Agent Reasoning with Long Videos,LongVideoAgent：基于长视频的多智能体推理框架,Agent,Other,https://arxiv.org/pdf/2512.20618,https://huggingface.co/papers/2512.20618,本文提出了一种多智能体框架LongVideoAgent，用于提升长视频问答的时序定位和多模态理解能力。该框架由主控语言模型协调定位智能体和视觉智能体，分别负责定位相关视频片段和提取视觉细节，结合字幕信息实现更精准的回答。通过强化学习训练，系统能够高效协同，避免信息丢失，显著优于传统单一模型。实验在新构建的长视频问答数据集上验证了方法的有效性，展示了多智能体协作在长视频理解中的潜力。,38
The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning,思维的分子结构：长链思维推理拓扑映射,LLM,"ByteDance, PKU",https://arxiv.org/pdf/2601.06002,https://huggingface.co/papers/2601.06002,本文针对大语言模型在长链式推理中的表现不稳定问题，提出了一种基于“分子结构”视角的分析方法，揭示有效推理路径由三种关键交互模式构成，类似化学键的稳定结构。研究发现，这些稳定结构源于长链推理的微调过程，而非简单模仿。基于此，作者设计了Mole-Syn方法，通过引导推理结构的合成，显著提升了模型的训练稳定性和推理性能，为改善复杂逻辑推理提供了新思路。,38
Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning,Fast-ThinkAct：通过可表述潜在规划实现高效的视觉-语言-动作推理,Embodied AI,Other,https://arxiv.org/pdf/2601.09708,https://huggingface.co/papers/2601.09708,本文提出了Fast-ThinkAct，一种高效的视觉-语言-动作推理框架。该方法通过紧凑的潜在推理策略，大幅降低了推理延迟（最高减少89.3%），同时保持了长远规划和少量样本适应能力。Fast-ThinkAct通过从教师模型蒸馏学习，将语言和视觉规划能力融合，实现了推理与动作执行的有效衔接。大量实验表明，该方法在多种任务中表现出色，兼顾效率与性能，显著推动了视觉语言驱动的智能体控制研究。,38
DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry,DentalGPT：激励牙科多模态复杂推理的大语言模型,Multimodal LLM,PKU,https://arxiv.org/pdf/2512.11558,https://huggingface.co/papers/2512.11558,本文提出了DentalGPT，一种专门针对牙科领域设计的多模态大语言模型。通过构建包含12万余张配有详细诊断描述的牙科图像数据集，并结合领域知识注入与强化学习训练，DentalGPT显著提升了对牙科视觉信息的理解和复杂推理能力。在多个牙科图像和问答任务中，DentalGPT表现优异，超越了许多参数更多的先进模型。该研究展示了高质量专业数据与分阶段训练策略在打造高效牙科智能辅助系统中的重要作用。,37
"DEER: Draft with Diffusion, Verify with Autoregressive Models",DEER：用扩散模型进行草稿生成，结合自回归模型进行验证,LLM,Other,https://arxiv.org/pdf/2512.15176,https://huggingface.co/papers/2512.15176,本文提出了DEER框架，通过结合扩散模型与自回归模型，实现高效的“先草拟后验证”解码策略。相比传统依赖自回归草拟器的方法，DEER利用扩散模型的并行解码优势，克服了逐步不确定性积累和解码顺序限制的问题，从而大幅提升解码速度和草稿质量。实验结果显示，DEER在生成草稿长度和加速比方面均显著优于现有方法，展示了其在提升大型语言模型推理效率上的潜力。,37
Fast and Accurate Causal Parallel Decoding using Jacobi Forcing,使用Jacobi Forcing的快速且准确的因果并行解码,LLM,Other,https://arxiv.org/pdf/2512.14681,https://huggingface.co/papers/2512.14681,本文提出了Jacobi Forcing，一种渐进式蒸馏方法，实现了基于Transformer模型的高效并行解码，同时保持了自回归模型的生成质量。该方法通过在模型自身生成的并行解码轨迹上训练，有效解决了传统扩散语言模型在训练分布和推理方式上的不匹配问题。基于此，作者进一步设计了多块解码与拒绝回收机制，显著提升了每次迭代的有效生成令牌数，最大实现近4倍的推理加速，且性能损失极小。该方法为大规模语言模型的低延迟推理提供了实用且高效的解决方案。,36
StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors,StereoPilot：通过生成先验学习统一且高效的立体转换,Other,Other,https://arxiv.org/pdf/2512.16915,https://huggingface.co/papers/2512.16915,本文提出了StereoPilot，一种高效的单阶段模型，用于将单目视频转换为高质量立体视频，避免了传统多阶段深度估计和图像修复流程中的误差累积和格式不一致问题。为支持模型训练，作者构建了首个涵盖多种立体格式的大规模统一数据集UniStereo。StereoPilot通过引入可学习的格式切换机制和循环一致性约束，实现了对不同立体格式的自适应转换。实验结果表明，该方法在视觉效果和计算效率上均优于现有技术，推动了自动立体视频生成的发展。,36
Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation,驯服幻觉：通过反事实视频生成提升多模态大语言模型的视频理解能力,Multimodal LLM,"THU, Alibaba",https://arxiv.org/pdf/2512.24271,https://huggingface.co/papers/2512.24271,本文针对多模态大语言模型（MLLMs）在视频理解中易受语言先验影响导致的“幻觉”问题，提出了一种基于反事实视频生成的数据增强方法。通过Diffusion模型对真实视频进行可控编辑，自动生成高质量的反事实视频及对应问答对，构建了大规模数据集DualityVidQA用于对比训练。同时，设计了两阶段训练策略DNA-Train以提升模型稳定性和效果。实验表明，该方法显著减少了模型在反事实视频上的错误回答，提升了视频理解的准确性和泛化能力。,36
Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers,可学习乘子：释放语言模型矩阵层的尺度,LLM,Other,https://arxiv.org/pdf/2601.04890,https://huggingface.co/papers/2601.04890,本文提出了一种引入可学习乘子的方法，用以解决大型语言模型训练中权重衰减引起的尺度限制问题。通过为权重矩阵及其行列分别添加可调节的乘子，模型能够自动学习最优的权重尺度，从而提升性能并减少调参开销。该方法是对现有乘子技术的扩展，实验证明其在多种优化器下均优于传统方案，显著改善了训练效果和模型表现。,36
SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL,SkinFlow：通过动态视觉编码与分阶段强化学习实现开放式皮肤病诊断的信息高效传输,Multimodal LLM,"THU, PKU",https://arxiv.org/pdf/2601.09136,https://huggingface.co/papers/2601.09136,本文提出SkinFlow，一种针对皮肤病诊断的视觉语言模型框架，通过优化视觉信息传输效率提升诊断准确性。该方法引入动态视觉编码器和分阶段强化学习，分别对病变特征的显性描述和隐性纹理进行优化，避免了单纯依赖模型参数规模扩展。实验结果表明，SkinFlow在Fitzpatrick17k数据集上显著优于大型通用模型，验证了提升信息表达能力比盲目增大模型规模更有效。此研究为医学影像诊断提供了新的思路，兼顾诊断安全性和临床相关性。,36
RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes,RL-AWB：基于深度强化学习的低光夜间场景自动白平衡校正,Agent,Other,https://arxiv.org/pdf/2601.05249,https://huggingface.co/papers/2601.05249,本文提出了RL-AWB，一种结合统计方法与深度强化学习的自动白平衡校正框架，专门针对低光照夜间场景中的复杂光照和噪声问题。该方法首先利用夜间场景特有的灰度像素检测和光照估计算法作为基础，随后通过强化学习动态调整白平衡参数，模拟专业调节人员的优化过程。为验证方法的泛化能力，作者还构建了首个多传感器夜间数据集。实验结果表明，RL-AWB在低光及多样光照条件下均表现出优异的适应性和准确性。,35
SpatialTree: How Spatial Abilities Branch Out in MLLMs,SpatialTree：多模态大语言模型中空间能力的分支发展,Multimodal LLM,ByteDance,https://arxiv.org/pdf/2512.20617,https://huggingface.co/papers/2512.20617,本文提出了SpatialTree，一种基于认知科学的空间能力层级框架，将多模态大语言模型（MLLMs）的空间能力划分为感知、心智映射、模拟和自主操作四个层次。基于此，构建了首个能力中心的分层评测体系，全面评估了主流MLLM在27项子能力上的表现，揭示了低层感知能力相对独立、高层能力间高度相关的结构特征。研究发现低层能力内部存在负迁移，而低层向高层能力存在显著正迁移。针对提升整体表现，提出了“自动思考”策略，有效避免过度推理，稳定提升各层能力表现。该工作为系统理解和提升MLLM空间智能提供了新思路。,34
DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer,DreamID-V：通过扩散变换器弥合图像到视频的高保真人脸替换差距,Diffusion Model,"ByteDance, THU",https://arxiv.org/pdf/2601.01425,https://huggingface.co/papers/2601.01425,本文提出了一种名为DreamID-V的视频换脸新框架，有效融合了图像换脸技术与扩散变换器模型，解决了视频换脸中身份保持和视觉一致性难题。通过设计同步身份数据生成管线和多模态条件注入模块，结合合成到真实的视频训练策略及身份一致性强化学习，显著提升了换脸的真实感和连续性。此外，作者构建了涵盖多场景的换脸评测基准IDBench-V。大量实验表明，DreamID-V在换脸效果和适应性上均优于现有方法，推动了高质量视频换脸技术的发展。,34
ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands,ShowUI-π：基于流模型的GUI灵巧手生成模型,Agent,Other,https://arxiv.org/pdf/2512.24965,https://huggingface.co/papers/2512.24965,本文提出了ShowUI-π，一种基于流模型的图形界面操作智能体，首次实现了将离散点击与连续拖拽动作统一建模，支持在多种交互模式下灵活操作。通过预测连续的光标轨迹，ShowUI-π能够实现类似人类的灵巧拖拽操作，解决了现有方法仅支持离散点击的局限。作者还构建了包含2万条拖拽轨迹的多领域数据集及评测基准ScreenDrag，实验证明ShowUI-π在拖拽任务上显著优于现有系统。该工作推动了数字环境中智能体向更自然、高效的交互控制发展。,34
SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder,SVG-T2I：无需变分自编码器的文本到图像潜在扩散模型的规模化,Diffusion Model,THU,https://arxiv.org/pdf/2512.11749,https://huggingface.co/papers/2512.11749,本文提出了SVG-T2I，一种基于视觉基础模型特征空间的文本到图像扩散生成框架，突破了传统依赖变分自编码器的限制。该方法直接在视觉基础模型的高维特征域中进行训练，实现了高质量的文本驱动图像合成，且在多个评测指标上表现优异。SVG-T2I不仅验证了视觉基础模型在生成任务中的强大表现力，还通过开源完整的模型及训练流程，推动了基于表示学习的视觉生成研究发展。,33
Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics,无误差线性注意力是一种免费午餐：来自连续时间动力学的精确解,LLM,Other,https://arxiv.org/pdf/2512.12602,https://huggingface.co/papers/2512.12602,本文提出了一种名为Error-Free Linear Attention（EFLA）的新型注意力机制，解决了传统软max注意力在处理长序列时计算成本高的问题。EFLA通过将在线学习更新建模为连续时间动力系统，推导出一个精确且可并行计算的闭式解，实现了线性时间复杂度且无误差累积。实验表明，EFLA在语言建模和下游任务中表现优于现有方法DeltaNet，且无需增加参数，展现了其在噪声环境下的鲁棒性和高效性。该工作为构建高效且精确的线性注意力模型提供了新的理论基础。,33
Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model,Seedance 1.5 pro：一种原生音视频联合生成基础模型,Diffusion Model,ByteDance,https://arxiv.org/pdf/2512.13507,https://huggingface.co/papers/2512.13507,本文介绍了Seedance 1.5 pro，一款专为同步生成高质量音视频内容设计的基础模型。该模型采用双分支结构，通过跨模态联合模块和多阶段数据处理，实现了精准的音视频同步与优异的生成效果。通过监督微调和人类反馈强化学习等后期优化，提升了模型的实用性和表现。此外，模型引入加速框架，使推理速度提升十倍以上。Seedance 1.5 pro支持多语言和方言的唇动同步，具备动态摄像机控制与更强的叙事连贯性，适用于专业内容创作领域。,33
SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents,SmartSnap：面向自我验证智能体的主动证据搜寻,Agent,Other,https://arxiv.org/pdf/2512.22322,https://huggingface.co/papers/2512.22322,本文提出了SmartSnap，一种主动式自我验证范式，旨在提升基于强化学习的自主智能体在复杂界面任务中的任务完成验证效率。不同于传统依赖事后分析完整交互轨迹的被动验证，SmartSnap设计了具备双重目标的自我验证智能体，通过精选关键截图作为证据，主动证明任务完成情况。该方法遵循完整性、简洁性和创造性原则，显著降低验证成本并提升可靠性。实验证明，SmartSnap在多种模型和任务上均带来显著性能提升，展示了其在训练高效、可扩展智能体方面的潜力。,33
"Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem","[翻译]Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",Other,Other,https://arxiv.org/pdf/2512.24873,https://huggingface.co/papers/2512.24873,无法生成摘要。,33
EditThinker: Unlocking Iterative Reasoning for Any Image Editor,EditThinker：解锁面向任意图像编辑器的迭代推理能力,Agent,THU,https://arxiv.org/pdf/2512.05965,https://huggingface.co/papers/2512.05965,本文提出了EditThinker，一种基于迭代推理的图像编辑框架，通过模拟人类“思考-编辑”循环，反复批判和优化编辑指令，从而显著提升图像编辑模型对指令的遵循能力。该方法利用单一大语言模型作为推理引擎，结合强化学习实现思考与编辑的协调，能够针对性地改进编辑指令。大量实验表明，EditThinker在多个基准测试中均显著优于现有方法，提升了多种图像编辑模型的表现。论文还将开放相关数据和模型，促进社区发展。,32
OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory,OneStory：基于自适应记忆的连贯多镜头视频生成,Diffusion Model,Meta,https://arxiv.org/pdf/2512.07802,https://huggingface.co/papers/2512.07802,本文提出了OneStory，一种针对多镜头视频生成的新方法，旨在实现跨镜头的全局语义连贯性。通过将多镜头视频生成任务重构为“下一镜头”预测，OneStory利用两个关键模块——基于前镜头关键帧构建全局记忆的帧选择模块和生成紧凑上下文信息的自适应调节器，实现了对复杂叙事的有效建模。结合精心整理的多镜头数据集和预训练图像到视频模型，OneStory在多样化场景下显著提升了叙事一致性，推动了可控且沉浸式长视频故事生成的发展。,32
Generative Refocusing: Flexible Defocus Control from a Single Image,生成式重聚焦：基于单幅图像的灵活散焦控制,Other,Other,https://arxiv.org/pdf/2512.16923,https://huggingface.co/papers/2512.16923,本文提出了一种名为Generative Refocusing的方法，实现了从单张图像灵活控制景深和虚化效果。该方法通过两步处理，先用DeblurNet恢复全聚焦图像，再用BokehNet生成可调节的虚化效果。创新点在于采用半监督学习，结合合成配对数据和真实无配对虚化图像，利用照片元数据捕捉真实光学特性。实验表明，该方法在去模糊、虚化合成和重新聚焦任务中表现优异，支持文本引导调整和自定义光圈形状，提升了单图景深控制的实用性和灵活性。,32
Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation,扩散模型知晓透明性：将视频扩散模型用于透明物体的深度与法线估计,Diffusion Model,Other,https://arxiv.org/pdf/2512.23705,https://huggingface.co/papers/2512.23705,本文提出了一种基于视频扩散模型的新方法，用于估计透明和反射物体的深度与法线。作者构建了大规模合成视频数据集TransPhy3D，结合物理渲染技术训练模型，实现了对透明现象的准确捕捉和时序一致的深度预测。所提模型在多个真实和合成数据集上表现出色，显著优于现有深度估计方法，并在机器人抓取任务中提升了成功率。研究表明，生成式视频模型蕴含透明物体的光学规律，可高效无监督地支持复杂场景的视觉感知。,32
Token-Level LLM Collaboration via FusionRoute,基于FusionRoute的Token级大语言模型协作,LLM,Meta,https://arxiv.org/pdf/2601.05106,https://huggingface.co/papers/2601.05106,本文提出了FusionRoute，一种基于轻量级路由器的多大语言模型（LLM）协作框架。该方法在生成文本的每个步骤动态选择最合适的专家模型，并通过补充的预测结果对专家输出进行优化，克服了传统仅依赖单一专家的局限性。理论分析表明，FusionRoute在较宽松的条件下能实现更优的生成策略。实验证明，该框架在数学推理、代码生成和指令执行等多项任务中，优于现有的模型合并和微调方法，且在效率和性能上均表现出色。,32
MemoBrain: Executive Memory as an Agentic Brain for Reasoning,MemoBrain：作为推理智能体大脑的执行记忆模型,Agent,Other,https://arxiv.org/pdf/2601.08079,https://huggingface.co/papers/2601.08079,本文提出了MemoBrain，一种专为工具辅助智能体设计的执行记忆模型，旨在解决复杂推理过程中信息积累导致的上下文限制问题。MemoBrain通过构建依赖关系感知的记忆结构，有效捕捉和组织关键的中间推理状态，主动管理和优化工作上下文，避免无效信息干扰，保持推理的连贯性和目标一致性。实验结果表明，MemoBrain在多个长时序推理任务中显著提升了智能体的表现，展示了其在支持复杂、长期任务中维持高效推理的潜力。,32
Unified Video Editing with Temporal Reasoner,带有时间推理器的统一视频编辑,Diffusion Model,Other,https://arxiv.org/pdf/2512.07469,https://huggingface.co/papers/2512.07469,本文提出了一种名为VideoCoF的新颖视频编辑方法，解决了现有技术中精确编辑依赖用户提供掩码与统一模型缺乏空间定位信息之间的矛盾。VideoCoF通过“观察-推理-编辑”流程，先预测编辑区域的推理信息，再生成目标视频，实现了无需掩码的精准指令定位和细粒度编辑。此外，提出的时序位置编码对齐策略增强了动作一致性并支持超长视频编辑。该方法仅用5万对视频数据训练，已在基准测试中达到领先性能，展示了高效且实用的统一视频编辑能力。,31
OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification,OPV：基于结果的流程验证器用于高效的长链思维链验证,LLM,Shanghai AI Lab,https://arxiv.org/pdf/2512.10756,https://huggingface.co/papers/2512.10756,本文提出了一种名为Outcome-based Process Verifier（OPV）的新型验证方法，用于提升大型语言模型在处理复杂长推理链时的准确性和效率。OPV结合了结果验证与过程验证，通过迭代主动学习和细化训练，显著降低了人工标注成本。实验结果表明，OPV在多个基准测试中表现优异，超越了更大规模的开源模型，并能有效识别错误推理，提升了模型的整体推理质量和应用性能。该方法为复杂推理任务的自动化验证提供了有力支持。,31
KlingAvatar 2.0 Technical Report,KlingAvatar 2.0 技术报告,Multimodal LLM,Other,https://arxiv.org/pdf/2512.13313,https://huggingface.co/papers/2512.13313,本文提出了KlingAvatar 2.0，一种高效生成长时长高分辨率虚拟人视频的框架。该方法通过先生成低分辨率关键帧，再细化为高分辨率连续片段，实现空间和时间上的逐步提升，保证视频的连贯性和画质。引入多模态协同推理机制，有效理解和融合多种输入指令，提升生成内容的准确性和一致性。同时支持多角色身份控制。实验结果表明，该模型在视觉清晰度、口型同步、身份保持及指令响应方面均有显著提升，推动了虚拟人视频生成技术的发展。,31
RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics,RoboTracer：基于视觉-语言模型的空间追踪与推理在机器人领域的掌握,Embodied AI,PKU,https://arxiv.org/pdf/2512.13660,https://huggingface.co/papers/2512.13660,本文提出了RoboTracer，一种具备三维空间感知能力的视觉语言模型，专为机器人执行复杂空间追踪任务设计。通过结合监督学习和强化学习，RoboTracer能够准确理解和测量三维空间中的位置关系，实现多步骤的空间推理。为支持训练，作者构建了包含3000万问答对的TraceSpatial数据集，并设计了评测基准TraceSpatial-Bench。实验表明，RoboTracer在空间定位、测量和追踪任务上显著优于现有方法，且能有效指导机器人在复杂环境中完成长时任务，展示了其在机器人空间理解与操作中的广泛应用潜力。,31
DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset,DanQing：最新的大规模中文视觉-语言预训练数据集,Multimodal LLM,Other,https://arxiv.org/pdf/2601.10305,https://huggingface.co/papers/2601.10305,本文提出了DanQing，一个包含1亿对高质量中文图文数据的大规模视觉语言预训练数据集，填补了中文图文数据缺乏的空白。DanQing数据主要来源于2024-2025年的网络内容，经过严格筛选，保证了数据的新颖性和质量。通过对SigLIP2模型的持续预训练，DanQing在多项中文下游任务中表现出显著优势。该数据集将以开源形式发布，推动中文视觉语言预训练领域的发展。,31
Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering,迈向超长时域智能体科学：面向机器学习工程的认知积累,Agent,Other,https://arxiv.org/pdf/2601.10402,https://huggingface.co/papers/2601.10402,本文提出了ML-Master 2.0，一种具备超长时间自主学习能力的智能代理，专注于机器学习工程这一科学探索的典型场景。通过引入分层认知缓存机制，ML-Master 2.0能够有效管理和积累长期实验中的执行经验，将短期操作与长期策略分离，突破传统模型在处理大规模、延迟反馈环境中的限制。实验结果显示，该系统在24小时的测试中表现优异，显著提升了任务完成率。该研究为实现具备持续自主探索能力的人工智能提供了新的可扩展路径。,31
BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain,BrainExplore：人脑中可解释视觉表征的大规模发现,Other,Other,https://arxiv.org/pdf/2512.08560,https://huggingface.co/papers/2512.08560,本文提出了BrainExplore，一种大规模自动化框架，用于发现和解释人脑视觉表征。该方法通过无监督分解技术识别脑部fMRI数据中的可解释活动模式，进而结合自然图像和语言描述，生成每种模式对应的视觉含义。该框架自动评估多种解释方案，确保结果的可靠性，揭示了成千上万种细粒度且多样的视觉概念表征，突破了以往研究规模小、依赖人工和缺乏系统验证的局限，推动了对人脑视觉信息处理机制的深入理解。,30
HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices,HyperVL：面向边缘设备的高效动态多模态大语言模型,Multimodal LLM,Other,https://arxiv.org/pdf/2512.14052,https://huggingface.co/papers/2512.14052,本文提出了HyperVL，一种专为移动设备设计的高效多模态大语言模型。通过图像切分、视觉分辨率压缩器和双重一致性学习三项技术，HyperVL有效降低了内存占用、延迟和功耗，同时保持了良好的性能表现。实验结果表明，HyperVL在多个基准测试中表现优异，显著提升了移动端多模态推理的实用性，推动了强大且高效的多模态模型在边缘设备上的应用。,30
Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation,Depth Any Panoramas：一种用于全景深度估计的基础模型,Other,Other,https://arxiv.org/pdf/2512.16913,https://huggingface.co/papers/2512.16913,本文提出了一种面向全景图像的深度估计基础模型，能够在多样化的真实场景中实现稳健且一致的深度预测。研究团队构建了一个大规模数据集，融合了公开数据、高质量合成数据及网络真实全景图像，并设计了三阶段伪标签筛选流程以提升无标签图像的标注质量。模型基于具备强泛化能力的预训练骨干网络，结合创新的优化策略，提高了对不同距离的适应性和几何一致性。实验结果表明，该方法在多个基准测试中表现优异，具备良好的零样本泛化能力，推动了全景深度估计技术的发展。,30
4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation,4D-RGPT：通过感知蒸馏实现区域级4D理解,Multimodal LLM,Other,https://arxiv.org/pdf/2512.17012,https://huggingface.co/papers/2512.17012,本文提出了4D-RGPT，一种专门针对视频输入的多模态大模型，旨在提升对动态场景中物体深度和时间变化的理解能力。通过引入感知式4D蒸馏技术，模型能够从预训练专家模型中学习更全面的四维表示。此外，作者构建了R4D-Bench，一个包含区域级提示的深度感知动态场景问答基准，促进对模型时空理解能力的评估。实验结果表明，4D-RGPT在现有和新提出的4D视频问答任务中均表现出显著提升，推动了多模态模型在复杂时空理解上的发展。,30
Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion,Stream-DiffVSR：基于自回归扩散的低延迟可流式视频超分辨率,Diffusion Model,Other,https://arxiv.org/pdf/2512.23709,https://huggingface.co/papers/2512.23709,本文提出了Stream-DiffVSR，一种基于自回归扩散模型的在线视频超分辨率方法。该方法仅依赖过去帧，结合了快速的四步去噪器、运动引导模块及轻量时序解码器，有效提升细节和时间一致性。Stream-DiffVSR在RTX4090上处理720p视频帧仅需0.328秒，显著降低了延迟并提升了视觉质量，首次实现了扩散模型在低延迟在线视频超分辨率中的实用性，适用于实时视频处理和低延迟应用场景。,30
SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning,SenseNova-MARS：通过强化学习赋能多模态智能体推理与搜索,Multimodal LLM,THU,https://arxiv.org/pdf/2512.24330,https://huggingface.co/papers/2512.24330,本文提出了SenseNova-MARS，一种通过强化学习增强视觉语言模型的多模态推理与工具使用框架。该方法动态结合图像搜索、文本搜索和图像裁剪等工具，实现了对复杂、高分辨率图像中知识密集型任务的细粒度理解。为提升训练稳定性，作者设计了新的优化算法BN-GSPO。实验表明，SenseNova-MARS在多个公开搜索和图像理解基准上表现优异，超越了多款先进模型。该工作推动了具备灵活工具调用能力的智能视觉系统的发展，并将开源相关代码和数据，促进后续研究。,30
Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning,Atlas：异构模型与工具的编排用于多领域复杂推理,Agent,THU,https://arxiv.org/pdf/2601.03872,https://huggingface.co/papers/2601.03872,本文提出了ATLAS，一种结合多种语言模型与工具的动态调度框架，旨在解决跨领域复杂推理中的最优模型-工具组合选择问题。ATLAS通过无训练的聚类路由和基于强化学习的多步路由两条路径，实现了领域特定对齐与泛化能力的提升。实验表明，该方法在15个基准测试中优于包括GPT-4o在内的现有方案，特别在视觉推理任务中表现出显著优势，展示了其在多模态复杂推理中的广泛应用潜力。,30
Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards,证据链构建：基于引文感知评分奖励的鲁棒强化学习用于深度搜索智能体,Agent,THU,https://arxiv.org/pdf/2601.06021,https://huggingface.co/papers/2601.06021,本文提出了一种面向深度搜索智能体的新型奖励机制——引用感知评分奖励（CaRR），通过细化推理过程的评估标准，强调推理的全面性、事实依据和证据链的构建，有效减少了智能体的投机取巧和虚假信息生成。结合该奖励机制，作者设计了引用感知群体相对策略优化方法（C-GRPO），显著提升了智能体在多个深度搜索任务中的表现和泛化能力。实验验证了该方法在促进基于证据的推理和提高答案准确性方面的优势，推动了更加稳健和可信的智能搜索技术发展。,30
"X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests",X-Coder：通过完全合成的任务、解答与测试推动竞赛编程的发展,LLM,"Microsoft, THU",https://arxiv.org/pdf/2601.06953,https://huggingface.co/papers/2601.06953,本文提出了一种完全基于合成数据训练代码语言模型的新方法，通过自动生成编程任务、解决方案和测试用例，减少对真实编程数据的依赖。作者设计了名为SynthSmith的数据合成流程，能够产生多样且具有挑战性的竞赛题目，支持监督微调和强化学习。基于此，X-Coder模型在多个竞赛编程基准测试中表现优异，超过了参数更多的现有模型。研究还揭示了合成数据的扩展规律及训练策略，对提升代码推理能力具有重要意义。,30
ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking,ArenaRL：通过基于锦标赛的相对排名扩展开放式智能体的强化学习,Agent,Alibaba,https://arxiv.org/pdf/2601.06487,https://huggingface.co/papers/2601.06487,本文提出了ArenaRL，一种针对开放式任务中大型语言模型智能体的强化学习新范式。传统方法依赖对单个响应的标量评分，难以区分细微差异，导致奖励信号失效和训练停滞。ArenaRL通过组内相对排名和基于锦标赛的对抗评估机制，提升了奖励信号的稳定性和区分度，有效平衡了计算效率与准确性。作者还构建了两个涵盖训练与评估的开放式任务基准，实验证明ArenaRL显著优于传统强化学习方法，增强了智能体解决复杂实际问题的能力。,30
Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning,通过复杂度提升强化学习实现奥林匹克级几何大语言模型智能体,Agent,"Shanghai AI Lab, PKU",https://arxiv.org/pdf/2512.10534,https://huggingface.co/papers/2512.10534,本文提出了基于大语言模型的几何问题求解代理InternGeometry，通过反复提出和验证命题及辅助构造，结合动态记忆机制，有效提升了复杂几何问题的解决能力。引入的复杂度递增强化学习策略加速训练，使模型在仅使用极少训练数据的情况下，成功解决了44道国际数学奥林匹克几何题，表现超过平均金牌得主。该方法展示了大语言模型在高水平几何推理中的潜力，并能提出创新辅助构造，推动了自动化数学推理的发展。,29
Puzzle Curriculum GRPO for Vision-Centric Reasoning,面向视觉推理的Puzzle Curriculum GRPO,Multimodal LLM,Other,https://arxiv.org/pdf/2512.14944,https://huggingface.co/papers/2512.14944,本文提出了Puzzle Curriculum GRPO（PC-GRPO），一种无需人工标注和外部验证器的自监督强化学习方法，旨在提升视觉语言模型的视觉推理能力。通过设计三种拼图类自监督环境和引入难度感知的课程策略，PC-GRPO有效缓解了奖励稀疏和平坦问题，增强了推理过程与最终答案的一致性。实验证明，该方法在多个基准测试和不同模型规模上均显著提升了推理质量、训练稳定性和任务准确率，为视觉语言模型的可验证且可解释的强化学习后训练提供了实用路径。,29
Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing,语义与重建同等重要：使表示编码器适用于文本到图像生成与编辑,Diffusion Model,Other,https://arxiv.org/pdf/2512.17909,https://huggingface.co/papers/2512.17909,本文提出了一种系统性框架，通过引入语义与像素重建目标，有效调整视觉理解编码器的特征，使其适用于文本到图像生成及编辑任务。该方法在保持语义丰富性的同时，实现了对细节的高效压缩，提升了图像重建质量和生成准确性。实验结果显示，该框架不仅加快了模型训练收敛速度，还在图像生成与编辑性能上取得显著提升，证明了理解型编码器在生成领域的潜力和实用价值。,29
MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head,MHLA：通过基于Token维度的多头机制恢复线性注意力的表达能力,Other,PKU,https://arxiv.org/pdf/2601.07832,https://huggingface.co/papers/2601.07832,本文提出了一种名为多头线性注意力（MHLA）的新方法，旨在解决现有线性注意力模型中表现下降的问题。MHLA通过在令牌维度上划分多个注意力头，有效保持了模型的表达多样性，避免了全局上下文信息的丧失，同时保持了线性计算复杂度。实验证明，MHLA在图像分类、自然语言处理、图像生成和视频生成等多个领域均显著提升了性能，且未增加额外计算负担，展现出较强的实用价值和广泛适用性。,29
Ministral 3,Ministral 3系列,Multimodal LLM,Other,https://arxiv.org/pdf/2601.08584,https://huggingface.co/papers/2601.08584,本文介绍了Ministral 3系列语言模型，该系列包含3B、8B和14B三种参数规模，每种规模均提供基础版、指令微调版和推理版三种变体，适用于计算和内存受限的应用场景。模型通过一种称为Cascade Distillation的迭代剪枝与蒸馏训练方法，从大规模预训练模型高效迁移知识，显著减少训练资源需求。Ministral 3在保持竞争性能的同时，具备图像理解能力，并以开源许可方式发布，推动了高效且实用的多模态语言模型发展。,29
Voxify3D: Pixel Art Meets Volumetric Rendering,Voxify3D：像素艺术与体积渲染的结合,Other,Other,https://arxiv.org/pdf/2512.07834,https://huggingface.co/papers/2512.07834,本文提出了Voxify3D，一种结合3D网格优化与2D像素艺术监督的两阶段框架，用于自动生成高质量的体素艺术。该方法通过正交视角消除透视失真，实现像素与体素的精确对齐；利用基于图像语义的对齐保持模型语义信息；并采用可微分的调色板约束策略，实现对离散颜色空间的有效优化。实验表明，Voxify3D在保持语义清晰和像素艺术风格的同时，支持多种颜色数量和分辨率设置，生成结果在视觉效果和用户偏好上均优于现有方法，适用于游戏和数字媒体中的体素艺术创作。,28
VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation,VAR RL正确实践：解决视觉自回归生成中的异步策略冲突,Agent,"ByteDance, THU",https://arxiv.org/pdf/2601.02256,https://huggingface.co/papers/2601.02256,本文针对视觉自回归生成模型在强化学习训练中因异步策略冲突导致的不稳定性问题，提出了一种改进的优化框架。该方法通过引入中间奖励以稳定早期生成过程，动态调整各时间步的权重以精准分配反馈，以及设计新的掩码传播算法以隔离优化影响，有效缓解了跨时间步的冲突。实验结果表明，该框架显著提升了生成样本质量和训练稳定性，为视觉自回归模型的强化学习优化提供了实用且高效的解决方案。,28
SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence,SciEvalKit：面向科学通用智能的开源评测工具包,AI4Science,Shanghai AI Lab,https://arxiv.org/pdf/2512.22334,https://huggingface.co/papers/2512.22334,本文介绍了SciEvalKit，一款开源的统一评测工具，专门用于评价人工智能在科学领域的综合能力。该工具涵盖物理、化学、天文学等六大科学领域，聚焦科学感知、推理、符号计算、代码生成和假设提出等核心科学智能能力。SciEvalKit基于真实领域数据构建专家级基准，支持多模型批量评测和自定义扩展，确保结果透明且可复现。该平台为科学智能模型的标准化评价提供了灵活且多样化的框架，推动AI在科学研究中的应用与发展。,28
Benchmark^2: Systematic Evaluation of LLM Benchmarks,Benchmark^2：大语言模型基准测试的系统评估,LLM,Other,https://arxiv.org/pdf/2601.03986,https://huggingface.co/papers/2601.03986,本文提出了Benchmark^2，一种系统评估大型语言模型（LLM）测试基准质量的框架，包含三项指标：评估不同基准间模型排序的一致性、区分模型能力的强弱，以及检测测试题中异常表现的情况。通过对15个涵盖数学、推理和知识领域的基准，以及11个模型的广泛实验，作者发现现有基准质量参差不齐。基于该框架，能够有针对性地构建更高效的测试集，实现以更小规模获得与完整测试相当的评估效果，促进了LLM性能评价的科学性和实用性。,28
BEAVER: An Efficient Deterministic LLM Verifier,[翻译]BEAVER: An Efficient Deterministic LLM Verifier,Other,Other,https://arxiv.org/pdf/2512.05439,https://huggingface.co/papers/2512.05439,无法生成摘要。,27
OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value,OpenDataArena：一个公平开放的后训练数据集价值基准测试平台,LLM,Shanghai AI Lab,https://arxiv.org/pdf/2512.14051,https://huggingface.co/papers/2512.14051,本文提出了OpenDataArena（ODA），一个公开且公平的平台，用于系统评估大型语言模型后训练数据集的价值。ODA通过统一的训练与评测流程、多维度的数据质量评分、数据来源追踪工具以及开源工具包，促进数据的透明化和可复现性。基于120余个数据集和22个任务的广泛实验，ODA揭示了数据复杂度与模型性能的权衡关系，发现了常用基准数据的冗余，并绘制了数据集之间的关联图谱。该平台推动从经验式数据选择向科学化数据管理转变，为数据驱动的模型优化提供了重要支持。,27
SpotEdit: Selective Region Editing in Diffusion Transformers,SpotEdit：扩散Transformer中的选择性区域编辑,Diffusion Model,Other,https://arxiv.org/pdf/2512.22323,https://huggingface.co/papers/2512.22323,本文提出了SpotEdit，一种无需额外训练的图像编辑框架，专注于仅更新图像中被修改的区域。通过引入SpotSelector模块识别并跳过稳定未变区域的计算，以及SpotFusion模块动态融合编辑内容与原始特征，SpotEdit有效减少了冗余计算，同时保持未修改部分的高质量和上下文一致性。该方法提升了编辑效率和精度，为基于扩散变换器的图像局部编辑提供了更实用且高效的解决方案。,27
Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone,Dream-VL 与 Dream-VLA：基于扩散语言模型骨干的开放视觉-语言与视觉-语言-动作模型,Embodied AI,Other,https://arxiv.org/pdf/2512.22615,https://huggingface.co/papers/2512.22615,本文提出了基于扩散语言模型的新型视觉-语言模型Dream-VL及其扩展的视觉-语言-动作模型Dream-VLA。相比传统的自回归模型，这些扩散模型在视觉规划和机器人控制任务中表现出更优的性能和更快的训练收敛速度。Dream-VLA在多个公开机器人环境中实现了领先的成功率，展示了其在复杂视觉理解与动作生成中的潜力。作者同时开源了模型，推动相关领域的进一步研究与应用。,27
CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation,CoF-T2I：将视频模型作为纯视觉推理器应用于文本到图像生成,Diffusion Model,"PKU, ByteDance",https://arxiv.org/pdf/2601.10061,https://huggingface.co/papers/2601.10061,本文提出了CoF-T2I，一种将视频模型中的“逐帧推理”能力引入文本到图像生成的新方法。通过设计一系列中间生成步骤，模型能够逐步优化图像内容，实现更清晰且富有逻辑的视觉表达。为支持该过程，作者构建了包含生成轨迹的数据集，并采用独立编码策略提升图像质量与稳定性。实验结果表明，CoF-T2I在多个基准测试中表现优异，展示了视频模型在提升文本生成图像质量方面的巨大潜力。,27
Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation,Infinite-Homography作为摄像机控制视频生成的鲁棒条件,Diffusion Model,Other,https://arxiv.org/pdf/2512.17040,https://huggingface.co/papers/2512.17040,本文提出了InfCam，一种无需深度信息的摄像机控制视频生成框架，通过引入无限单应性变换在视频的潜在空间中直接编码三维旋转信息，实现了对摄像机运动的高精度控制。同时，设计了数据增强方法以丰富训练数据中的摄像机轨迹和焦距多样性。实验结果表明，InfCam在保持摄像机姿态准确性和视觉效果方面优于现有方法，且能有效从合成数据推广到真实场景，提升了动态场景中新视角视频生成的质量和实用性。,26
From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks,从模仿到辨别：面向跨领域推理任务的通用课程优势机制,LLM,"THU, PKU",https://arxiv.org/pdf/2512.02580,https://huggingface.co/papers/2512.02580,本文提出了CAPO，一种基于优势信号的自适应课程机制，用于提升大语言模型的强化学习效果。CAPO通过先利用正向优势样本进行模仿学习，建立稳固基础，再引入负向信号培养模型的辨别能力，从而有效改善模型在复杂跨领域推理任务中的泛化表现。该方法兼容多种优化算法，在数学推理和多模态图形界面推理任务中均表现出稳定且显著的提升，展示了其作为通用且鲁棒优化框架的潜力。,25
Scaling Zero-Shot Reference-to-Video Generation,Scaling Zero-Shot Reference-to-Video Generation（零样本参考图像到视频生成的可扩展方法）,Other,Meta,https://arxiv.org/pdf/2512.06905,https://huggingface.co/papers/2512.06905,本文提出了Saber，一种无需专门参考视频数据的零-shot参考图像到视频生成框架。通过仅利用视频与文本对进行训练，Saber采用掩码训练策略和定制的注意力机制，有效学习保持身份一致性的表示，同时引入掩码增强减少常见的拼贴伪影。该方法不仅显著降低了数据准备成本，还在公开基准测试中优于依赖显式参考数据训练的现有方法，展示了良好的泛化能力和实用价值。,25
MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos,MoCapAnything：基于单目视频的任意骨骼统一三维动作捕捉,Embodied AI,Other,https://arxiv.org/pdf/2512.10881,https://huggingface.co/papers/2512.10881,本文提出了MoCapAnything，一种基于单目视频和任意绑定的3D模型，实现通用运动捕捉的框架。该方法通过参考引导，先预测3D关节轨迹，再利用逆向运动学恢复模型特定的旋转动画，支持不同物种和多样骨架的动作迁移。系统包含三大模块，融合模型骨架、视频特征和时序信息，有效桥接视频与3D动作表示的差异。实验表明，MoCapAnything在多种场景下均能生成高质量骨骼动画，推动了面向任意3D资产的可扩展运动捕捉技术发展。,25
QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation,QuCo-RAG：基于预训练语料库不确定性量化的动态检索增强生成,LLM,Other,https://arxiv.org/pdf/2512.19134,https://huggingface.co/papers/2512.19134,本文提出了QuCo-RAG，一种基于预训练语料库统计数据来衡量生成过程中不确定性的方法，以减少大语言模型的虚假生成（幻觉）问题。该方法通过识别低频实体和验证实体共现频率，动态触发检索，提升了生成准确性。实验证明，QuCo-RAG在多跳问答任务中显著优于现有方法，且能有效迁移到不同模型和领域，展示了其稳健性和广泛适用性。该研究为动态检索增强生成提供了一种客观且模型无关的解决方案。,25
DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation,DreaMontage：任意帧引导的一次性视频生成,Diffusion Model,ByteDance,https://arxiv.org/pdf/2512.21252,https://huggingface.co/papers/2512.21252,本文提出了DreaMontage，一种基于任意帧引导的单次拍摄视频生成框架，旨在克服传统“一镜到底”拍摄成本高、限制多的问题。通过引入轻量级条件机制和自适应调优，提升了对任意帧的控制能力；结合高质量数据集与视觉表达微调，增强了视频的视觉效果和连贯性；设计分段自回归推理策略，有效支持长时序视频生成。实验表明，DreaMontage能高效生成视觉流畅、情感丰富且连贯的长时视频，为用户将零散素材转化为统一的电影级长镜头提供了创新解决方案。,25
UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision,UniCorn：通过自生成监督实现自我提升的统一多模态模型,Multimodal LLM,Other,https://arxiv.org/pdf/2601.03193,https://huggingface.co/papers/2601.03193,本文提出了UniCorn，一种无需外部数据或教师监督的自我提升框架，旨在解决统一多模态模型在理解与生成之间存在的差距（称为“传导失语”问题）。UniCorn通过将模型分为提议者、解答者和评判者三个角色，利用自我对弈和认知模式重构，将隐含理解转化为明确的生成信号。实验表明，UniCorn在多项图像生成基准测试中显著提升了生成质量，实现了多项指标的领先水平，展示了自监督方法在统一多模态智能中提升生成能力的潜力。,25
RelayLLM: Efficient Reasoning via Collaborative Decoding,RelayLLM：通过协同解码实现高效推理,LLM,Other,https://arxiv.org/pdf/2601.05167,https://huggingface.co/papers/2601.05167,本文提出了RelayLLM，一种通过令牌级别的协同解码实现高效推理的新框架。该方法让小型语言模型主动控制，仅在关键生成令牌时调用大型语言模型，从而显著降低计算成本。通过两阶段训练，模型学会在自主推理和寻求帮助间平衡。实验表明，RelayLLM在六个基准测试中将准确率提升至49.52%，同时仅调用大型模型1.07%的令牌，较传统方法节省了98.2%的计算资源，有效弥合了小型与大型模型性能差距。,25
"VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",VideoAuto-R1：通过一次思考，两次回答实现视频自动推理,Multimodal LLM,Meta,https://arxiv.org/pdf/2601.05175,https://huggingface.co/papers/2601.05175,本文提出了VideoAuto-R1框架，针对视频理解任务采用“必要时推理”的策略，结合“先思考一次，答复两次”的训练模式。模型先生成初始答案，再进行推理并输出复审答案，二者均通过可验证奖励进行监督。推理是否启动由初始答案的置信度决定。该方法在视频问答和定位任务中实现了领先的准确率，同时显著提升了效率，平均回答长度减少约3.3倍。实验还表明，语言推理对复杂任务有益，但在感知类任务中并非必需，体现了推理的选择性价值。,25
EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis,EnvScaler：通过程序合成扩展面向LLM智能体的工具交互环境,Agent,Other,https://arxiv.org/pdf/2601.05808,https://huggingface.co/papers/2601.05808,本文提出了EnvScaler，一种通过程序化合成自动生成多样化工具交互环境的框架，解决了真实系统访问受限、模拟环境不稳定及手工搭建难以扩展的问题。EnvScaler由环境骨架构建和场景生成两部分组成，能够批量合成丰富的任务场景及验证机制。实验表明，利用EnvScaler生成的环境对大语言模型进行监督微调和强化学习，显著提升了其在复杂多轮、多工具任务中的表现，推动了智能体在实际应用中的能力发展。,25
OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG,OpenDecoder：在RAG中融合文档质量的开放大语言模型解码方法,LLM,Other,https://arxiv.org/pdf/2601.09028,https://huggingface.co/papers/2601.09028,本文提出了OpenDecoder，一种改进的基于大语言模型的检索增强生成方法。该方法通过显式评估检索信息的质量，包括相关性、排序和查询性能预测得分，来提升生成内容的准确性和对噪声上下文的鲁棒性。实验结果表明，OpenDecoder在多个基准数据集上优于现有方法，且具备灵活集成外部质量指标和后训练调整的能力，推动了更可靠的信息检索与生成结合。,25
DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems,DoVer：基于干预驱动的LLM多智能体系统自动调试框架,Agent,Microsoft,https://arxiv.org/pdf/2512.06749,https://huggingface.co/papers/2512.06749,本文提出了DoVer，一种基于干预的自动调试框架，旨在提升大型语言模型多智能体系统的可靠性。传统仅依赖日志的调试方法存在假设未经验证和单步归因不准确的问题。DoVer通过主动干预（如修改消息或计划）验证失败假设，并以任务成功的实际进展作为评估标准。实验证明，DoVer能显著提升任务成功率并有效验证失败原因，展示了干预驱动调试在复杂多智能体系统中的实用性和推广潜力。,24
V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties,V-RGBX：基于内在属性精确控制的视频编辑,Other,Other,https://arxiv.org/pdf/2512.11799,https://huggingface.co/papers/2512.11799,本文提出了V-RGBX，一种首创的端到端视频编辑框架，能够准确理解和控制视频中的内在属性（如反照率、法线、材质和光照）。该方法结合视频逆向渲染、真实感合成及关键帧编辑，实现了物理合理且时间一致的视频编辑效果。用户通过编辑关键帧中的内在属性，V-RGBX能将修改自然地传播至整个视频序列，支持多样化应用如物体重纹理、材质调整及场景重新光照。实验结果表明，V-RGBX在保持视频真实感和一致性的同时，优于现有方法，推动了视频编辑技术的发展。,24
DeContext as Defense: Safe Image Editing in Diffusion Transformers,DeContext作为防御：扩散变换器中的安全图像编辑,Diffusion Model,Other,https://arxiv.org/pdf/2512.16625,https://huggingface.co/papers/2512.16625,本文提出了DeContext，一种针对基于扩散变换器的大规模图像编辑模型的防护方法。该方法通过在输入图像中注入细微扰动，削弱模型中的跨模态注意力机制，有效阻断未经授权的图像编辑，防止身份伪造和恶意篡改，同时保持图像的视觉质量。实验结果表明，DeContext在多种编辑场景下均能稳定阻止不当修改，展示了基于注意力扰动的防御策略在保护个人图像安全方面的潜力和实用价值。,24
Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection,Alchemist：通过元梯度数据选择提升文本到图像模型训练效率,Diffusion Model,Other,https://arxiv.org/pdf/2512.16905,https://huggingface.co/papers/2512.16905,本文提出了Alchemist，一种基于元梯度的自动数据选择框架，用于从大规模文本-图像数据中筛选高质量子集，从而提升文本生成图像模型的训练效率和视觉效果。通过训练一个轻量级评分模型评估样本对训练的影响，并结合多层次信息进行筛选，Alchemist能够有效剔除低质和冗余数据。实验表明，使用Alchemist选择的50%数据训练，模型性能优于使用全部数据，显著提高了数据利用率和训练稳定性，推动了文本到图像生成领域的数据效率优化。,24
GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts,GlimpRouter：通过观察一个思维标记实现高效协同推理,LLM,Other,https://arxiv.org/pdf/2601.05110,https://huggingface.co/papers/2601.05110,本文提出了GlimpRouter，一种无需额外训练的协同推理框架，通过仅观察推理步骤的首个生成词的熵值，判断该步骤的难度并决定是否调用大型模型处理。该方法有效降低了推理延迟和计算成本，同时保持甚至提升了准确率。实验结果显示，GlimpRouter在多个基准测试中显著提升了推理效率和性能，展示了基于“思想一瞥”进行计算资源分配的简洁且高效的推理策略。,24
Lost in the Noise: How Reasoning Models Fail with Contextual Distractors,迷失于噪声：推理模型如何在上下文干扰下失效,Agent,Other,https://arxiv.org/pdf/2601.07226,https://huggingface.co/papers/2601.07226,本文提出了NoisyBench，一个系统评估AI推理模型在面对多种噪声干扰（如无关文档和对话历史）时鲁棒性的基准。实验发现，当前最先进模型在含有上下文干扰时性能大幅下降，且多步骤推理流程往往加剧错误。传统的优化方法难以提升模型稳定性，而作者提出的“理据感知奖励”机制显著增强了模型识别有用信息的能力。此外，研究还揭示了计算资源增加反而导致噪声环境下性能恶化的现象，并通过注意力分析指出模型过度关注干扰信息。这些发现为构建更稳健的推理智能体提供了重要指导。,24
Composing Concepts from Images and Videos via Concept-prompt Binding,通过概念-提示绑定从图像和视频中合成概念,Diffusion Model,Other,https://arxiv.org/pdf/2512.09824,https://huggingface.co/papers/2512.09824,本文提出了Bind & Compose，一种能够灵活组合图像和视频中复杂视觉概念的一次性方法。该方法通过将视觉概念与对应的文本提示绑定，并采用分层结构和时间解耦策略，有效提升了不同来源概念的融合准确性和时序一致性。此外，设计的多样化吸收机制帮助消除无关细节干扰。实验结果表明，Bind & Compose在概念一致性、提示还原度和动态表现方面均优于现有方法，推动了视觉创作的创新与发展。,23
Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views,揭示隐藏的陷阱并从任务中心视角引导航向下一代向量相似性搜索,Other,Alibaba,https://arxiv.org/pdf/2512.12980,https://huggingface.co/papers/2512.12980,本文提出了Iceberg，一个面向实际应用场景的向量相似性搜索（VSS）综合评测套件。Iceberg从任务视角出发，揭示了影响搜索性能的三大关键因素：特征提取中的信息损失、距离度量与任务相关性不符，以及索引对数据分布变化的敏感性。通过覆盖图像分类、人脸识别、文本检索等多个领域的八个大规模数据集，Iceberg评估了13种先进方法，发现其应用层面表现与传统基于召回率和延迟的排名存在显著差异。论文还基于任务特征构建了决策树，为实际选择和调优VSS方法提供指导，促进理论研究与工业应用的有效结合。,23
REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion,REGLUE：结合全局与局部语义的纠缠扩散潜变量建模,Diffusion Model,Other,https://arxiv.org/pdf/2512.16636,https://huggingface.co/papers/2512.16636,本文提出了REGLUE，一种统一的潜空间扩散框架，通过同时融合图像潜变量、局部图像块级语义和全局图像特征，提升了图像生成的语义理解和训练效率。该方法利用轻量级的语义压缩器非线性整合多层视觉模型特征，并在扩散过程中与潜变量紧密结合，辅以外部对齐损失，增强了语义监督。实验表明，REGLUE在ImageNet图像生成任务中显著提高了生成质量和训练速度，验证了空间语义和非线性压缩在提升模型性能中的关键作用。,23
Are We on the Right Way to Assessing LLM-as-a-Judge?,我们是否走在正确的道路上评估LLM-as-a-Judge？,LLM,Other,https://arxiv.org/pdf/2512.16041,https://huggingface.co/papers/2512.16041,本文提出了Sage，一种无需人工标注、基于理性选择理论的新型评估工具，用于检测大型语言模型（LLM）作为评判者时的一致性和可靠性。通过局部偏好稳定性和全局逻辑一致性两种指标，Sage在包含结构化测试题和真实用户问题的数据集上验证了其评估的稳定性和有效性。研究发现，当前顶尖LLM在评判任务中存在显著一致性问题，且人类标注也存在较大不稳定性，表明传统人工标准存在局限。论文进一步指出微调和多模型联合评判能提升一致性，推动了更稳定可靠的自动评估体系的发展。,23
T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation,T2AV-Compass：面向文本到音视频生成的统一评估基准,Multimodal LLM,Other,https://arxiv.org/pdf/2512.21094,https://huggingface.co/papers/2512.21094,本文提出了T2AV-Compass，一个统一的评测基准，专门用于评价文本驱动的音视频生成系统。该基准包含500条丰富且复杂的测试指令，通过结合客观的音视频质量指标和主观的多模态大模型评审，全面衡量生成内容的视觉与听觉质量、跨模态一致性以及对指令的遵循度。对11个主流系统的评测结果显示，目前模型在真实感和多模态协调性方面仍存在显著差距，表明该基准为推动该领域技术进步提供了有力的诊断工具和挑战平台。,23
GARDO: Reinforcing Diffusion Models without Reward Hacking,GARDO：无奖励欺骗的扩散模型强化方法,Diffusion Model,Other,https://arxiv.org/pdf/2512.24138,https://huggingface.co/papers/2512.24138,本文提出了GARDO，一种针对扩散模型在线强化学习微调中“奖励劫持”问题的解决方案。传统方法因使用代理奖励导致模型生成质量下降和多样性丧失，且常规正则化限制了探索效率。GARDO通过选择性正则化高不确定样本、动态更新参考模型以及奖励多样性高且质量优的样本，有效平衡了样本效率、探索能力和防止奖励劫持。大量实验表明，GARDO显著提升了生成图像的质量和多样性，且兼容多种强化学习算法，展现出良好的稳定性和实用价值。,23
3AM: Segment Anything with Geometric Consistency in Videos,3AM：基于几何一致性的多视角视频分割方法,Other,Other,https://arxiv.org/pdf/2601.08831,https://huggingface.co/papers/2601.08831,本文提出了3AM，一种提升视频物体分割一致性的方法。通过在训练阶段将具备隐式几何信息的3D特征融合进现有的基于外观特征的分割模型，3AM实现了在大视角变化下的稳定物体识别。该方法仅需普通RGB图像作为输入，无需额外的相机位姿或深度信息，简化了实际应用流程。在多个具有挑战性的数据集上，3AM显著优于现有方法，展示了其在动态多视角场景中保持物体跨视图一致性的强大能力。,23
Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders,Think-Then-Generate：具备推理能力的基于大语言模型编码器的文本到图像扩散生成,Diffusion Model,Other,https://arxiv.org/pdf/2601.10332,https://huggingface.co/papers/2601.10332,本文提出了一种“先思考后生成”的文本到图像扩散模型新范式，通过让大语言模型（LLM）不仅作为文本编码器，还能对输入文本进行推理和重写，从而更准确地理解和表达文本含义。该方法通过轻量级微调和联合优化，使模型在生成图像时更好地结合世界知识和语义信息。实验结果显示，该方法显著提升了生成图像的事实一致性、语义对齐度和视觉真实感，表现接近先进的GPT-4o水平，推动了具备推理与表达能力的下一代统一生成模型的发展。,23
"EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",EMMA：基于统一架构的高效多模态理解、生成与编辑,Multimodal LLM,Other,https://arxiv.org/pdf/2512.04810,https://huggingface.co/papers/2512.04810,本文提出了EMMA，一种高效且统一的多模态架构，能够同时支持多模态的理解、生成与编辑任务。EMMA通过引入高压缩率的自动编码器、基于通道的特征融合、共享与独立结合的网络设计，以及专家混合机制，显著减少了计算资源需求并提升了性能。实验结果表明，EMMA在效率和效果上均超越了现有先进的统一多模态模型，展示了其在未来多模态技术发展中的重要潜力和应用价值。,22
OmniPSD: Layered PSD Generation with Diffusion Transformer,OmniPSD：基于扩散变换器的分层PSD生成,Diffusion Model,Other,https://arxiv.org/pdf/2512.09247,https://huggingface.co/papers/2512.09247,本文提出了OmniPSD，一种基于扩散模型的统一框架，能够实现从文本生成分层PSD文件以及从单张图像分解为可编辑PSD图层。该方法通过空间注意力机制学习图层间的组合关系，保持图层的语义一致性和层次结构，同时利用辅助模块有效保留透明度信息。实验表明，OmniPSD在生成高质量、结构清晰且透明度准确的分层设计文件方面表现出色，为图像设计的层级生成与分解提供了新的解决方案。,22
MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment,MentraSuite：用于心理健康推理与评估的大型语言模型后训练框架,LLM,Other,https://arxiv.org/pdf/2512.09636,https://huggingface.co/papers/2512.09636,本文提出了MentraSuite，一个针对心理健康领域设计的统一框架，旨在提升大语言模型在心理健康推理和评估中的可靠性。该框架包括MentraBench，一个涵盖多任务和多数据集的综合评测基准，评估模型的任务表现与推理质量；以及Mindora，一种通过结合监督微调与强化学习优化的后训练模型，能够生成连贯、一致且符合临床逻辑的推理过程。实验证明，Mindora在复杂心理健康场景中的表现优于多种现有模型，展现了其在心理健康辅助领域的应用潜力和价值。,22
Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure,Vector Prism：通过语义结构分层实现矢量图形动画,Multimodal LLM,Other,https://arxiv.org/pdf/2512.14336,https://huggingface.co/papers/2512.14336,本文提出了一种框架，通过整合多个弱预测结果，恢复矢量图形（SVG）的语义结构，从而实现更连贯的动画效果。该方法解决了现有视觉语言模型难以处理SVG中碎片化元素的问题，使模型能够将相关部分合理分组，提升动画的整体协调性。实验表明，该框架显著优于现有方法，表明语义结构的恢复是实现稳定且易于理解的SVG动画的关键。这为矢量图形的自动动画和人机交互提供了新的思路。,22
GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction,GaMO：面向稀疏视角三维重建的几何感知多视角扩散外延,Diffusion Model,Other,https://arxiv.org/pdf/2512.25073,https://huggingface.co/papers/2512.25073,本文提出了GaMO，一种面向稀疏视角3D重建的几何感知多视图扩展方法。与传统生成新视点不同，GaMO通过扩展现有视角的视野范围，提升场景覆盖度并保持几何一致性，有效缓解了视角稀少导致的重建质量下降问题。该方法无需额外训练，结合多视图条件和几何引导的去噪策略，实现了高质量的3D重建效果。实验证明，GaMO在多个数据集上优于现有技术，且计算效率提升显著，显著加快了处理速度。,22
AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction,AdaGaR：用于动态场景重建的自适应Gabor表示,Other,Other,https://arxiv.org/pdf/2601.00796,https://huggingface.co/papers/2601.00796,本文提出了AdaGaR，一种用于动态三维场景重建的自适应表示方法。该方法通过引入可学习的频率权重和能量补偿，有效提升了细节表现的同时保持稳定性；同时采用三次Hermite样条和时间曲率正则化，保证了运动的平滑连续性。此外，结合深度估计和点跟踪的自适应初始化策略，提升了训练初期的点云稳定性。实验结果表明，AdaGaR在多个指标上优于现有方法，且在视频插帧、深度一致性和视图合成等任务中表现出良好的泛化能力。,22
Deep Delta Learning,深度Delta学习,Other,Other,https://arxiv.org/pdf/2601.00417,https://huggingface.co/papers/2601.00417,本文提出了一种名为Deep Delta Learning的新型神经网络架构，旨在增强深度残差网络的表达能力。传统残差网络依赖恒等快捷连接，虽有效缓解梯度消失，但限制了网络对复杂状态变化的建模能力。该方法通过引入一个可学习的、数据驱动的几何变换来调节快捷连接，使网络能够灵活控制层间特征的更新方式，从而更好地捕捉复杂动态变化，同时保持训练的稳定性。实验表明，该方法在建模非单调和复杂动态任务中具有显著优势。,22
NitroGen: An Open Foundation Model for Generalist Gaming Agents,NitroGen：面向通用游戏智能体的开放基础模型,Agent,Other,https://arxiv.org/pdf/2601.02427,https://huggingface.co/papers/2601.02427,本文提出了NitroGen，一种面向通用游戏代理的视觉-动作基础模型。该模型通过自动从超过1000款游戏的4万小时公开游戏视频中提取玩家操作，构建了大规模视频-动作数据集，并在多游戏环境中进行训练和评测。NitroGen在不同类型的游戏任务中表现出强大的跨游戏适应能力和迁移效果，相较于从零训练的模型，任务成功率提升最高达52%。作者同时公开了数据集、评测工具和模型权重，推动通用智能体研究的发展。,22
AT^2PO: Agentic Turn-based Policy Optimization via Tree Search,AT²PO：基于树搜索的智能体回合制策略优化,Agent,Tencent,https://arxiv.org/pdf/2601.04767,https://huggingface.co/papers/2601.04767,本文提出了AT²PO，一种针对多轮交互任务的强化学习框架，通过引入基于回合的树搜索结构，提升了探索多样性和奖励分配的精细度，有效解决了传统方法中探索不足、奖励稀疏及策略优化不匹配的问题。该框架设计了回合级别的学习目标，使策略更新更符合多轮决策的自然节奏。实验结果表明，AT²PO在多个基准测试中显著优于现有方法，验证了其各组成部分的有效性。该方法可灵活集成于多轮强化学习系统，推动了智能体在复杂交互任务中的表现提升。,22
RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation,RoboVIP：基于视觉身份提示的多视角视频生成增强机器人操作,Embodied AI,"Shanghai AI Lab, THU",https://arxiv.org/pdf/2601.05241,https://huggingface.co/papers/2601.05241,本文提出了一种名为RoboVIP的方法，通过引入视觉身份提示（即使用示例图像作为条件）指导多视角视频生成，提升机器人操作数据的多样性和质量。该方法克服了现有基于文本提示的图像生成技术在多视角一致性和场景准确性上的不足，构建了一个可扩展的视觉身份库用于数据增强。利用增强后的数据训练的机器人视觉与动作策略，在模拟和真实环境中均表现出更优的性能，推动了机器人操作任务的学习效率和泛化能力。,22
Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking,Qwen3-VL-Embedding与Qwen3-VL-Reranker：面向最先进多模态检索与排序的统一框架,Multimodal LLM,Alibaba,https://arxiv.org/pdf/2601.04720,https://huggingface.co/papers/2601.04720,本文提出了Qwen3-VL-Embedding和Qwen3-VL-Reranker两款模型，构建了一个端到端的多模态检索与排序框架。该框架通过多阶段训练和跨模注意力机制，将文本、图像、文档图像及视频等多种数据类型映射到统一的表示空间，实现高精度的多模态搜索。模型支持多语言和灵活的嵌入维度，表现出色，在多项多模态检索基准测试中取得领先成绩，展示了其在图文检索、视觉问答和视频文本匹配等任务中的广泛应用潜力。,22
OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent,OS-Symphony：一个面向稳健且通用计算机使用智能体的整体框架,Agent,Shanghai AI Lab,https://arxiv.org/pdf/2601.07779,https://huggingface.co/papers/2601.07779,本文提出了OS-Symphony，一种面向计算机操作任务的综合智能框架，旨在提升在长时间、多步骤任务中的稳定性和跨领域适应能力。该框架通过引入反思记忆机制，实现对任务过程的自我纠正，减少视觉信息丢失；并配备多模态搜索工具，能够实时生成与视觉环境高度匹配的操作指导，解决了在新场景下的执行准确性问题。实验表明，OS-Symphony在多个在线测试中表现优异，显著超越现有方法，展示了其在通用自动化任务中的潜力和实用价值。,22
OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding,OpenVoxel：用于开放词汇3D场景理解的无训练体素分组与描述,Multimodal LLM,Other,https://arxiv.org/pdf/2601.09575,https://huggingface.co/papers/2601.09575,本文提出了OpenVoxel，一种无需训练即可实现稀疏体素分组与描述的算法，支持开放词汇的三维场景理解。通过利用多视角图像生成的稀疏体素表示，OpenVoxel能够有效地将场景中的不同物体进行分组，并借助视觉语言模型和多模态大语言模型为每个分组生成文字描述，构建信息丰富的场景地图。该方法不依赖传统文本编码器的嵌入，直接进行文本间搜索，显著提升了开放词汇分割和复杂指代表达分割任务的表现，且实现简洁高效，具备良好的应用前景。,22
"The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",WorldCanvas：利用参考图像、轨迹和文本绘制可提示事件的世界画布,Multimodal LLM,Other,https://arxiv.org/pdf/2512.16924,https://huggingface.co/papers/2512.16924,本文提出了WorldCanvas，一种结合文本描述、运动轨迹和参考图像的多模态框架，用于生成连贯且可控的虚拟世界事件。该方法通过融合运动信息、语义意图和视觉参考，实现了多主体互动、物体进出场及外观变化等复杂场景的生成，保证了视频的时间连续性和物体身份的一致性。WorldCanvas将传统被动的世界模型转变为用户可交互、可定制的模拟工具，推动了虚拟环境生成技术的发展。,21
Spatia: Video Generation with Updatable Spatial Memory,Spatia：具有可更新空间记忆的视频生成,Other,Microsoft,https://arxiv.org/pdf/2512.15716,https://huggingface.co/papers/2512.15716,本文提出了Spatia，一种基于空间记忆的视频生成框架，通过维护和动态更新三维场景点云，实现了视频在空间和时间上的长期一致性。该方法将静态场景作为持久的空间记忆，与动态元素分离生成，提升了视频的真实感和连贯性。此外，Spatia支持显式的摄像机控制和三维交互式编辑，拓展了视频生成的应用场景。该框架为实现可扩展且具备记忆能力的视频生成提供了新的思路。,21
MAI-UI Technical Report: Real-World Centric Foundation GUI Agents,MAI-UI技术报告：面向真实世界的基础GUI智能体,Agent,Alibaba,https://arxiv.org/pdf/2512.22047,https://huggingface.co/papers/2512.22047,本文提出了MAI-UI，一套涵盖多种规模的基础图形用户界面（GUI）智能代理，旨在推动人机交互的革新。针对实际应用中的交互缺失、操作局限、部署架构不足及环境适应性差等挑战，MAI-UI采用自我进化的数据处理流程、设备与云端协同系统及在线强化学习框架，有效提升了导航与界面理解能力。实验结果显示，MAI-UI在多个GUI基准测试和移动导航任务中均创下新纪录，显著优于现有模型，同时提升了设备性能并保障用户隐私，具备广泛的应用潜力。,21
"GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models",GRAN-TED：为扩散模型生成鲁棒、对齐且细腻的文本嵌入,Diffusion Model,"PKU, ByteDance",https://arxiv.org/pdf/2512.15560,https://huggingface.co/papers/2512.15560,本文提出了GRAN-TED，一种用于扩散模型的文本编码新方法，旨在提升生成内容的语义准确性和细腻度。为解决现有文本编码器评估困难和预训练模型适配难题，作者设计了TED-6K，一个高效且可靠的文本编码质量评测基准，显著加快了性能验证过程。基于此，提出了两阶段训练策略，结合多模态大语言模型微调和层级加权技术，获得更强的文本特征表达能力。实验表明，GRAN-TED在文本到图像及视频生成任务中均实现了性能提升，推动了视觉生成模型的文本理解能力发展。,21
Nested Learning: The Illusion of Deep Learning Architectures,嵌套学习：深度学习架构的幻象,LLM,Other,https://arxiv.org/pdf/2512.24695,https://huggingface.co/papers/2512.24695,本文提出了一种名为“嵌套学习”的新范式，将机器学习模型视为由多层次、并行优化问题组成的系统，每层拥有独立的上下文信息流。该方法揭示了现有深度学习在压缩自身上下文信息中实现学习的本质，解释了大模型中“上下文学习”的自然产生。论文的核心贡献包括：重新理解常用优化器为信息压缩模块，设计更具表现力的优化算法；提出能自我修改更新规则的序列模型；以及构建统一的记忆系统，突破传统长短期记忆的限制。结合这些创新，作者开发了持续学习模块“Hope”，在语言建模和持续学习等任务中表现出良好潜力，推动了机器学习模型的自适应和持续进化能力。,21
VINO: A Unified Visual Generator with Interleaved OmniModal Context,VINO：一种具有交错全模态上下文的统一视觉生成器,Diffusion Model,Other,https://arxiv.org/pdf/2601.02358,https://huggingface.co/papers/2601.02358,本文提出了VINO，一种统一的视觉生成器，能够在单一框架下实现图像和视频的生成与编辑。VINO通过共享的扩散模型骨干，结合文本、图像和视频等多模态输入，支持多参考依据、长指令理解及身份一致性维护。其多阶段训练策略使模型逐步具备处理多任务和多模态的能力。实验结果表明，VINO在视觉质量、指令执行、参考保持及多身份编辑控制方面表现优异，展示了统一视觉生成的可行路径和广泛应用潜力。,21
The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents,置信度二分法：工具使用智能体中的误校准分析与缓解,Agent,Other,https://arxiv.org/pdf/2601.07264,https://huggingface.co/papers/2601.07264,本文系统分析了基于大语言模型的自主代理在使用不同类型工具时的置信度校准问题，发现证据类工具（如网页搜索）易导致过度自信，而验证类工具（如代码解释器）则有助于减少误差。为提升各类工具下的校准效果，作者提出了一种结合任务准确率和置信度优化的强化学习微调框架，并通过多样化奖励设计验证其有效性。实验表明，该方法不仅显著改善了代理的置信度表达，还具备良好的跨环境和跨领域泛化能力。研究为构建能够可靠传达不确定性的自主智能体奠定了基础。,21
PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling,PaCo-RL：结合成对奖励建模推进一致性图像生成的强化学习,Other,Other,https://arxiv.org/pdf/2512.04784,https://huggingface.co/papers/2512.04784,本文提出了PaCo-RL框架，结合专门设计的成对一致性奖励模型和高效的强化学习算法，提升多图像间的一致性生成效果。通过自动构建的大规模数据集训练的PaCo-Reward模块，能够更准确地评估图像间的视觉一致性；而PaCo-GRPO优化策略则显著降低训练成本并稳定奖励优化。实验证明，PaCo-RL在保持图像身份、风格和逻辑连贯性方面表现优异，展示了其在故事叙述和角色设计等领域应用的潜力和实用价值。,20
An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges,视觉-语言-行动模型剖析：从模块到里程碑与挑战,Embodied AI,Other,https://arxiv.org/pdf/2512.11362,https://huggingface.co/papers/2512.11362,本文系统梳理了视觉-语言-动作（VLA）模型的发展脉络，重点分析了该领域的五大核心挑战：表示、执行、泛化、安全性及数据集与评估。通过从基本模块出发，结合关键里程碑，论文全面呈现了VLA模型在机器人领域实现感知与行动闭环、扩展多样环境适应能力及保障可靠部署的关键路径。该综述不仅为新入门研究者提供清晰的学习框架，也为资深学者指明未来研究方向，助力推动具身智能的发展。,20
Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction,大语言模型能否评估学生的学习困难？基于能力模拟的人机难度对齐与题目难度预测,LLM,Other,https://arxiv.org/pdf/2512.18880,https://huggingface.co/papers/2512.18880,本文针对大型语言模型（LLMs）在预测题目难度时能否准确反映学生认知困难进行了大规模实证研究。结果表明，尽管模型具备强大解题能力，但其难度估计与人类认知存在系统性偏差，且模型规模扩大并未改善这一现象。模型难以模拟学生的能力限制，且缺乏对自身局限性的反思能力。研究揭示了当前语言模型在自动预测教育题目难度方面的挑战，提示其通用解题能力并不等同于理解人类学习困难。,20
WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion,WorldWarp：基于异步视频扩散的3D几何传播,Diffusion Model,Other,https://arxiv.org/pdf/2512.19678,https://huggingface.co/papers/2512.19678,本文提出了WorldWarp，一种结合三维几何缓存与二维扩散模型的新框架，用于生成长时序且几何一致的视频。通过在线维护基于高斯点云的三维结构缓存，WorldWarp能将历史画面准确映射到新视角，确保结构连贯性。针对遮挡导致的空洞和伪影，设计了时空变化噪声调度的扩散模型，实现“填充与修正”功能。该方法动态更新三维缓存，有效保持视频片段间一致性，显著提升了视频的几何合理性和纹理质量，推动了长距离新视角视频合成的发展。,20
Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models,超越记忆：一个多模态序数回归基准以揭示视觉-语言模型中的流行度偏差,Multimodal LLM,Other,https://arxiv.org/pdf/2512.21337,https://huggingface.co/papers/2512.21337,本文揭示了当前视觉-语言模型在建筑年代预测任务中存在显著的“人气偏差”，即模型对知名建筑的预测准确率比普通建筑高出约34%，显示其更多依赖记忆而非普适理解。为系统研究这一问题，作者构建了涵盖157个国家、超过5.5万张建筑图片及其建造年份、地理位置和人气指标的YearGuessr数据集，并将建造年份预测任务设计为序数回归。基于该数据集，提出了考虑人气影响的评价指标，对30余个模型进行了评测，验证了视觉-语言模型在熟悉对象上表现优异但在不熟悉对象上能力不足，暴露了其推理能力的局限。,20
"UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",UniPercept：面向美学、质量、结构与纹理的统一感知层次图像理解,Multimodal LLM,"Shanghai AI Lab, PKU, THU",https://arxiv.org/pdf/2512.21675,https://huggingface.co/papers/2512.21675,本文提出了UniPercept，一种统一的感知层面图像理解框架，涵盖美学、质量、结构和纹理等关键视觉特征。作者构建了层级定义体系和大规模数据集，设计了强有力的基线模型，通过领域自适应预训练和任务对齐强化学习，实现了在视觉评分和视觉问答任务中的稳健泛化。UniPercept显著优于现有多模态大模型，且可作为文本生成图像的奖励模型使用。该工作为多模态环境下的感知层面图像理解提供了系统化的评测基准和方法基础，推动了相关领域的发展。,20
Act2Goal: From World Model To General Goal-conditioned Policy,Act2Goal：从世界模型到通用目标条件策略,Embodied AI,Other,https://arxiv.org/pdf/2512.23541,https://huggingface.co/papers/2512.23541,本文提出了Act2Goal，一种结合视觉世界模型与多尺度时间控制的机器人操作策略，用于解决长时间跨度的复杂操作任务。该方法通过生成一系列中间视觉状态，规划任务进展，并利用多尺度时间哈希技术实现细粒度的闭环控制与全局任务一致性。Act2Goal具备良好的零样本泛化能力，能适应新物体和环境，并支持无需奖励的在线自主优化。实际机器人实验显示，Act2Goal显著提升了复杂任务的成功率，验证了其在长时操作中的鲁棒性和实用价值。,20
InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams,InfiniteVGGT：面向无限流的视觉几何基础变换器,Other,Other,https://arxiv.org/pdf/2601.02281,https://huggingface.co/papers/2601.02281,本文提出了InfiniteVGGT，一种基于因果变换器的视觉几何理解模型，能够实现对无限长视频流的连续三维几何估计。该方法通过引入自适应的滚动记忆机制，有效管理和更新关键缓存，避免了传统流式方法在长序列中出现的性能衰退问题。为验证模型的长期稳定性，作者还设计了首个包含约1万帧的Long3D基准测试，提供了持续三维几何理解的评测平台。实验结果表明，InfiniteVGGT在无限时域的流式处理任务中显著优于现有方法，推动了实时大规模三维视觉理解的发展。,20
ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition,ROI-Reasoning：通过预计算元认知实现推理的理性优化,LLM,Other,https://arxiv.org/pdf/2601.03822,https://huggingface.co/papers/2601.03822,本文提出了一种名为ROI-Reasoning的推理优化框架，帮助大型语言模型在有限计算资源下更合理地分配推理预算。该方法通过两阶段训练，首先让模型学会预测每个任务的计算成本和潜在收益，从而判断是否继续推理；随后利用强化学习优化整体计算分配策略，实现多任务间的高效权衡。实验表明，ROI-Reasoning在严格的计算限制下显著提升了模型的推理效果和资源利用效率，展现了模型的预算感知和元认知能力。,20
Alterbute: Editing Intrinsic Attributes of Objects in Images,Alterbute：图像中对象内在属性的编辑,Diffusion Model,Other,https://arxiv.org/pdf/2601.10714,https://huggingface.co/papers/2601.10714,本文提出了Alterbute，一种基于扩散模型的方法，用于编辑图像中物体的内在属性，如颜色、纹理、材质和形状，同时保持物体的身份特征和场景背景不变。该方法通过宽松的训练目标和细粒度的视觉命名实体，实现对身份特征的精准识别与保护，避免了传统方法中身份丢失或编辑受限的问题。实验表明，Alterbute在保持物体身份的前提下，能够更灵活且真实地修改物体的内在属性，提升了图像编辑的质量和实用性。,20
Transition Matching Distillation for Fast Video Generation,用于快速视频生成的Transition Matching Distillation,Diffusion Model,Other,https://arxiv.org/pdf/2601.09881,https://huggingface.co/papers/2601.09881,本文提出了一种名为Transition Matching Distillation（TMD）的新方法，用于将复杂的视频扩散模型压缩为高效的少步生成器。该方法通过匹配扩散模型的多步去噪过程与少步概率转移过程，利用条件流模型简化生成步骤，同时分解模型结构以提升蒸馏效率。实验表明，TMD在保持视频质量和文本提示一致性的同时，大幅提升了生成速度，优于现有蒸馏模型，适合实时交互式视频生成应用。,20
Universal Reasoning Model,Universal Reasoning Model（通用推理模型）,AGI,Other,https://arxiv.org/pdf/2512.14693,https://huggingface.co/papers/2512.14693,本文针对复杂推理任务（如ARC-AGI和数独）中的Universal Transformer（UT）模型进行了系统分析，发现其性能提升主要源于模型内在的递归归纳偏置和强非线性计算，而非复杂的架构设计。基于此，作者提出了Universal Reasoning Model（URM），通过引入短卷积和截断反向传播来强化递归特性，有效提升了推理能力。在ARC-AGI基准测试中，URM分别实现了53.8%和16.0%的领先准确率，显著优于现有方法，展示了其在深度迭代推理中的潜力和实用价值。,19
JustRL: Scaling a 1.5B LLM with a Simple RL Recipe,JustRL：使用简单强化学习方案扩展15亿参数大语言模型,LLM,"THU, Shanghai AI Lab",https://arxiv.org/pdf/2512.16649,https://huggingface.co/papers/2512.16649,本文提出了JustRL，一种简化的强化学习训练方法，仅采用单阶段训练和固定超参数，即可在两个15亿参数的推理模型上实现领先性能，平均准确率分别达到54.9%和64.3%，且计算资源消耗仅为复杂方法的一半。该方法训练过程稳定，表现持续提升，无需复杂的调参或辅助技巧，反而发现某些常用策略可能削弱探索效果。研究表明，当前领域对复杂训练流程的依赖或许并非必要，JustRL为社区提供了一个简单且有效的基线方案。,19
MemEvolve: Meta-Evolution of Agent Memory Systems,MemEvolve：智能体记忆系统的元进化,Agent,Other,https://arxiv.org/pdf/2512.18746,https://huggingface.co/papers/2512.18746,本文提出了MemEvolve，一种元进化框架，能够同时进化智能体的经验知识和记忆结构，突破了传统记忆系统设计固定、难以适应多样任务的限制。通过引入统一的代码库EvolveLab，整合多种记忆系统并提供标准化测试平台，MemEvolve在多个复杂任务上显著提升了智能体性能（最高提升17.06%），并展现出良好的跨任务和跨模型泛化能力。该方法为智能体自我进化和记忆系统优化提供了新思路，推动了更灵活高效的智能体发展。,19
SOP: A Scalable Online Post-Training System for Vision-Language-Action Models,SOP：一种面向视觉-语言-动作模型的可扩展在线后训练系统,Embodied AI,Other,https://arxiv.org/pdf/2601.03044,https://huggingface.co/papers/2601.03044,本文提出了一种可扩展的在线后训练系统SOP，旨在提升视觉-语言-动作（VLA）模型在真实机器人任务中的表现。该系统通过多机器人协同工作，实时收集操作经验和人工反馈，结合云端集中学习，实现多任务模型的在线更新和优化。实验覆盖折叠衣物、组装箱子和补货等多种操作，结果表明SOP能在数小时内显著提升模型性能，且性能提升与机器人数量近线性相关。该方法有效解决了传统离线、单机或单任务训练的局限，为通用机器人策略的高效、可靠和大规模应用提供了新路径。,19
Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models,少量Token关键：基于熵的视觉-语言模型对抗攻击,Multimodal LLM,Other,https://arxiv.org/pdf/2512.21815,https://huggingface.co/papers/2512.21815,本文揭示了视觉语言模型在生成过程中，少数约20%的高不确定性关键位置对最终输出影响最大。通过针对这些关键位置进行有针对性的对抗攻击，研究者在大幅减少攻击资源的同时，实现了与全面攻击相当的语义破坏效果。该方法在多种模型间表现出较强的迁移能力，能将大量正常输出转变为有害结果，暴露了视觉语言模型潜在的安全风险。基于此，论文提出了一种新的基于不确定性引导的攻击策略，显著提升了攻击成功率，揭示了当前模型安全机制的不足。,19
Can We Predict Before Executing Machine Learning Agents?,我们能在执行机器学习智能体之前进行预测吗？,Agent,Other,https://arxiv.org/pdf/2601.05930,https://huggingface.co/papers/2601.05930,本文针对自动化机器学习代理在执行过程中存在的高昂计算成本问题，提出了一种“预测-验证”框架。通过引入内部执行先验，利用大语言模型在执行前对方案效果进行预测，显著减少了对实际运行的依赖。作者构建了包含1.8万余对方案比较的数据集，验证了模型在预测准确性和置信度校准方面的有效性。基于此，开发的代理系统FOREAGENT实现了比传统执行方法快6倍的收敛速度，同时性能提升6%。该方法为提升机器学习自动化效率提供了新思路。,19
Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models,超越硬掩码：用于扩散语言模型的渐进式Token演化,Diffusion Model,Other,https://arxiv.org/pdf/2601.07351,https://huggingface.co/papers/2601.07351,本文提出了EvoToken-DLM，一种基于扩散模型的语言生成方法。该方法摒弃了传统扩散语言模型中使用的硬二值掩码，采用逐步演化的软性词元分布，实现了从模糊状态到离散输出的平滑过渡，支持生成过程中的可修正性。通过引入连续轨迹监督，训练目标更好地契合了迭代概率更新。实验结果表明，EvoToken-DLM在多个基准测试中显著优于现有的扩散和掩码语言模型，提升了生成质量和效率。,19
Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding,Molmo2：用于视频理解与定位的视觉-语言模型开放权重与数据,Multimodal LLM,Other,https://arxiv.org/pdf/2601.10611,https://huggingface.co/papers/2601.10611,本文提出了Molmo2，一套开源的视频-语言模型及其训练数据集，专注于提升视频理解与像素级定位能力。Molmo2构建了7个新视频数据集和2个多图像数据集，涵盖详细视频描述、视频问答、复杂目标追踪及视频指点任务，且未依赖任何闭源模型。通过创新的训练方法和编码策略，Molmo2在多个视频理解和定位任务上超越了现有开源模型，且在部分任务上优于部分闭源模型，推动了视频语言模型在开源领域的发展与应用。,19
Distribution Matching Variational AutoEncoder,分布匹配变分自编码器,Diffusion Model,"PKU, Tencent",https://arxiv.org/pdf/2512.07778,https://huggingface.co/papers/2512.07778,本文提出了一种名为Distribution-Matching VAE（DMVAE）的新型变分自编码器方法，通过显式对齐编码器的潜在分布与任意参考分布，打破了传统VAE仅使用高斯先验的限制。该方法允许利用自监督学习特征或其他先验分布，提升潜在空间的结构合理性和建模效率。实验证明，DMVAE在保持高质量图像重建的同时，大幅提升了生成效果，在ImageNet数据集上仅用64个训练周期即可达到优异性能。研究表明，选择合适的潜在分布结构是实现高保真图像合成的关键。,18
"EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",EgoEdit：用于第一人称视频编辑的数据集、实时流式模型与基准测试,Embodied AI,Other,https://arxiv.org/pdf/2512.06065,https://huggingface.co/papers/2512.06065,本文提出了EgoEdit，一个专为第一人称视角视频编辑设计的实时系统。针对该领域存在的快速视角变化和频繁手部互动等挑战，作者构建了包含丰富手物交互的专用数据集EgoEditData，开发了支持单GPU实时推理的编辑模型EgoEdit，并设计了综合评测基准EgoEditBench。该系统在保证编辑指令准确执行、手部细节保留及视频时序稳定性的同时，实现了低延迟交互，显著优于现有方法，推动了交互式增强现实中的第一人称视频编辑技术发展。,18
ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models,ThreadWeaver：用于语言模型高效并行推理的自适应线程框架,LLM,Meta,https://arxiv.org/pdf/2512.07843,https://huggingface.co/papers/2512.07843,本文提出了ThreadWeaver，一种适应性并行推理框架，旨在提升大型语言模型的推理效率。通过并行生成推理路径、基于字典树的训练与推理协同设计，以及并行感知的强化学习，ThreadWeaver在保持与传统顺序推理模型相当准确率的同时，显著降低了推理延迟。实验证明，该方法在多个复杂数学推理任务中实现了高准确率和最高1.53倍的速度提升，开辟了准确性与效率之间的新平衡，为实际应用中的快速智能推理提供了有效方案。,18
N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models,N3D-VLM：原生3D定位实现视觉-语言模型中的精准空间推理,Multimodal LLM,Tencent,https://arxiv.org/pdf/2512.16561,https://huggingface.co/papers/2512.16561,本文提出了N3D-VLM，一种将原生3D感知与视觉语言模型结合的统一框架，能够基于文本描述准确定位三维空间中的物体，并进行明确的空间关系推理。该方法突破了传统仅依赖二维图像的限制，实现了更精确和可解释的三维空间理解。为支持模型训练，作者设计了一个大规模数据构建流程，将二维标注通过深度估计转换为三维数据，显著提升了三维物体定位数据的规模和多样性。实验结果表明，N3D-VLM在三维定位和空间推理任务上均优于现有方法，展现出强大的三维视觉语言理解能力。,18
DreamStyle: A Unified Framework for Video Stylization,DreamStyle：一个统一的视频风格化框架,Diffusion Model,ByteDance,https://arxiv.org/pdf/2601.02785,https://huggingface.co/papers/2601.02785,本文提出了DreamStyle，一种统一的视频风格化框架，支持文本、风格图像和首帧三种风格条件，突破了现有方法单一条件的限制。通过设计高质量的数据整理流程和采用低秩适应训练策略，DreamStyle有效提升了风格一致性和视频的时间连贯性，减少了闪烁现象。实验证明，该方法在多种风格化任务中表现优异，显著优于现有技术，推动了视频风格化技术的发展与应用。,18
MiMo-V2-Flash Technical Report,MiMo-V2-Flash 技术报告,LLM,Other,https://arxiv.org/pdf/2601.02780,https://huggingface.co/papers/2601.02780,本文介绍了MiMo-V2-Flash，一种拥有3090亿参数的稀疏专家混合模型，结合滑动窗口与全局注意力机制，实现高效推理和强大推理能力。模型在27000亿词语上预训练，支持最长256k上下文。通过创新的多教师在线蒸馏方法，模型能够有效吸收领域专家知识，性能媲美参数更多的顶级开源模型。推理时利用多标记预测技术显著提升速度和生成长度。论文同时开源了模型权重，促进社区研究与合作。,18
AgentOCR: Reimagining Agent History via Optical Self-Compression,AgentOCR：通过光学自压缩重新构想智能体历史,Agent,Alibaba,https://arxiv.org/pdf/2601.04786,https://huggingface.co/papers/2601.04786,本文提出了AgentOCR，一种通过将多轮交互历史压缩为视觉图像来减少大语言模型代理系统中令牌消耗的新方法。该方法利用视觉信息的高密度优势，通过分段光学缓存避免重复渲染，并引入代理自适应压缩机制，在保持任务性能的同时显著降低了令牌使用和内存需求。实验证明，AgentOCR在多个复杂任务上能保持超过95%的性能，令牌消耗减少超过50%，且渲染速度提升20倍，显著提升了代理系统的效率和可扩展性。,18
An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift,在领域迁移下偏好调优的泛化性与多样性实证研究,LLM,Other,https://arxiv.org/pdf/2601.05882,https://huggingface.co/papers/2601.05882,本文系统研究了语言模型在领域变化下的偏好调优（preference tuning）泛化能力及其多样性表现。通过比较五种主流的对齐目标和多种适应策略（包括目标领域的监督微调和伪标签方法），在文本摘要和问答任务中评估模型性能。结果表明，不同对齐目标在领域转移时表现出显著差异，且基于伪标签的适应策略能有效缓解领域变化带来的性能下降。该研究为提升语言模型在新领域中的实用性和鲁棒性提供了重要实证依据。,18
Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction,可控内存使用：在长期人机交互中平衡锚定与创新,Agent,Other,https://arxiv.org/pdf/2601.05107,https://huggingface.co/papers/2601.05107,本文提出了一种可控记忆使用框架SteeM，解决了基于大语言模型的智能代理在长期交互中记忆依赖的“全有或全无”问题。该框架引入了记忆依赖的行为度量，允许用户动态调节代理对历史信息的依赖程度，从而在创新和历史忠实之间实现平衡。实验表明，SteeM在多种场景下优于传统的固定记忆策略，提升了个性化人机协作的灵活性和效果。该方法为长期人机交互中的记忆管理提供了更细致且有效的控制手段。,18
MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching,MatchTIR：通过二分匹配实现工具集成推理的细粒度监督,LLM,Other,https://arxiv.org/pdf/2601.10712,https://huggingface.co/papers/2601.10712,本文提出了MatchTIR，一种针对工具集成推理任务的细粒度监督框架。该方法通过将推理过程中的每一步与真实标准进行匹配，实现对每个交互步骤的精细奖励分配，克服了传统方法中对整个推理轨迹统一奖励的不足。MatchTIR还引入了结合局部步骤和整体任务成功的双层优势估计，有效区分有效与冗余的工具调用。实验结果表明，MatchTIR在多个基准测试中显著提升了模型表现，尤其在长序列、多轮推理任务中表现优异。,18
ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback,ToolSafe：通过主动的步骤级护栏与反馈提升基于LLM的智能体工具调用安全性,Agent,"Shanghai AI Lab, PKU",https://arxiv.org/pdf/2601.10156,https://huggingface.co/papers/2601.10156,本文提出了一种针对基于大语言模型（LLM）代理工具调用安全的新方法。作者构建了TS-Bench基准，用于检测代理在每一步调用工具时的安全风险；并设计了TS-Guard模型，通过多任务强化学习，能够在工具执行前主动识别潜在危险操作，给出可解释的安全反馈。同时，基于此模型，提出TS-Flow推理框架，有效减少了恶意工具调用行为，提升了任务完成率。该方法在应对恶意输入攻击时显著增强了LLM代理的安全性和可靠性，促进其在实际环境中的安全部署。,18
Relational Visual Similarity,关系视觉相似性,Multimodal LLM,Other,https://arxiv.org/pdf/2512.07833,https://huggingface.co/papers/2512.07833,本文提出了一种新的视觉相似性度量——关系视觉相似性，旨在捕捉图像中元素之间的内在关系，而非仅基于表面属性。作者构建了一个包含11.4万条匿名描述关系逻辑的图像-文本数据集，并基于此微调视觉-语言模型，实现了对图像关系相似性的有效测量。该方法突破了现有视觉相似性模型仅关注外观特征的局限，首次使计算机能够识别和比较图像中的抽象关系结构，填补了视觉计算领域的重要空白，具有广泛的应用潜力。,17
Thinking with Images via Self-Calling Agent,通过自调用智能体进行图像思考,Agent,Other,https://arxiv.org/pdf/2512.08511,https://huggingface.co/papers/2512.08511,本论文提出了一种名为Self-Calling Chain-of-Thought（sCoT）的新型视觉推理方法，将传统的多模态推理任务转化为仅基于语言的推理过程。通过将复杂任务拆解为子任务，并由共享参数的子代理独立解决，sCoT显著提升了训练效率和效果，避免了多模态信息交织带来的优化难题。实验结果表明，sCoT在视觉推理基准测试中相较于现有方法，性能提升约1.9%，且训练所需计算资源减少约75%。该方法为高效的视觉推理提供了新的思路和实践路径。,17
IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning,IC-Effect：基于上下文学习的精确高效视频特效编辑框架,Diffusion Model,Other,https://arxiv.org/pdf/2512.15635,https://huggingface.co/papers/2512.15635,本文提出了IC-Effect，一种基于指令引导的视频特效编辑框架，能够在保持视频空间和时间一致性的同时，精确地合成复杂视觉效果如火焰、粒子和卡通角色。通过利用源视频作为上下文条件，结合两阶段训练策略和稀疏时空编码，IC-Effect实现了对背景的完美保留与自然特效注入。该方法在有限的配对数据下表现出强大的效果建模能力和高效计算性能，显著提升了视频特效编辑的质量和可控性，推动了视频创作技术的发展。,17
A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers,基于协作变换器的操作系统日志中点异常与集体异常检测统一框架,Other,Other,https://arxiv.org/pdf/2512.23380,https://huggingface.co/papers/2512.23380,本文提出了CoLog，一种基于协作式变换器的新型日志异常检测框架，能够同时识别操作系统日志中的点异常和集体异常。CoLog通过融合多种日志信息类型，利用多头注意力机制和模态适应层，有效捕捉不同信息间的交互关系，提升异常检测的准确性。实验结果表明，CoLog在七个基准数据集上表现优异，平均准确率、召回率和F1值均超过99.6%。该方法为日志异常检测提供了统一且高效的解决方案，适用于网络安全和系统监控等领域。,17
On the Role of Discreteness in Diffusion LLMs,离散性在扩散大语言模型中的作用,Diffusion Model,Other,https://arxiv.org/pdf/2512.22630,https://huggingface.co/papers/2512.22630,本文探讨了扩散模型在语言生成中的应用，重点分析了文本的离散性对扩散过程的影响。作者总结了扩散机制与语言建模需求之间的五个关键差异，并将现有方法分为连续嵌入空间扩散和离散词元扩散两类，指出两者各自存在结构性权衡。通过对大型扩散语言模型的分析，发现均匀扰动忽视了信息在不同位置的分布，且逐词训练难以捕捉多词依赖关系。研究结果强调了设计更符合文本结构的扩散过程的重要性，为未来构建更连贯的扩散语言模型提供了方向。,17
DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving,DrivingGen：自动驾驶中生成式视频世界模型的综合基准测试,Agent,Other,https://arxiv.org/pdf/2601.01528,https://huggingface.co/papers/2601.01528,本文提出了DrivingGen，这是首个针对自动驾驶生成视频世界模型的综合评测基准。该基准通过多样化的数据集和新设计的评测指标，全面衡量模型在视觉真实感、轨迹合理性、时间一致性及可控性等方面的表现，弥补了现有评测在安全关键因素和多样性覆盖上的不足。通过对14个先进模型的测试，揭示了视觉质量与物理合理性之间的权衡。DrivingGen为推动可靠且可控的驾驶世界模型发展提供了统一的评测框架，助力自动驾驶的仿真、规划和数据驱动决策。,17
Parallel Context-of-Experts Decoding for Retrieval Augmented Generation,用于检索增强生成的并行专家上下文解码,LLM,Other,https://arxiv.org/pdf/2601.08670,https://huggingface.co/papers/2601.08670,本文提出了一种名为Parallel Context-of-Experts Decoding（PCED）的无训练框架，用于提升检索增强生成模型在多文档推理中的表现。该方法将每个检索到的文档视为独立“专家”，通过一种新的对比解码策略，在生成过程中动态整合各专家的预测结果，从而实现跨文档信息的有效融合。PCED避免了传统长文本拼接带来的计算瓶颈和推理困难，同时无需构建共享注意力机制，显著提升了多文档推理的效率和准确性。,17
Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge,Openpi Comet：2025 BEHAVIOR挑战赛竞赛方案,Embodied AI,Other,https://arxiv.org/pdf/2512.10071,https://huggingface.co/papers/2512.10071,本文提出了Openpi Comet方案，针对2025年BEHAVIOR挑战赛中的复杂家务任务，展示了基于大规模预训练和后续训练技术的有效策略。该方案在模拟环境中实现了长时间、多步骤的机器人操作，显著优于其他参赛作品，获得第二名。通过系统性实验，作者验证了训练阶段规模扩展对性能提升的重要性，并总结了实用经验与设计建议，为未来将强大基础模型应用于复杂机器人任务提供了有价值的参考。,16
Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning,Skyra：基于有据可依的伪影推理的AI生成视频检测,Multimodal LLM,THU,https://arxiv.org/pdf/2512.15693,https://huggingface.co/papers/2512.15693,本文提出了Skyra，一种专门针对AI生成视频的多模态大语言模型，能够识别视频中的视觉伪影并基于此进行检测与解释。为支持模型训练，作者构建了首个大规模带有细粒度人工标注的AI视频伪影数据集ViF-CoT-4K，并设计了两阶段训练策略以提升模型的时空伪影感知和解释能力。通过包含多种生成器产出样本的ViF-Bench评测，Skyra在检测准确性和可解释性上均优于现有方法，推动了AI生成视频检测技术的发展。,16
Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition,Qwen-Image-Layered：通过图层分解实现内在可编辑性,Diffusion Model,Alibaba,https://arxiv.org/pdf/2512.15603,https://huggingface.co/papers/2512.15603,本文提出了Qwen-Image-Layered，一种基于扩散模型的图像分层方法，能够将单张RGB图像分解为多个语义独立的RGBA图层，实现对每个图层的独立编辑而不影响其他内容。该方法引入了统一RGB与RGBA表示的编码器、支持可变层数分解的网络结构及多阶段训练策略，有效提升了分解质量和一致性。为解决训练数据不足，作者构建了从Photoshop文件中提取多层图像的流程。实验表明，该方法显著优于现有技术，为图像编辑提供了新的高效解决方案。,16
RadarGen: Automotive Radar Point Cloud Generation from Cameras,RadarGen：基于摄像头的汽车雷达点云生成,Diffusion Model,Other,https://arxiv.org/pdf/2512.17897,https://huggingface.co/papers/2512.17897,本文提出了RadarGen，一种基于扩散模型的方法，能够从多视角摄像头图像生成逼真的汽车雷达点云。通过将雷达数据以鸟瞰图形式表示，并结合深度、语义和运动信息，RadarGen提升了生成点云的物理合理性。该方法兼容现有视觉数据集和仿真平台，支持多模态传感器数据的统一生成。实验证明，RadarGen生成的雷达点云在统计特性和感知模型性能上接近真实数据，为自动驾驶感知系统的多传感器模拟提供了有效途径。,16
LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry,LoGoPlanner：基于定位的导航策略与度量感知视觉几何,Embodied AI,Other,https://arxiv.org/pdf/2512.19629,https://huggingface.co/papers/2512.19629,本文提出了LoGoPlanner，一种端到端导航框架，旨在提升移动机器人在复杂环境中的路径规划能力。该方法通过结合定位、环境几何重建和策略调控，利用视觉信息实现准确的自我定位和细致的环境感知，从而增强障碍物规避和规划稳定性。实验结果表明，LoGoPlanner在模拟和真实场景中均显著降低了累计误差，较传统基于定位的方案提升超过27%，且具备良好的跨平台和环境适应性。该框架为移动机器人导航提供了更高效且鲁棒的解决方案。,16
Web World Models,Web World Models（网页世界模型）,Agent,Other,https://arxiv.org/pdf/2512.23676,https://huggingface.co/papers/2512.23676,本文提出了Web World Models（WWMs），一种结合传统网页技术与大型语言模型的新型环境构建方法。WWMs通过网页代码实现世界状态和规则，保证逻辑一致性，同时利用语言模型生成情境、叙事和决策，实现既可控又开放的持久环境。作者构建了多种基于真实网络技术的示例系统，验证了该方法在可扩展性和灵活性上的优势。该研究为构建可持续、动态且结构化的智能代理环境提供了新的思路和实践框架。,16
DiRL: An Efficient Post-Training Framework for Diffusion Language Models,DiRL：一种高效的扩散语言模型后训练框架,LLM,Other,https://arxiv.org/pdf/2512.22234,https://huggingface.co/papers/2512.22234,本文提出了DiRL，一种高效的扩散语言模型（dLLM）后训练框架，通过结合加速的分块训练和优化的推理机制，实现了在线模型更新和两阶段后训练（监督微调与强化学习）。基于此框架，作者设计了首个针对dLLM的无偏策略优化方法DiPO。实验证明，DiRL-8B-Instruct在数学推理任务上表现优异，超越了同类模型，展示了该方法在提升复杂任务性能和计算效率方面的显著价值。,16
CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving,CogFlow：通过知识内化桥接感知与推理的视觉数学问题求解,Multimodal LLM,THU,https://arxiv.org/pdf/2601.01874,https://huggingface.co/papers/2601.01874,本文提出了CogFlow，一种受认知启发的三阶段框架，旨在提升多模态大语言模型在视觉数学问题解决中的表现。该框架通过感知、知识内化和推理三个环节，有效桥接视觉信息提取与后续推理过程，确保视觉线索被准确整合和利用。为此，作者设计了协同视觉奖励机制和视觉门控策略优化，增强模型对视觉信息的理解与推理的关联性。此外，论文还贡献了包含12万条高质量标注的MathCog数据集。实验结果表明，CogFlow在视觉数学推理任务中表现优越，显著提升了模型的准确性和推理一致性。,16
"A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",关于GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro和Seedream 4.5的安全性报告,Multimodal LLM,Other,https://arxiv.org/pdf/2601.10527,https://huggingface.co/papers/2601.10527,本文针对七款前沿大语言模型和多模态模型（包括GPT-5.2、Gemini 3 Pro等）进行了统一的安全性能评估，涵盖语言理解、视觉语言融合及图像生成等多种应用场景。研究发现，各模型在安全表现上存在显著差异，且在面对对抗性测试时普遍表现出较大脆弱性。GPT-5.2表现较为均衡且稳定，而其他模型则在基准安全性、多语言适应性和合规性等方面存在权衡。结果强调了安全评估的多维度特性，呼吁建立标准化的安全测试体系，以更全面地衡量模型风险，促进其负责任的开发与应用。,16
PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution,PACEvolve：实现长远进展感知的一致进化,Agent,Other,https://arxiv.org/pdf/2601.10657,https://huggingface.co/papers/2601.10657,本文提出了PACEvolve框架，系统解决了基于大型语言模型（LLM）的进化搜索中常见的三大问题：历史信息干扰、搜索陷入局部最优和协作效率低下。通过引入层级上下文管理、动量回溯机制和自适应采样策略，PACEvolve有效提升了搜索过程的稳定性和探索能力，实现了长期持续的自我优化。实验结果表明，该方法在多个基准测试中取得了领先性能，展示了其在复杂优化任务中的广泛应用潜力。,16
FlowAct-R1: Towards Interactive Humanoid Video Generation,FlowAct-R1：迈向交互式类人视频生成,Diffusion Model,ByteDance,https://arxiv.org/pdf/2601.10103,https://huggingface.co/papers/2601.10103,本文提出了FlowAct-R1，一种专为实时交互式类人视频生成设计的框架。该方法基于MMDiT架构，结合分段扩散强制策略，有效减少误差积累，保证长时间交互中的时序一致性。通过系统优化和知识蒸馏，FlowAct-R1实现了480p分辨率下稳定25fps的实时视频生成，响应延迟仅约1.5秒。该框架支持全身细粒度控制，使生成的虚拟人能自然切换多种行为状态，表现出高度的行为生动性和视觉真实感，同时具备良好的风格泛化能力，适用于多样化交互场景。,16
Multi-view Pyramid Transformer: Look Coarser to See Broader,多视角金字塔变换器：粗看以见广,Other,Other,https://arxiv.org/pdf/2512.07806,https://huggingface.co/papers/2512.07806,本文提出了多视角金字塔变换器（MVP），一种高效且可扩展的多视角3D重建架构。MVP通过结合两层层次结构——从局部到全局的视角扩展和从细节到粗略的特征聚合，实现了对大规模复杂场景的快速重建。该方法能在单次前向传播中处理数十至数百张图像，显著提升了计算效率和重建质量。实验结果表明，MVP在多种数据集上均达到先进的重建性能，同时具备良好的通用性和扩展能力。,15
From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models,从宏观到微观：通过视觉-语言模型对分子微观空间智能的基准测试,Multimodal LLM,"THU, PKU, Alibaba",https://arxiv.org/pdf/2512.10867,https://huggingface.co/papers/2512.10867,本论文提出了“微观空间智能”（MiSI）概念，指理解和推理微观不可见空间关系的能力，对科学发现至关重要。为评估视觉-语言模型（VLMs）在该领域的表现，作者设计了包含约16.3万问答对和58.7万图像的MiSI-Bench基准，涵盖九项任务，测试从基本空间变换到复杂分子关系的理解能力。实验表明，现有顶尖模型整体表现低于人类，但经过微调的模型在空间变换任务上表现优异，显示出潜力；同时在科学知识密集型任务上表现不足，强调了结合专业领域知识的重要性。该工作为推动科学人工智能的发展提供了重要基准和数据资源。,15
Stronger Normalization-Free Transformers,更强的无归一化Transformer,Other,Other,https://arxiv.org/pdf/2512.10938,https://huggingface.co/papers/2512.10938,本文提出了一种新的点式函数Derf，用于替代传统的归一化层，在多种任务上表现优于现有方法。Derf基于高斯累积分布函数，通过大规模搜索设计出最优形式，实现了更稳定的训练和更好的泛化能力。实验覆盖视觉识别与生成、语音表示及DNA序列建模等领域，结果显示Derf不仅性能优异且结构简单，适合构建无需归一化层的Transformer模型，推动了深度学习架构的简化与性能提升。,15
Robust and Calibrated Detection of Authentic Multimedia Content,真实多媒体内容的鲁棒且校准的检测,Other,Other,https://arxiv.org/pdf/2512.15182,https://huggingface.co/papers/2512.15182,本论文针对当前深度伪造内容检测面临的高误报率和易被对抗攻击的问题，提出了一种基于重合成的验证框架。该方法能够更可靠地确认多种类型媒体内容的真实性，同时有效控制误报率，并具备对资源受限攻击者的鲁棒性。通过利用先进的逆向技术，本文的方法在保证高精度的同时，显著提升了检测的稳定性和安全性，克服了传统检测手段在伪造内容高度逼真和对抗性强的环境下的局限，为多模态媒体真实性鉴别提供了实用且稳健的解决方案。,15
UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models,UCoder：通过大语言模型内部探测实现无监督代码生成,LLM,Other,https://arxiv.org/pdf/2512.17385,https://huggingface.co/papers/2512.17385,本文提出了一种名为IPC的无监督代码生成框架，通过内部探测大型语言模型的知识和置信度模式，实现无需任何标注数据或外部代码库的代码生成。IPC通过多维度探查模型内部状态，筛选高质量代码候选，并基于此训练出UCoder。实验结果表明，该方法在多个代码基准测试中表现出与监督学习方法相当的性能，同时显著降低了对标注数据和计算资源的依赖。研究揭示了模型内部状态中蕴含的丰富代码质量信号，为资源受限环境下的代码生成模型训练提供了新思路。,15
HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming,HiStream：通过消除冗余的流式处理实现高效高分辨率视频生成,Diffusion Model,Meta,https://arxiv.org/pdf/2512.21338,https://huggingface.co/papers/2512.21338,本文提出了HiStream，一种高效的高分辨率视频生成框架，通过系统性地减少空间、时间和步骤上的冗余，显著提升了生成速度。具体方法包括先在低分辨率下去噪再细化高分辨率细节、分块处理视频帧并利用缓存保证速度稳定，以及减少后续分块的去噪步骤。实验表明，HiStream在1080p视频生成任务中实现了最高视觉质量，同时相比现有方法加速最高达107倍，且几乎无质量损失。该方法有效解决了高分辨率视频生成的计算瓶颈，推动了其实用化和规模化发展。,15
UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement,UltraShape 1.0：通过可扩展几何细化实现高保真三维形状生成,Diffusion Model,PKU,https://arxiv.org/pdf/2512.21185,https://huggingface.co/papers/2512.21185,本文提出了UltraShape 1.0，一种用于高质量三维形状生成的可扩展扩散框架。该方法采用两阶段生成流程，先构建粗略结构，再进行细节精炼，从而生成精细的三维几何形状。为提升数据质量，作者设计了新颖的数据处理流程，修复模型缺陷并过滤低质样本。通过将空间定位与细节合成分离，模型能够专注于局部几何细节的生成。仅基于公开数据集训练，UltraShape 1.0在生成效果和数据处理质量上均表现出竞争力，推动了自动化高保真三维内容生成的发展。,15
VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control,VerseCrafter：基于4D几何控制的动态真实视频世界模型,Diffusion Model,Tencent,https://arxiv.org/pdf/2601.05138,https://huggingface.co/papers/2601.05138,本文提出了VerseCrafter，一种基于四维几何控制的新型视频世界模型，实现了对摄像机和多目标运动的统一且精确控制。通过结合静态背景的点云表示和每个物体的三维高斯轨迹，模型能够捕捉物体的空间路径及其随时间变化的概率分布，超越了传统的刚性框或参数化模型限制。借助自动数据引擎从真实视频中提取四维控制信号，VerseCrafter在大规模多样化数据上训练，生成高质量且视角一致的动态视频，显著提升了对复杂场景中多目标运动的模拟能力。,15
ExpSeek: Self-Triggered Experience Seeking for Web Agents,ExpSeek：面向网页智能体的自触发经验寻求,Agent,Alibaba,https://arxiv.org/pdf/2601.08605,https://huggingface.co/papers/2601.08605,本文提出了ExpSeek，一种基于自触发机制的经验获取方法，提升了网页代理在多轮交互中的表现。与传统方法被动地全局注入经验不同，ExpSeek通过利用模型内部的不确定性指标，主动判断何时介入经验，并针对每一步设计定制化内容。实验证明，该方法在多个基准测试中显著提升了性能，且即使是小规模经验模型也能有效增强大型模型的能力，展示了主动经验寻求在动态环境中的应用潜力。,15
SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations,SCAIL：通过上下文学习3D一致性姿态表示实现工作室级角色动画,Diffusion Model,THU,https://arxiv.org/pdf/2512.05905,https://huggingface.co/papers/2512.05905,本文提出了SCAIL框架，旨在实现符合电影制作标准的高质量角色动画。通过引入一种新颖的三维姿态表示和基于扩散-变换器的全局姿态注入机制，SCAIL能够更准确地捕捉复杂动作和多角色互动中的时空信息，显著提升动画的结构一致性和时间连贯性。结合多样且高质量的数据处理流程，实验结果表明该方法在动画真实感和稳定性方面达到领先水平，为角色动画的工业应用提供了有力支持。,14
Sliding Window Attention Adaptation,滑动窗口注意力适配,LLM,Other,https://arxiv.org/pdf/2512.10411,https://huggingface.co/papers/2512.10411,本文提出了一种名为Sliding Window Attention Adaptation（SWAA）的方法，旨在使基于Transformer的大型语言模型在无需重新训练的情况下，采用滑动窗口注意力机制以提升长文本处理效率。该方法通过结合五种策略（如仅在预填充阶段使用滑动窗口、保留关键“汇聚”标记、交替使用全注意力和滑动窗口层、链式思维以及微调），有效缓解了训练与推理不匹配导致的性能下降问题。实验结果表明，SWAA能够在保持长文本理解能力的同时，大幅降低计算复杂度，具有重要的实用价值和广泛应用前景。,14
RecGPT-V2 Technical Report,RecGPT-V2 技术报告,LLM,Other,https://arxiv.org/pdf/2512.14503,https://huggingface.co/papers/2512.14503,RecGPT-V2通过引入层级多智能体系统、混合表示推理、动态提示生成和约束强化学习，显著提升了推荐系统的效率、解释多样性、泛化能力和用户偏好匹配度。相比前代版本，它减少了60%的计算资源消耗，提升了推荐准确率和解释接受度。在线A/B测试显示，RecGPT-V2在点击率、浏览量和转化率等关键指标上均有显著提升，验证了其在大规模工业环境中应用的技术可行性和商业价值。,14
ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement,ShowTable：通过协作反思与精炼解锁创意表格可视化,Multimodal LLM,Other,https://arxiv.org/pdf/2512.13303,https://huggingface.co/papers/2512.13303,本文提出了ShowTable，一种结合多模态大语言模型（MLLMs）与扩散模型的创新管线，专注于将数据表格转化为既美观又准确的信息图表。通过多轮自我纠正的协作流程，ShowTable能有效规划视觉方案并修正生成错误，显著提升图表的质量和数据映射的精确度。为支持该任务，作者构建了自动化数据生成流程及包含800个实例的评测基准TableVisBench。实验结果表明，ShowTable在多模态推理和生成表现上均优于现有方法，推动了复杂数据可视化领域的发展。,14
MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives,MemFlow：用于一致且高效长视频叙事的流动自适应记忆,Other,Other,https://arxiv.org/pdf/2512.14699,https://huggingface.co/papers/2512.14699,本文提出了MemFlow，一种针对长视频生成中保持内容连贯性和生成效率的动态记忆机制。MemFlow通过根据当前视频片段的文本提示，动态检索并更新相关历史帧，确保故事叙述的连贯性和场景切换的合理性。同时，在生成过程中仅激活与当前内容最相关的记忆信息，显著提升计算效率。实验表明，MemFlow在保证长时序一致性的同时，仅带来极小的计算开销，且可兼容现有流式视频生成模型，推动了长视频自动生成技术的发展。,14
GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation,GroundingME：通过多维度评估揭示多模态大语言模型中的视觉定位差距,Multimodal LLM,PKU,https://arxiv.org/pdf/2512.17495,https://huggingface.co/papers/2512.17495,本文提出了GroundingME基准测试，系统评估多模态大语言模型（MLLMs）在视觉定位任务中的真实能力。该测试从区分相似物体、理解空间关系、处理遮挡和微小物体，以及识别无法定位的查询四个维度设计，包含1005个复杂样本。对25个主流模型的评测显示，当前模型在复杂视觉定位任务中表现有限，尤其在拒绝无效查询方面准确率极低，存在安全隐患。论文还探讨了通过推理路径选择和混合训练数据两种策略提升模型性能的可能性。GroundingME为揭示和改进MLLM视觉理解能力提供了重要工具和方向。,14
Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding,Robust-R1：面向鲁棒视觉理解的降质感知推理框架,Multimodal LLM,Other,https://arxiv.org/pdf/2512.17532,https://huggingface.co/papers/2512.17532,本文提出了Robust-R1，一种针对视觉退化问题的多模态大语言模型框架。该方法通过显式建模视觉退化过程，结合有监督微调、基于奖励的参数感知校准及动态调整推理深度，有效提升模型对真实世界复杂视觉退化的鲁棒性。为支持该方法，作者构建了包含多阶段真实退化及结构化推理链标注的11K样本数据集。实验证明，Robust-R1在多个真实退化基准上显著优于现有通用及鲁棒模型，展现出更强的抗退化性能和解释能力。,14
"Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",Nemotron 3 Nano：用于智能推理的开放高效混合Mamba-Transformer专家混合模型,LLM,Other,https://arxiv.org/pdf/2512.20848,https://huggingface.co/papers/2512.20848,本文介绍了Nemotron 3 Nano 30B-A3B，一种高效的混合专家模型，结合了Mamba-Transformer架构，支持长达100万令牌的上下文处理。该模型在预训练阶段使用了250亿条文本数据，经过监督微调和大规模强化学习，显著提升了推理速度和准确性。相比前代模型，Nemotron 3 Nano在激活更少参数的情况下，推理速度提升至3.3倍，并在多个基准测试中表现优异，特别是在推理和对话能力上表现突出。作者同时公开了模型权重和训练资源，促进开放研究和应用。,14
Recursive Language Models,递归语言模型,LLM,Other,https://arxiv.org/pdf/2512.24601,https://huggingface.co/papers/2512.24601,本文提出了一种名为递归语言模型（RLMs）的新方法，旨在突破大型语言模型的上下文长度限制。通过将长文本视为外部环境，模型能够程序化地分解并递归处理文本片段，从而有效应对远超自身上下文窗口规模的输入。实验表明，RLMs在多种长文本任务中显著提升了生成质量，且推理成本与传统方法相当甚至更低，展示了其在处理超长文本和复杂任务中的潜力与优势。,14
VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction,VideoAR：通过下一帧与尺度预测的自回归视频生成,Other,Other,https://arxiv.org/pdf/2601.05966,https://huggingface.co/papers/2601.05966,本文提出了VideoAR，一种结合多尺度下一帧预测与自回归建模的视频生成框架。该方法通过分离空间和时间依赖关系，并利用三维多尺度编码器高效捕捉时空动态，有效提升了视频的时序一致性和生成效率。通过多阶段预训练和创新的误差校正机制，VideoAR显著减少了推理步骤，实现了在UCF-101数据集上优于现有自回归模型的性能，并在视频质量评测中达到与更大规模扩散模型相当的水平。该工作为高效且稳定的视频生成提供了新的可扩展基础。,14
MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era,MegaFlow：面向智能体时代的大规模分布式编排系统,Agent,Alibaba,https://arxiv.org/pdf/2601.07526,https://huggingface.co/papers/2601.07526,本文提出了MegaFlow，一种面向智能代理训练的分布式调度系统，解决了大规模复杂任务中代理与环境交互协调的基础设施难题。MegaFlow通过将训练架构划分为模型、代理和环境三大独立服务，实现灵活资源分配和独立扩展，支持数万个代理任务的并发执行，确保系统稳定性和资源高效利用。该系统填补了当前开源平台在大规模智能代理训练与评估方面的空白，推动了智能代理时代基础设施的发展。,14
Boosting Latent Diffusion Models via Disentangled Representation Alignment,通过解耦表示对齐提升潜在扩散模型,Diffusion Model,Other,https://arxiv.org/pdf/2601.05823,https://huggingface.co/papers/2601.05823,本文针对潜在扩散模型（LDM）中图像编码器——变分自编码器（VAE）与生成模型对表示的不同需求，提出了语义解耦变分自编码器（Send-VAE）。该方法通过非线性映射将VAE潜在空间与预训练视觉基础模型的语义层级对齐，提升了VAE对细粒度属性信息的结构化表达能力。实验表明，Send-VAE在属性预测任务中表现出更好的语义解耦能力，显著加速了基于该编码器的生成模型训练，并在ImageNet 256×256图像生成中实现了领先的生成质量指标（FID）。,14
What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models,用户未言明的内容：欠明确查询限制视觉-语言模型,Multimodal LLM,Other,https://arxiv.org/pdf/2601.06165,https://huggingface.co/papers/2601.06165,本论文针对视觉语言模型（VLM）在处理真实用户提出的含糊、不完整的问题时表现不佳的问题，构建了HAERAE-Vision数据集，收集了653条来自韩国网络社区的真实视觉问答及其明确重写版本。实验发现，即使是最先进的模型在原始含糊查询上的准确率不足50%，而将查询明确化后性能显著提升8至22个百分点。研究进一步表明，现有的检索技术难以弥补用户表达不足带来的影响，揭示了当前评测与实际应用之间的重要差距。该工作强调了完善用户查询表达对提升VLM实用性的关键作用。,14
"M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",M4olGen：基于精确多属性约束的多智能体多阶段分子生成,AI4Science,Other,https://arxiv.org/pdf/2601.10131,https://huggingface.co/papers/2601.10131,本文提出了M4olGen，一种基于分子片段的两阶段生成框架，用于在多个物理化学性质约束下精确设计分子。第一阶段通过多智能体推理结合检索机制生成初步分子原型，第二阶段采用强化学习优化细化分子结构，精准满足多属性数值目标。该方法依托大规模自动构建的数据集，实现了可控、多步分子编辑和数值调控。实验结果表明，M4olGen在多属性约束下生成分子的有效性和精确度均优于现有大型语言模型及图模型，推动了多目标分子设计的应用发展。,14
HeartMuLa: A Family of Open Sourced Music Foundation Models,HeartMuLa：一系列开源音乐基础模型,Multimodal LLM,Other,https://arxiv.org/pdf/2601.10547,https://huggingface.co/papers/2601.10547,本文介绍了一套开源的音乐基础模型——HeartMuLa，涵盖音频与文本对齐、歌词识别、高保真音乐编码及基于大型语言模型的歌曲生成。该系统支持多样化任务和多模态输入，具备细粒度的音乐风格控制及适合短视频背景音乐的生成能力。HeartMuLa在参数规模扩大至70亿时性能显著提升，实现了以学术资源复现商业级音乐生成系统的突破。这些模型为未来音乐理解与生成研究提供了坚实基础，推动多模态内容创作的实际应用。,14
Action100M: A Large-scale Video Action Dataset,Action100M：大规模视频动作数据集,Multimodal LLM,Meta,https://arxiv.org/pdf/2601.10592,https://huggingface.co/papers/2601.10592,本文介绍了Action100M，一个由1.2百万互联网教学视频构建的大规模视频动作数据集，涵盖约1亿个带时间定位的动作片段及丰富的开放式文本描述。该数据集通过全自动流程生成，结合先进的视觉嵌入和语言推理模型，实现多层次结构化注释。基于Action100M训练的模型在多个动作识别任务中表现出显著的规模效应和强大的零样本泛化能力。Action100M为视频理解和物理世界建模研究提供了重要的数据基础，推动了开放领域动作识别的发展。,14
Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning,作为稳定强化学习软全局约束的熵比率裁剪,LLM,Other,https://arxiv.org/pdf/2512.05591,https://huggingface.co/papers/2512.05591,本文针对强化学习中策略更新过程中因分布变化导致的不稳定问题，提出了一种基于策略熵比率的新型约束机制——熵比率裁剪（ERC）。该方法通过衡量当前策略与先前策略在探索行为上的整体变化，施加双向限制，从而稳定全局策略更新，弥补了传统方法对未采样动作概率变化调控的不足。将ERC集成到多种强化学习算法中，实验证明其在多个任务上均显著提升了训练的稳定性和性能表现。,13
UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation,UnityVideo：统一多模态多任务学习以增强具世界感知的视频生成,Diffusion Model,Other,https://arxiv.org/pdf/2512.07831,https://huggingface.co/papers/2512.07831,本文提出了UnityVideo，一种统一的多模态多任务视频生成框架，通过融合分割掩码、人体骨架、DensePose、光流和深度图等多种信息，实现对视频内容的全面理解和生成。该方法引入动态噪声和模态切换机制，有效整合不同训练范式，提升模型的收敛速度和对新场景的零样本泛化能力。实验表明，UnityVideo在视频质量、一致性及物理约束符合度方面表现优异，推动了更具世界感知能力的视频生成技术发展。相关代码和数据已公开。,13
Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos,通过人体视频实现视觉-物理对齐的空间感知VLA预训练,Embodied AI,PKU,https://arxiv.org/pdf/2512.13080,https://huggingface.co/papers/2512.13080,本文提出了一种空间感知的视觉-语言-动作（VLA）预训练方法，通过将二维视觉信息与三维物理动作进行显式对齐，提升机器人对三维空间的理解能力。该方法利用大规模人类示范视频，提取三维视觉和动作标注，弥合了视觉感知与物理动作之间的差距。基于此，作者设计了VIPA-VLA双编码器架构，融合三维视觉特征，显著增强了机器人在实际任务中的视觉与动作关联性，推动了更稳健且泛化能力强的机器人策略学习。,13
Feedforward 3D Editing via Text-Steerable Image-to-3D,通过文本可控图像到3D的前馈编辑,Other,Other,https://arxiv.org/pdf/2512.13678,https://huggingface.co/papers/2512.13678,本文提出了Steer3D，一种基于文本指令对AI生成的3D模型进行快速编辑的方法。该方法借鉴ControlNet架构，通过自动生成大规模训练数据并结合两阶段训练策略，实现了在单次前向传播中用语言指导3D资产的修改。相比现有技术，Steer3D不仅编辑效果更符合文本要求，且保持了原始模型的整体一致性，速度提升显著（快2.4到28.5倍）。该工作展示了将文本作为新模态成功融入预训练图像到3D生成模型的可能性，推动了3D内容创作的便捷性和实用性。,13
SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning,SAGE：利用强化学习训练智能任意时长智能体以实现长视频推理,Agent,Other,https://arxiv.org/pdf/2512.13874,https://huggingface.co/papers/2512.13874,本文提出了SAGE，一种模仿人类灵活观看视频方式的多轮推理系统，能够根据任务需求选择性地浏览长视频或完整观看短视频。通过引入基于合成数据的训练方法和强化学习策略，SAGE的核心组件SAGE-MM获得了跨不同视频时长的推理能力。作者还构建了包含长视频的评测集SAGE-Bench，用于真实场景下的视频理解测试。实验结果显示，SAGE在开放式视频推理任务上相比现有方法提升显著，尤其在超过10分钟长的视频中表现更优，展示了其在长视频理解中的实用价值。,13
"Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",语言模型物理学：第4.1部分，架构设计与Canon层的魔力,LLM,Meta,https://arxiv.org/pdf/2512.17351,https://huggingface.co/papers/2512.17351,本文提出了一种名为“Canon层”的轻量级架构组件，旨在提升语言模型在推理深度和广度上的能力。通过设计受控的合成预训练任务，作者系统地评估了模型的核心能力，发现Canon层通过促进相邻词之间的信息流动，显著增强了模型的知识处理和推理表现。该组件可无缝集成于多种序列模型架构中，并在合成任务与大规模真实预训练中均表现出优越性能。研究为理解和改进语言模型架构提供了新的思路，并为未来模型设计和训练优化指明了方向。,13
NVIDIA Nemotron 3: Efficient and Open Intelligence,NVIDIA Nemotron 3：高效且开放的智能,LLM,Other,https://arxiv.org/pdf/2512.20856,https://huggingface.co/papers/2512.20856,本文介绍了NVIDIA Nemotron 3系列模型，包括Nano、Super和Ultra三种规格，具备强大的推理和对话能力。该系列采用混合专家架构，实现了高效的推理速度和超长上下文处理（最高可达100万标记）。其中，Super和Ultra模型引入了新颖的技术提升质量和生成速度。所有模型通过多环境强化学习进行后期训练，支持复杂推理和多步工具使用。Nano模型在保持高准确率的同时极具成本效益，Super适合协同和高负载场景，Ultra则提供顶尖性能。Nemotron 3系列将开放模型权重和训练资源，推动智能代理应用的发展。,13
Scaling Open-Ended Reasoning to Predict the Future,扩展开放式推理以预测未来,LLM,Other,https://arxiv.org/pdf/2512.25070,https://huggingface.co/papers/2512.25070,无法生成摘要。,13
AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents,AI遇见大脑：从认知神经科学到自主智能体的记忆系统,Agent,PKU,https://arxiv.org/pdf/2512.23343,https://huggingface.co/papers/2512.23343,本文系统梳理了认知神经科学与自主智能体领域中记忆系统的相关知识，旨在弥合人类记忆机制与人工智能记忆设计之间的差距。作者从记忆的定义与功能出发，比较分析了生物和人工记忆的分类、存储机制及管理流程，评述了当前主流的记忆评估标准，并探讨了记忆安全的攻防问题。最后，论文展望了多模态记忆系统和技能习得等未来研究方向，为推动智能体记忆系统的发展提供了理论基础和实践指导。,13
PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation,PhyGDPO：面向物理一致性的文本到视频生成的物理感知组内直接偏好优化,Multimodal LLM,Meta,https://arxiv.org/pdf/2512.24551,https://huggingface.co/papers/2512.24551,无法生成摘要。,13
"The Illusion of Specialization: Unveiling the Domain-Invariant ""Standing Committee"" in Mixture-of-Experts Models",专精的错觉：揭示Mixture-of-Experts模型中跨域不变的“常务委员会”,LLM,Other,https://arxiv.org/pdf/2601.03425,https://huggingface.co/papers/2601.03425,本文通过提出COMMITTEEAUDIT框架，系统分析了Mixture of Experts（MoE）模型中的专家路由行为，挑战了其广泛认同的领域专属化假设。研究发现，在不同任务领域和模型架构中，存在一个稳定且跨域共享的“常设委员会”专家组，承担了大部分计算任务，显示出模型更倾向于集中处理而非严格分工。此发现揭示了MoE模型内在的结构偏向，暗示当前的训练策略可能限制了模型的效率和性能提升，为未来优化MoE模型提供了新的视角。,13
Agent-as-a-Judge,Agent-as-a-Judge（代理作为评判者）,Agent,Other,https://arxiv.org/pdf/2601.05111,https://huggingface.co/papers/2601.05111,本文系统回顾了从“大型语言模型评审”向“智能代理评审”范式的演变。传统单一大型语言模型在处理复杂、多步骤任务时存在偏见和推理浅显等局限，难以进行可靠验证。为此，智能代理评审引入了规划、工具辅助验证、多代理协作及持久记忆等机制，实现了更为稳健、可验证和细致的评估。论文提出了该领域的统一框架和发展分类，梳理了核心方法及应用，分析了前沿挑战并指出未来研究方向，为智能代理评审的进一步发展提供了明确的路线图。,13
Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency,信心的错觉？通过邻域一致性诊断大语言模型的真实性,LLM,Other,https://arxiv.org/pdf/2601.05905,https://huggingface.co/papers/2601.05905,本文针对大型语言模型在面对轻微上下文干扰时，表现出自信但不稳定的事实回答问题，提出了一种新的评估方法——邻域一致性信念（NCB），用于衡量模型回答在相关语义邻域中的稳定性。通过设计认知压力测试，验证了高NCB值的数据对干扰更具鲁棒性。基于此，作者进一步提出结构感知训练（SAT）策略，有效提升了模型对长尾知识的稳定性，显著减少了约30%的知识脆弱性。该研究为提升语言模型在实际应用中的可靠性提供了新的思路和方法。,13
ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration,ET-Agent：通过行为校准激励高效的工具集成推理智能体,Agent,Other,https://arxiv.org/pdf/2601.06860,https://huggingface.co/papers/2601.06860,本文提出了ET-Agent，一种针对大型语言模型在工具集成推理任务中行为校准的训练框架。该方法通过自我进化的数据循环机制和分阶段行为校准训练，有效纠正模型在调用工具时的冗余或不足行为，提升了任务执行的正确性和效率。实验结果表明，ET-Agent在推理简洁性和工具执行准确性等方面表现优越，为提升基于工具的推理智能体提供了实用的解决方案和研究参考。,13
Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image,基于单张图像的联合三维几何重建与运动生成用于4D合成,Diffusion Model,THU,https://arxiv.org/pdf/2512.05044,https://huggingface.co/papers/2512.05044,本文提出MoRe4D方法，实现了从单张静态图像联合生成高质量的三维几何结构和动态运动，进而合成一致且细节丰富的四维场景。为解决现有方法中几何与运动分离导致的不一致问题，作者设计了基于扩散模型的运动轨迹生成器，并结合深度信息进行运动归一化，有效融合几何与动态信息。此外，构建了包含6万条稠密点轨迹的大规模数据集TrajScene-60K，支持模型训练与评估。实验结果表明，MoRe4D在多视角一致性和动态表现上均优于现有方法，推动了单图像四维场景合成的发展。,12
LongCat-Image Technical Report,LongCat-Image技术报告,Diffusion Model,Other,https://arxiv.org/pdf/2512.07584,https://huggingface.co/papers/2512.07584,本文介绍了LongCat-Image，一款开源的中英双语图像生成基础模型。通过严格的数据筛选和多阶段训练策略，该模型在多语言文本渲染和逼真图像生成方面表现出色，尤其在复杂中文字符的准确呈现上领先业界。其紧凑的6亿参数设计大幅降低了计算资源需求，实现高效推理和部署。此外，LongCat-Image在图像编辑任务中也取得了优异表现。作者同时发布了完整的模型版本和训练工具链，促进社区开发与研究，推动多语言视觉内容创作的发展。,12
Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training,利用自动质量引导自训练提升无监督视频实例分割,Other,Other,https://arxiv.org/pdf/2512.06864,https://huggingface.co/papers/2512.06864,本文提出了一种名为AutoQ-VIS的无监督视频实例分割方法，通过自动质量评估引导的自训练策略，有效缩小了合成数据与真实视频之间的差距。该方法建立了伪标签生成与质量评估的闭环机制，实现了从合成到真实视频的逐步适应。实验结果表明，AutoQ-VIS在YouTubeVIS-2019验证集上达到52.6%的准确率，较现有最佳方法提升4.4%，且无需人工标注，展示了质量感知自训练在无监督视频实例分割中的应用潜力。,12
Arbitrage: Efficient Reasoning via Advantage-Aware Speculation,Arbitrage：基于优势感知推测的高效推理,LLM,Other,https://arxiv.org/pdf/2512.05033,https://huggingface.co/papers/2512.05033,本文提出了Arbitrage，一种基于优势感知的动态推理加速框架，用于提升大型语言模型推理效率。传统的投机解码方法在推理任务中因逐步验证导致大量不必要的重复计算，效率有限。Arbitrage通过训练轻量级路由器，动态判断何时由目标模型生成更优结果，从而避免固定阈值带来的无效重算，接近理想选择效果。在多项数学推理基准测试中，Arbitrage显著优于现有方法，实现了在保持准确率的前提下推理速度提升近两倍，展示了其在复杂推理任务中的高效性和实用价值。,12
"InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",InfiniteVL：协同线性与稀疏注意力实现高效无限输入视觉-语言模型,Multimodal LLM,Other,https://arxiv.org/pdf/2512.08829,https://huggingface.co/papers/2512.08829,本文提出了InfiniteVL，一种结合滑动窗口注意力和门控增量网络的视觉语言模型架构，有效解决了传统方法在长序列处理和信息密集任务中的性能瓶颈。通过设计三阶段训练策略，InfiniteVL在使用不到主流模型2%训练数据的情况下，实现了与领先Transformer模型相当的性能，同时显著提升推理速度和内存效率。该模型在视频流处理场景中表现稳定，支持实时24帧处理，展示了优异的长时记忆能力，推动了高效、无限输入视觉语言模型的发展。,12
"VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",VQRAE：用于多模态理解、生成与重建的表示量化自编码器,Multimodal LLM,THU,https://arxiv.org/pdf/2511.23386,https://huggingface.co/papers/2511.23386,本文提出了VQRAE，一种基于向量量化的表示自编码器，实现了图像理解、生成和重建的统一表示。该方法通过一个统一的编码器同时生成连续的语义特征和离散的视觉令牌，采用两阶段训练策略提升模型性能。相比传统方法，VQRAE利用高维语义码本实现了高效的码本利用率和细粒度重建能力，在多个视觉任务上表现出竞争力，展示了其在多模态统一建模中的潜力和良好的扩展性。,12
MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence,MMSI-Video-Bench：面向视频空间智能的综合基准测试,Multimodal LLM,Shanghai AI Lab,https://arxiv.org/pdf/2512.10863,https://huggingface.co/papers/2512.10863,本文提出了MMSI-Video-Bench，一个全面的人类标注视频空间智能评测基准，涵盖感知、规划、预测和跨视频推理四个层面，基于多样化视频数据设计1106个问题。通过对25个先进多模态大模型的评测，揭示了模型在几何推理、运动定位和长时预测等方面与人类存在显著差距，且现有微调和常用策略难以提升表现。该基准为推动视频空间智能的发展提供了系统而细致的测试平台。,12
FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition,FiNERweb：用于可扩展多语言命名实体识别的数据集与工具,LLM,Other,https://arxiv.org/pdf/2512.13884,https://huggingface.co/papers/2512.13884,本文提出了FiNERweb，一个覆盖91种语言和25种文字的多语言命名实体识别（NER）数据集构建流程。该方法通过训练回归模型筛选相关文本段落，并利用多语言大语言模型生成标注，产出约22.5万段文本和23.5万个实体标签。实验表明，FiNERweb训练的模型在零样本迁移任务中表现优异，且标注质量高且可靠。该数据集同时提供英文及目标语言标签，促进了多语言NER模型的有效训练和评估，推动了跨语言信息抽取研究的发展。,12
Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning,大语言模型能否引导自身探索？基于梯度引导的强化学习用于大语言模型推理,LLM,Tencent,https://arxiv.org/pdf/2512.15687,https://huggingface.co/papers/2512.15687,本文提出了一种名为G2RL的强化学习框架，通过利用大语言模型自身的梯度更新信息来引导探索，克服了传统探索方法依赖外部启发式信号的问题。G2RL基于模型最终层的敏感度特征，识别并奖励对策略更新方向有实质贡献的样本，抑制冗余或无效更新，从而实现更有效的自我驱动探索。在多个数学和推理基准测试中，G2RL显著优于基于熵的和外部嵌入的方法，提升了模型的推理能力，验证了利用模型自身更新空间引导探索的有效性和合理性。,12
EasyV2V: A High-quality Instruction-based Video Editing Framework,EasyV2V：一种高质量基于指令的视频编辑框架,Other,Other,https://arxiv.org/pdf/2512.16920,https://huggingface.co/papers/2512.16920,本文提出了EasyV2V，一种高效且灵活的基于指令的视频编辑框架。该方法通过整合多样化数据源、利用预训练文本到视频模型并结合轻量级微调，实现了对视频编辑的一致性和时空控制的统一管理。EasyV2V支持多种输入形式，如视频、文本、掩码及参考图像，显著提升了编辑质量和灵活性。实验结果表明，该框架在视频编辑效果上优于现有主流和商业系统，推动了视频编辑技术向更高质量和更广泛应用的方向发展。,12
Step-DeepResearch Technical Report,Step-DeepResearch 技术报告,Agent,Other,https://arxiv.org/pdf/2512.20491,https://huggingface.co/papers/2512.20491,本文提出了Step-DeepResearch，一种端到端的深度研究智能体，通过创新的数据合成策略和渐进式训练方法，显著提升了模型在开放式复杂研究任务中的表现。该方法强化了规划、信息收集和报告撰写等关键能力，结合检查表式评估机制，提高了系统的鲁棒性。作者还构建了针对中文场景的ADR-Bench评测基准，验证了模型的实际应用效果。实验结果显示，Step-DeepResearch在保持较低计算成本的同时，达到了与顶尖闭源模型相当的专业水平，展现出较高的成本效益和实用价值。,12
Reinforcement Learning for Self-Improving Agent with Skill Library,基于技能库的自我提升智能体的强化学习,Agent,Amazon,https://arxiv.org/pdf/2512.17102,https://huggingface.co/papers/2512.17102,本文提出了一种基于强化学习的新框架SAGE，用于提升大型语言模型驱动的智能体在新环境中的自我改进能力。该方法通过引入技能库，系统地积累和应用任务中学到的技能，结合顺序任务执行和技能融合奖励机制，实现技能的持续学习和有效利用。实验表明，SAGE在复杂任务环境中显著提升了任务完成率，同时减少了交互步骤和生成内容的数量，展现出较现有方法更优的准确性和效率。,12
TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior,TokSuite：衡量分词器选择对语言模型行为的影响,LLM,DeepMind,https://arxiv.org/pdf/2512.20757,https://huggingface.co/papers/2512.20757,本文提出了TokSuite，一个包含多种使用不同分词器但架构和训练条件相同的语言模型集合及对应基准测试，旨在独立评估分词器选择对语言模型表现的影响。通过统一控制其他变量，TokSuite揭示了不同分词器在处理文本时的优缺点，并通过真实场景下的扰动测试，系统地衡量分词策略对模型行为的作用。该工作填补了分词器对语言模型性能影响研究的空白，为未来分词器设计和优化提供了重要工具和参考。,12
Schoenfeld's Anatomy of Mathematical Reasoning by Language Models,Schoenfeld的数学推理结构解析及其在语言模型中的应用,LLM,Other,https://arxiv.org/pdf/2512.19995,https://huggingface.co/papers/2512.19995,本文提出了ThinkARM框架，基于Schoenfeld的Episode Theory，将大型语言模型在数学推理中的思考过程抽象为分析、探索、执行、验证等功能步骤。通过该框架，作者揭示了不同模型在推理结构和动态上的显著差异，超越了传统的词元级别分析。研究发现，探索步骤是正确推理的关键分支点，而提高效率的方法倾向于减少反馈评估环节。该工作使得推理过程更加透明，促进了对现代语言模型推理机制的系统理解和分析。,12
ProEdit: Inversion-based Editing From Prompts Done Right,ProEdit：基于反演的提示驱动编辑方法,Diffusion Model,Other,https://arxiv.org/pdf/2512.22118,https://huggingface.co/papers/2512.22118,本文提出了ProEdit，一种基于反演技术的图像和视频编辑方法，旨在解决现有方法中过度依赖原始图像信息，导致编辑效果受限的问题。通过引入KV-mix模块在注意力层面混合源图像与目标特征，以及Latents-Shift模块扰动潜在空间中的编辑区域，ProEdit有效减少了源图像对编辑区域的影响，同时保持背景一致性。大量实验表明，该方法在多个编辑任务中表现优异，且具备良好的兼容性，可无缝集成到现有编辑框架中，提升编辑的准确性和灵活性。,12
TimeBill: Time-Budgeted Inference for Large Language Models,TimeBill：面向大语言模型的时间预算推理,LLM,Other,https://arxiv.org/pdf/2512.21859,https://huggingface.co/papers/2512.21859,本文提出了TimeBill，一种针对大语言模型（LLMs）的时间预算推理框架，旨在确保模型在限定时间内生成高质量响应。通过引入细粒度的响应长度预测和执行时间估计，TimeBill能够准确预测推理所需时间，并根据时间预算动态调整缓存策略，有效平衡推理速度与响应性能。实验结果表明，TimeBill显著提升了任务完成率，同时保持了较好的响应质量，适用于机器人、自动驾驶等对响应时效性要求严格的应用场景。,12
Training AI Co-Scientists Using Rubric Rewards,使用Rubric奖励训练AI协同科学家,AI4Science,Meta,https://arxiv.org/pdf/2512.23707,https://huggingface.co/papers/2512.23707,本文提出了一种利用自动提取的研究目标和评分标准，结合强化学习自我评分的方法，训练语言模型生成更符合约束和隐含要求的科研计划。通过大规模、多领域的训练数据和无人工监督的生成-验证机制，模型在机器学习和医学等多个领域表现出显著提升。专家评审结果显示，该方法生成的计划更受认可，且具备良好的跨领域泛化能力，展示了构建高效、可扩展AI科研助手的潜力。,12
GR-Dexter Technical Report,GR-Dexter 技术报告,Embodied AI,ByteDance,https://arxiv.org/pdf/2512.24210,https://huggingface.co/papers/2512.24210,本文提出了GR-Dexter，一个面向双手灵巧机器人操作的综合框架，结合了硬件设计、模型训练和数据采集。该系统设计了一款紧凑的21自由度机械手，配备直观的双手远程操作装置，利用远程操控数据和大规模多模态数据进行训练。实验证明，GR-Dexter在执行复杂的日常操作和通用抓取任务时表现出良好的适应性和鲁棒性，能够处理未见过的物体和指令。该工作为实现通用灵巧手机器人操作提供了实用的解决方案。,12
Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling,Falcon-H1R：通过混合模型推动高效测试时扩展的推理前沿,LLM,Other,https://arxiv.org/pdf/2601.02346,https://huggingface.co/papers/2601.02346,本文介绍了Falcon-H1R，一款拥有7亿参数的小型语言模型，通过精心的数据筛选和针对性的训练策略，实现了在多项复杂推理任务中与远大于自身规模的顶尖模型相媲美甚至超越的表现。该模型采用混合并行架构，提升了推理速度和计算效率，支持大规模链式思维生成和并行推理扩展。结合最新的DeepConf方法，Falcon-H1R在测试时展现出卓越的准确性和成本效益，证明了紧凑模型在保持高性能推理能力方面的潜力和实用价值。,12
Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals,Goal Force：教视频模型实现物理条件目标,Embodied AI,Other,https://arxiv.org/pdf/2601.05848,https://huggingface.co/papers/2601.05848,本文提出了Goal Force，一种通过显式的力向量和中间动态来定义目标的新框架，帮助视频生成模型更准确地模拟物理任务。该模型在合成的基础物理场景（如弹性碰撞和多米诺骨牌）上训练，能够在未见过的复杂真实场景中实现零样本泛化，如工具操作和多物体因果链。研究表明，将视频生成建立在基本物理交互之上，可使模型具备隐式物理模拟能力，从而无需外部引擎即可进行精确的物理感知规划。作者公开了相关数据和代码，推动该领域的发展。,12
FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection,FocusUI：通过保持位置连续性的视觉令牌选择实现高效的用户界面定位,Multimodal LLM,Other,https://arxiv.org/pdf/2601.03928,https://huggingface.co/papers/2601.03928,本论文提出了FocusUI，一种高效的用户界面（UI）定位框架，通过选择与指令相关的视觉区域并保持其位置信息连续性，实现了计算资源的显著节省。该方法结合指令驱动的评分与基于规则的图结构评分，有效剔除冗余视觉信息，同时引入PosPad策略压缩连续丢弃的视觉令牌，避免位置信息丢失。实验结果表明，FocusUI在多个UI定位基准测试中优于现有方法，即使仅保留30%的视觉令牌，也能保持较高准确率并提升推理速度和降低内存消耗，展现出良好的实用价值。,12
EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines,EvoFSM：基于有限状态机的可控自我进化深度研究框架,Agent,"THU, PKU",https://arxiv.org/pdf/2601.09465,https://huggingface.co/papers/2601.09465,本文提出了EvoFSM，一种基于有限状态机的可控自我进化框架，旨在提升大型语言模型代理在处理开放性复杂任务时的适应性与稳定性。通过将系统行为划分为状态转换（流程）和状态内操作（技能）两层结构，EvoFSM在明确约束下优化模型表现，避免了无约束自我修改带来的不稳定和偏差问题。框架还引入了自我进化记忆机制，积累成功经验并规避失败模式。实验证明，EvoFSM在多跳问答和交互决策任务中表现优异，展现出良好的泛化能力和实际应用潜力。,12
COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence,COOPER：一种用于空间智能中协同感知与推理的统一模型,Multimodal LLM,Other,https://arxiv.org/pdf/2512.04563,https://huggingface.co/papers/2512.04563,本文提出了COOPER，一种统一的多模态大语言模型，结合深度信息和图像分割两种辅助模态，通过两阶段训练实现辅助模态生成和自适应交错推理能力。该模型能够同时提升空间感知和空间推理，显著增强视觉空间理解能力。实验表明，COOPER在空间推理任务上平均提升6.91%，且即使仅训练辅助模态生成，也能在距离和尺寸估计上获得7.92%的提升，表明生成辅助模态有助于内化空间知识，推动空间智能的发展。,11
RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards,RealGen：基于检测器引导奖励的真实感文本到图像生成,Diffusion Model,"Shanghai AI Lab, THU, PKU",https://arxiv.org/pdf/2512.00473,https://huggingface.co/papers/2512.00473,本文提出了RealGen，一种面向高真实感文本生成图像的新框架。RealGen结合大语言模型优化文本提示和扩散模型生成图像，创新引入“检测器奖励”机制，通过检测图像中的人工痕迹来提升图像的真实性和细节表现。同时，作者设计了RealBench自动评测体系，实现无人工干预的真实感评价。实验结果表明，RealGen在图像真实度、细节丰富度及美学效果上显著优于现有主流和专业模型，推动了文本到图像生成技术向更高真实感方向的发展。,11
"On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",关于预训练、中期训练与强化学习在推理语言模型中的相互作用,LLM,Other,https://arxiv.org/pdf/2512.07783,https://huggingface.co/papers/2512.07783,本文构建了一个严格控制的实验框架，系统分析了预训练、中期训练和强化学习（RL）三者在提升语言模型推理能力中的作用。研究发现，RL只有在预训练留有提升空间且训练数据难度适中的情况下才能带来显著能力提升；中期训练在固定计算资源下显著增强模型表现，且对上下文泛化至关重要；过程级奖励则有效减少了奖励作弊，提升推理准确性。该工作澄清了三阶段训练的相互关系，为优化语言模型推理训练策略提供了理论基础。,11
Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models,Nemotron-Cascade：用于通用推理模型的级联强化学习扩展,LLM,Other,https://arxiv.org/pdf/2512.13607,https://huggingface.co/papers/2512.13607,本文提出了一种名为Nemotron-Cascade的级联领域强化学习方法，用于提升通用推理模型的性能。该方法通过按领域顺序训练，简化了跨领域训练的复杂性，有效应对推理长度和验证延迟的异质性问题。实验结果显示，Nemotron-Cascade在多个基准测试中表现优异，超越了其监督学习教师模型，并在2025年国际信息学奥林匹克竞赛中获得银牌。论文同时公开了训练和数据方案，为构建高效、通用的推理模型提供了实用路径。,11
Olmo 3,Olmo 3,LLM,Other,https://arxiv.org/pdf/2512.13961,https://huggingface.co/papers/2512.13961,本文介绍了Olmo 3，一系列在7亿和320亿参数规模下的先进开源语言模型。该模型专注于提升长文本推理、函数调用、代码生成、指令执行、日常对话及知识回忆等能力。论文不仅发布了模型本身，还完整公开了模型训练的全过程，包括所有训练阶段、数据集、检查点和依赖，确保高度透明和可复现。Olmo 3的旗舰版本Olmo 3.1 Think 32B在开放领域中表现出色，代表了当前最强的开源思考型语言模型。,11
DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models,DiffusionVL：将任意自回归模型转换为扩散视觉语言模型,Multimodal LLM,Other,https://arxiv.org/pdf/2512.15713,https://huggingface.co/papers/2512.15713,本文提出了DiffusionVL，一种通过简单微调将强大的自回归（AR）视觉语言模型转换为扩散视觉语言模型（dVLM）的方法。该方法不仅实现了从AR范式向扩散范式的有效转变，还支持任意长度生成和缓存复用，显著提升了推理速度。实验表明，DiffusionVL在使用远少于以往方法的数据量下，分别在视觉和认知基准测试中取得了超过30%的性能提升，并实现了2倍的推理加速，展示了扩散模型在多模态任务中的巨大潜力和应用价值。,11
AdaTooler-V: Adaptive Tool-Use for Images and Videos,AdaTooler-V：面向图像和视频的自适应工具使用,Multimodal LLM,THU,https://arxiv.org/pdf/2512.16918,https://huggingface.co/papers/2512.16918,本文提出了AdaTooler-V，一种多模态大语言模型，能够根据视觉任务的实际需求自适应调用视觉工具，从而提升推理准确性并减少不必要的工具使用。通过引入基于奖励调整的强化学习算法AT-GRPO，模型学会仅在工具真正有助于提升性能时才调用它们。作者还构建了两个大规模数据集支持训练和评估。实验结果表明，AdaTooler-V在多种视觉推理任务中表现优异，显著超过现有开源模型，甚至优于部分商业模型，展示了其在高效且精准的视觉理解方面的潜力。,11
GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators,GenEnv：基于难度对齐的LLM智能体与环境模拟器协同进化框架,Agent,ByteDance,https://arxiv.org/pdf/2512.19682,https://huggingface.co/papers/2512.19682,本文提出了GenEnv框架，通过在大型语言模型（LLM）代理与生成环境模拟器之间建立难度匹配的协同进化机制，有效提升了代理的学习效率和表现。与传统依赖静态数据训练的方法不同，GenEnv动态生成适合代理当前能力水平的任务，形成自适应的训练课程。实验证明，GenEnv在多个基准测试中相比7B模型提升性能最高达40.3%，且在使用更少训练数据的情况下超越了基于离线数据增强的方法。该方法为高效扩展智能体能力提供了新的思路。,11
Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes,Talk2Move：基于强化学习的文本指令场景中对象级几何变换,Diffusion Model,Amazon,https://arxiv.org/pdf/2601.02356,https://huggingface.co/papers/2601.02356,本文提出了Talk2Move，一种基于强化学习的框架，实现了通过自然语言指令对场景中物体进行精确的几何变换，包括平移、旋转和缩放。该方法通过创新的策略优化和空间奖励设计，有效解决了缺乏配对数据和像素级优化限制的问题，提升了变换的准确性和语义一致性。实验结果表明，Talk2Move在空间精度和场景连贯性方面显著优于现有文本驱动的图像编辑方法，推动了多模态场景编辑技术的发展。,11
Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity,LLM是否易受偏好破坏攻击（PUA）？一种用于诊断偏好对齐与现实有效性权衡的因子分析方法,LLM,Other,https://arxiv.org/pdf/2601.06596,https://huggingface.co/papers/2601.06596,本文研究了大型语言模型在追求用户偏好一致性过程中，可能被操纵性提示误导，导致偏离事实的风险。作者提出了一种基于因子分析的诊断方法，细致拆解不同提示因素对模型输出的影响，揭示了模型在真实与偏好之间的权衡关系。实验发现，部分先进模型反而更易受到此类操控，且不同模型表现出差异，表明需要针对性防御策略。该方法为评估和改进模型的偏好对齐与真实性提供了新工具，有助于提升模型在实际应用中的可靠性和安全性。,11
"HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",HiF-VLA：通过运动表示实现视觉-语言-动作模型中的回顾、洞察与前瞻,Embodied AI,Other,https://arxiv.org/pdf/2512.09928,https://huggingface.co/papers/2512.09928,本文提出了HiF-VLA，一种结合运动信息进行双向时间推理的视觉-语言-动作模型框架。该方法通过捕捉动作的动态变化，有效克服了传统模型仅依赖当前观察导致的时间短视问题，实现了对过去状态的回顾和未来动作的预测。HiF-VLA在多项长时序机器人操作任务中显著优于现有基线方法，且几乎不增加推理延迟，展现了其在复杂长期操作场景中的实用性和广泛应用潜力。,10
Evaluating Gemini Robotics Policies in a Veo World Simulator,在Veo世界模拟器中评估Gemini Robotics策略,Embodied AI,DeepMind,https://arxiv.org/pdf/2512.10675,https://huggingface.co/papers/2512.10675,本文提出了一种基于前沿视频模型（Veo）的生成式评估系统，用于机器人策略的全面测试。该系统不仅能准确模拟机器人在训练环境中的表现，还能有效评估其在不同环境下的泛化能力和安全性。通过多视角一致性和图像编辑技术，系统能够生成包含新物体和背景的真实场景变化，支持多维度的策略评估。作者通过1600多次真实实验验证了该方法在多任务双臂机械手策略评估中的有效性，展示了该系统在机器人策略开发和安全保障中的重要价值。,10
StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space,StereoSpace：基于端到端扩散的无深度立体几何合成方法,Diffusion Model,Other,https://arxiv.org/pdf/2512.10959,https://huggingface.co/papers/2512.10959,本文提出了StereoSpace，一种基于扩散模型的单目图像到立体图像合成框架。该方法通过视点条件引导生成器在一个规范化空间内隐式推断几何对应关系和填补遮挡区域，无需显式深度信息或图像扭曲。为保证评估公平，设计了不依赖真实或代理几何数据的测试协议，重点衡量感知舒适度和几何一致性。实验表明，StereoSpace在多种复杂场景下表现优异，生成的立体图像视差清晰且鲁棒性强，展示了基于视点条件扩散的深度无关立体合成的潜力和实用价值。,10
PersonaLive! Expressive Portrait Image Animation for Live Streaming,PersonaLive！用于直播的富表现力肖像图像动画,Diffusion Model,Other,https://arxiv.org/pdf/2512.11253,https://huggingface.co/papers/2512.11253,本文提出了PersonaLive，一种面向直播场景的实时人像动画生成框架。通过结合多阶段训练、混合隐式信号以及外观蒸馏策略，显著提升了动画的表达能力和推理速度。此外，采用自回归微块流式生成及历史关键帧机制，实现了低延迟且稳定的长序列视频输出。实验结果表明，PersonaLive在保证动画质量的同时，推理速度较现有扩散模型提升7至22倍，极大拓展了扩散技术在人像直播动画中的应用潜力。,10
WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment,WebOperator：面向Web环境自主智能体的动作感知树搜索,Agent,Other,https://arxiv.org/pdf/2512.12692,https://huggingface.co/papers/2512.12692,本文提出了WebOperator，一种面向网页环境的自主代理的树搜索框架，解决了现有方法在处理不可逆操作和部分可见网页内容时缺乏安全回溯与长远规划的问题。WebOperator通过结合基于奖励和安全性的优先搜索策略，以及验证路径可行性的回溯机制，有效避免了操作失误和副作用。此外，它通过多样化的动作生成和严格筛选，提升了探索的多样性和准确性。实验结果表明，WebOperator在复杂网页任务中显著提升了成功率，展示了其在自主网页操作中的实用价值。,10
VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse,VersatileFFN：通过自适应宽度与深度复用实现大语言模型的参数效率提升,LLM,Other,https://arxiv.org/pdf/2512.14531,https://huggingface.co/papers/2512.14531,本文提出了一种名为VersatileFFN的新型前馈网络结构，通过在固定参数预算内灵活复用参数，实现模型宽度和深度上的扩展能力。该方法借鉴认知双重加工理论，设计了两条自适应路径：一条通过共享参数生成多子专家以提升表达能力，另一条递归应用相同参数实现更深层次处理。同时，难度感知门控机制动态分配计算资源，使简单输入走高效路径，复杂输入获得更细致处理。该方案无需增加额外参数，显著提升大规模语言模型的性能与计算效率，实验验证了其广泛适用性和有效性。,10
Differentiable Evolutionary Reinforcement Learning,可微进化强化学习,Agent,Other,https://arxiv.org/pdf/2512.13399,https://huggingface.co/papers/2512.13399,本文提出了一种名为Differentiable Evolutionary Reinforcement Learning（DERL）的新框架，旨在自动优化强化学习中的奖励函数。与传统依赖启发式或黑箱进化方法不同，DERL通过可微分的双层优化结构，将任务表现作为反馈信号，指导奖励函数的进化，从而更有效地捕捉任务内在结构。实验证明，DERL在机器人控制、科学模拟和数学推理等多领域取得了领先表现，尤其在面对未见过的任务时表现优异，显著提升了智能体的自主学习能力，减少了人工设计奖励的需求。,10
VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs,VOYAGER：一种利用大语言模型生成多样化数据集的无训练方法,LLM,Other,https://arxiv.org/pdf/2512.12072,https://huggingface.co/papers/2512.12072,本文提出了VOYAGER，一种无需额外训练即可利用大语言模型生成多样化合成数据集的方法。该方法通过迭代优化一种数学指标，利用确定性点过程确保生成数据的多样性。VOYAGER不仅适用于闭源模型且具备良好的扩展性，还在多项实验中显示出比现有主流方法高1.5至3倍的多样性提升。该方法为提升合成数据质量提供了一种高效且通用的解决方案，促进了下游模型的评估与训练。,10
Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image,Multimodal RewardBench 2：评估交错文本与图像的全能奖励模型,Multimodal LLM,Meta,https://arxiv.org/pdf/2512.16899,https://huggingface.co/papers/2512.16899,本文提出了Multimodal RewardBench 2（MMRB2），首个针对同时处理图文交织内容的奖励模型的综合评测基准。MMRB2涵盖文本生成图像、图像编辑、交织生成及多模态推理四大任务，收录了来自23个模型和代理的专家标注偏好对。通过评测多种先进模型，结果显示最新的Gemini 3 Pro表现最佳，但仍低于人类水平。研究还发现MMRB2得分与实际任务表现高度相关，揭示了奖励模型未来改进的关键方向。该基准为多模态奖励模型的评估和优化提供了重要工具。,10
"Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward","[翻译]Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",Other,Other,https://arxiv.org/pdf/2512.16912,https://huggingface.co/papers/2512.16912,本文探讨了基于可验证奖励的强化学习（RLVR）在提升大型语言模型推理能力中的探索与利用平衡问题。研究发现，通过引入表面上矛盾的机制——即奖励与真实结果无关的“虚假奖励”以及降低策略的不确定性（熵最小化），均能提升模型表现。作者分析指出，虚假奖励通过减少剪辑偏差降低策略熵，使模型输出更为确定，而单纯熵最小化并不足以带来改进。论文还提出了奖励错配模型，解释虚假奖励在非理想环境下的有效性，为RLVR训练提供了新的理论依据和实践指导。,10
RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing,RePlan：面向复杂指令驱动图像编辑的推理引导区域规划,Diffusion Model,Tencent,https://arxiv.org/pdf/2512.16864,https://huggingface.co/papers/2512.16864,本文提出了RePlan，一种结合视觉语言规划器与扩散编辑器的图像编辑框架，针对复杂指令和复杂场景下的图像编辑任务，通过分步骤推理明确定位编辑区域，实现多区域并行精准修改。该方法无需额外训练即可有效减少编辑伪影，并利用强化学习提升推理准确性。作者还构建了专注细粒度定位和知识密集型编辑的IV-Edit基准测试。实验表明，RePlan在复杂指令与视觉环境下显著优于现有方法，提升了编辑的区域精度和整体效果。,10
LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding,LoPA：通过前瞻并行解码扩展扩散大语言模型推理,Diffusion Model,Other,https://arxiv.org/pdf/2512.16229,https://huggingface.co/papers/2512.16229,本文提出了一种名为LoPA的无训练、即插即用算法，通过提前并行探索不同的生成顺序，有效提升扩散大语言模型的推理并行度。LoPA显著增加了每次前向计算生成的词元数量，从而加快了模型的解码速度。结合多设备分支并行机制，LoPA在多GPU环境下实现了超过1000词元每秒的单样本处理速度，且保持了优于基线模型的性能。该方法为扩散语言模型的高效推理提供了新的思路和实践方案。,10
SAM Audio: Segment Anything in Audio,SAM Audio：音频中的任意分割,Diffusion Model,Meta,https://arxiv.org/pdf/2512.18099,https://huggingface.co/papers/2512.18099,本文提出了SAM Audio，一种基于扩散变换器的通用音频分离基础模型。该模型创新性地融合了文本、视觉和时间跨度多模态提示，能够灵活分离语音、音乐及一般声音中的目标音源。SAM Audio在多种公开和真实世界音频分离任务中表现优异，显著优于现有专用及通用方法。此外，作者还构建了包含人类标注多模态提示的真实分离基准和无参考评估模型，有效提升了音频分离的实用性和评价可靠性。该工作推动了多模态AI系统对声音理解和处理能力的发展。,10
DreamOmni3: Scribble-based Editing and Generation,DreamOmni3：基于涂鸦的编辑与生成,Multimodal LLM,ByteDance,https://arxiv.org/pdf/2512.22525,https://huggingface.co/papers/2512.22525,本文提出了DreamOmni3，一个结合文本、图像与手绘涂鸦的灵活编辑与生成系统，解决了传统基于文本指令难以准确表达编辑位置和细节的问题。通过设计多样的涂鸦编辑与生成任务，并构建相应的数据合成流程，DreamOmni3实现了更细粒度的视觉内容控制。其创新的联合输入框架利用彩色标记同时处理原图与涂鸦图，提升了复杂编辑的准确性和效率。实验结果表明，该方法在多任务编辑与生成上表现优异，推动了交互式图像编辑技术的发展。,10
The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving,推理与创造力的权衡：迈向以创造力驱动的问题解决,LLM,Other,https://arxiv.org/pdf/2601.00747,https://huggingface.co/papers/2601.00747,本文针对当前大型语言模型在训练过程中追求正确性导致推理路径多样性严重下降的问题，提出了一个统一的变分框架——分布式创造性推理（DCR）。该框架揭示了多种训练方法多样性衰减的机制，设计了防止多样性崩溃的策略，并给出了可行的实践方案。DCR实现了模型在保持推理准确性的同时，维持丰富多样的思维路径，从而提升模型在新颖任务中的创造力和泛化能力，推动了创造性驱动的问题解决方法的发展。,10
Klear: Unified Multi-Task Audio-Video Joint Generation,Klear：统一的多任务音视频联合生成,Multimodal LLM,Other,https://arxiv.org/pdf/2601.04151,https://huggingface.co/papers/2601.04151,本文提出了Klear，一种统一的多任务音视频联合生成框架，针对音视频不同步、唇语对齐差及单模态性能下降等问题，通过单一模型架构、渐进式多任务训练和大规模高质量音视频密集字幕数据集构建，有效提升了音视频的语义和时间对齐能力。Klear在多种生成任务中表现优异，具备良好的泛化能力和稳定性，显著优于现有开源方法，为下一代音视频合成技术提供了统一且可扩展的解决方案。,10
TranslateGemma Technical Report,TranslateGemma 技术报告,Multimodal LLM,Other,https://arxiv.org/pdf/2601.09012,https://huggingface.co/papers/2601.09012,本文介绍了TranslateGemma，一款基于Gemma 3模型的开源机器翻译系统。通过结合高质量的人类翻译数据与合成平行语料，采用两阶段微调策略（监督微调和强化学习），显著提升了多语言翻译质量和效率。实验结果表明，TranslateGemma在多个语言对和标准测试集上均优于基线模型，且较小模型也能达到大模型的性能水平。此外，该系统保持了良好的多模态处理能力。TranslateGemma的发布为机器翻译研究提供了高效且灵活的工具。,10
Self-Improving VLM Judges Without Human Annotations,无需人工标注的自我提升视觉-语言模型评判器,Multimodal LLM,Meta,https://arxiv.org/pdf/2512.05145,https://huggingface.co/papers/2512.05145,本文提出了一种无需人工标注、通过自我合成数据自训练视觉-语言模型（VLM）评判器的框架。该方法通过生成多样化的多模态指令响应对，结合推理过程筛选高质量样本，迭代训练评判模型。实验表明，该评判器在多个评测基准上显著提升准确率，甚至超越了参数量更大的模型。此研究展示了无需昂贵人工偏好数据即可构建高效VLM评判器的潜力，为未来随模型能力提升而自我进化的评判机制奠定基础。,9
MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment,MIND-V：基于强化学习物理对齐的分层视频生成用于长时域机器人操作,Embodied AI,THU,https://arxiv.org/pdf/2512.06628,https://huggingface.co/papers/2512.06628,本文提出了MIND-V，一种层级化视频生成框架，旨在合成符合物理规律且逻辑连贯的长时段机器人操作视频。该方法结合了高级语义推理和像素级视频合成，通过任务规划、语义转换和条件视频生成三个模块实现。引入的测试时视觉预测优化和基于强化学习的物理一致性奖励，有效提升了视频的物理合理性和长期稳定性。实验结果表明，MIND-V在长时段机器人操作视频生成任务中性能领先，推动了机器人模仿学习数据的规模化和高质量合成。,9
FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction,FlashPortrait：基于自适应潜变量预测的6倍加速无限肖像动画,Diffusion Model,"Microsoft, Tencent, Alibaba",https://arxiv.org/pdf/2512.16900,https://huggingface.co/papers/2512.16900,本文提出了FlashPortrait，一种基于视频扩散模型的长时人像动画生成方法。该方法通过引入归一化面部表情模块和动态滑动窗口策略，有效保持了动画中的身份一致性和画面连贯性。利用高阶潜在变量导数预测未来帧，显著减少了推理过程中的计算步骤，实现了最高6倍的加速。实验结果表明，FlashPortrait在生成无限长度且身份稳定的人像动画方面表现优异，兼具速度和质量优势，推动了长时人像动画技术的发展。,9
HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering,HERBench：用于视频问答中多证据整合的基准测试,Multimodal LLM,Other,https://arxiv.org/pdf/2512.14870,https://huggingface.co/papers/2512.14870,本文提出了HERBench，一个专门评估视频问答模型跨时间整合多条视觉线索能力的新基准。HERBench包含2.6万多个多项选择题，要求模型结合至少三个不同时间段的证据才能正确回答，显著提高了证据整合的难度。通过引入“最小必要帧集”指标，量化了模型对多帧信息融合的需求。对13个先进视频大模型的测试显示，其表现仅略优于随机猜测，暴露出证据检索和信息融合两大瓶颈。HERBench为推动更强的时序推理和综合理解提供了重要评价工具。,9
Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs,Turn-PPO：基于PPO的回合级优势估计用于提升智能体大语言模型中的多回合强化学习,Agent,Amazon,https://arxiv.org/pdf/2512.17008,https://huggingface.co/papers/2512.17008,本文提出了Turn-PPO，一种基于Proximal Policy Optimization（PPO）的多轮强化学习方法，专门针对多轮交互任务中优势估计的不稳定性和长远推理需求进行改进。与传统的基于token级别的策略优化不同，Turn-PPO采用基于“回合”级别的决策过程，更准确地反映每轮对最终结果的贡献，从而提升了训练的稳定性和效果。实验证明，该方法在WebShop和Sokoban数据集上均优于现有方法，显著提升了多轮任务中智能体的表现和推理能力。,9
Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs,Reasoning Palette：通过潜在上下文化调控推理以实现（视觉）大语言模型的可控探索,Multimodal LLM,Alibaba,https://arxiv.org/pdf/2512.17206,https://huggingface.co/papers/2512.17206,本文提出了Reasoning Palette，一种通过潜在变量调节推理过程的新框架，旨在提升大规模（视觉）语言模型的推理多样性和探索能力。该方法利用变分自编码器从问答对中提取潜在上下文，并将其转化为可学习的前缀，调控模型内部的推理路径，实现推理策略的多样化和可控化。在强化学习训练中，Reasoning Palette促进了更高效的结构化探索，显著提升了模型的持续学习能力和推理表现。实验结果表明，该方法在多个推理任务上优于传统强化学习技术，具备良好的解释性和应用潜力。,9
StoryMem: Multi-shot Long Video Storytelling with Memory,StoryMem：基于记忆的多镜头长视频故事讲述,Diffusion Model,ByteDance,https://arxiv.org/pdf/2512.19539,https://huggingface.co/papers/2512.19539,本文提出了StoryMem，一种基于记忆机制的多镜头长视频故事生成方法。通过维护和动态更新关键帧记忆库，StoryMem将预训练的单镜头视频扩散模型扩展为能够生成连贯多镜头故事的视频模型。该方法结合语义关键帧选择和美学过滤，确保生成视频在镜头间具有良好的视觉一致性和高质量画面效果。同时，StoryMem支持平滑镜头过渡和定制化故事生成。实验结果表明，StoryMem在跨镜头一致性和视觉表现上均优于现有方法，推动了分钟级长视频故事生成的发展。,9
"See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",少看多对：用于多模态推理的双向感知塑形,Multimodal LLM,"Microsoft, THU",https://arxiv.org/pdf/2512.22120,https://huggingface.co/papers/2512.22120,本文提出了一种名为双向感知塑形（BiPS）的方法，旨在提升视觉语言模型对细粒度视觉信息的理解能力。BiPS通过训练时引入基于问题的视觉区域掩码，强化模型对关键视觉证据的关注，同时抑制仅依赖文本信息的推理路径。该方法在多个评测基准上显著提升了模型性能，并展示了优异的跨领域泛化能力，有效降低了推理成本，推动了多模态推理的准确性和实用性。,9
Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding,Omni-Weather：用于天气生成与理解的统一多模态基础模型,Multimodal LLM,Shanghai AI Lab,https://arxiv.org/pdf/2512.21643,https://huggingface.co/papers/2512.21643,本文提出了Omni-Weather，一种首创的多模态基础模型，统一了天气生成与理解任务。该模型通过共享的自注意力机制整合雷达编码器，实现了高质量的天气预测和因果推理。作者构建了链式思维数据集，提升了模型输出的可解释性和感知效果。大量实验表明，Omni-Weather在天气生成和理解方面均达到领先水平，且两类任务相互促进，展示了统一建模在气象领域的有效性和潜力。,9
Video-BrowseComp: Benchmarking Agentic Video Research on Open Web,Video-BrowseComp：基于开放网络的智能视频研究基准测试,Agent,Other,https://arxiv.org/pdf/2512.23044,https://huggingface.co/papers/2512.23044,本文提出了Video-BrowseComp，一个针对开放网络视频主动推理的新基准，包含210个需依赖视频时间线视觉证据的问题，弥补了现有视频任务多聚焦被动感知、缺乏外部检索和动态推理的不足。通过评测先进模型，发现即便是搜索增强的顶尖模型在此任务上准确率仅为15.24%，且在缺乏元数据的动态视频场景（如体育、游戏）中表现尤差。该基准推动视频研究从被动理解向主动、多模态证据交叉验证发展，促进更智能的视频信息处理能力。,9
DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs,DiffCoT：大语言模型中的扩散风格链式思维推理,LLM,Other,https://arxiv.org/pdf/2601.03559,https://huggingface.co/papers/2601.03559,本文提出了DiffCoT，一种基于扩散模型思想的链式思维推理框架，用于提升大语言模型在多步骤数学问题解决中的表现。DiffCoT将推理过程视为一个迭代去噪的过程，通过滑动窗口机制实现对中间推理步骤的统一生成与纠正，同时保持因果一致性。引入的因果扩散噪声调度尊重推理的时间顺序，有效减少早期错误的传播。实验证明，DiffCoT在多个推理基准测试中优于现有方法，显著增强了推理的鲁棒性和错误修正能力。,9
Plenoptic Video Generation,Plenoptic视频生成,Embodied AI,Other,https://arxiv.org/pdf/2601.05239,https://huggingface.co/papers/2601.05239,本文提出了PlenopticDreamer，一种能够在多视角下保持时空一致性的生成视频重渲染框架。该方法通过结合摄像机引导的视频检索和自回归训练，有效同步生成的视觉内容，解决了多视角视频中生成区域一致性差的问题。引入的渐进式上下文扩展、自我条件机制及长视频条件支持，提升了模型的收敛速度和生成视频的质量。实验结果表明，PlenopticDreamer在多视角同步、视觉保真度和摄像机控制精度方面均优于现有方法，具备广泛的视角变换能力，适用于机器人操作等复杂场景。,9
Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models,Imagine-then-Plan：基于世界模型的自适应前瞻智能体学习框架,Agent,Other,https://arxiv.org/pdf/2601.08955,https://huggingface.co/papers/2601.08955,本文提出了Imagine-then-Plan（ITP）框架，通过结合学习到的世界模型和自适应多步前瞻想象，提升智能体在复杂任务中的决策能力。该方法根据任务进展动态调整想象的时间长度，使生成的未来轨迹信息丰富且具指导意义，从而更有效地融合当前观察，辅助策略学习。实验结果表明，ITP显著优于现有方法，增强了智能体的推理能力，为解决更复杂任务提供了新的思路。,9
World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty,知道何时不确定的世界模型：具有校准不确定性的可控视频生成,Embodied AI,Other,https://arxiv.org/pdf/2512.05927,https://huggingface.co/papers/2512.05927,本文提出了一种名为C3的不确定性量化方法，用于训练可控视频生成模型，使其能够准确评估生成视频中每个局部区域的不确定性，从而有效识别和定位可能的错误或异常。该方法通过在潜在空间估计不确定性，避免了传统像素空间方法的训练难题，并将结果映射回像素空间以便直观展示。实验证明，C3不仅能提供校准良好的置信度估计，还能有效检测训练外的数据，提升了视频生成模型在机器人学习等领域的可靠性和实用性。,8
SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling,SpaceControl：引入测试时空间控制的3D生成建模,Other,Other,https://arxiv.org/pdf/2512.05343,https://huggingface.co/papers/2512.05343,本文提出了SpaceControl，一种无需额外训练即可在生成3D模型时实现精确空间控制的方法。该方法支持从简单几何形状到复杂网格的多种输入形式，允许用户在几何准确性与视觉真实感之间灵活权衡。通过大量评测和用户研究，SpaceControl在保持高视觉质量的同时，显著提升了生成模型对几何细节的忠实度。此外，作者还开发了交互式界面，方便用户在线编辑和生成带纹理的3D资产，促进其在创意设计中的实用应用。,8
ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning,ReVSeg：通过强化学习激励视频分割的推理链,Multimodal LLM,Other,https://arxiv.org/pdf/2512.02835,https://huggingface.co/papers/2512.02835,本文提出了ReVSeg，一种面向推理链的视频目标分割框架。该方法通过将推理过程拆解为语义理解、时间证据选择和空间定位三个步骤，利用预训练视觉语言模型进行多步决策，提升了对动态和因果关系的理解能力。通过强化学习优化推理链，模型能够基于结果反馈自我改进。实验表明，ReVSeg在多个视频分割基准测试中表现优异，同时具备良好的推理可解释性，推动了复杂视频理解任务的发展。,8
Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules,通过进度感知置信度调度实现扩散语言模型的快速解码,Diffusion Model,Other,https://arxiv.org/pdf/2512.02892,https://huggingface.co/papers/2512.02892,本文提出了一种名为SchED的早停算法，用于加速扩散式大语言模型（dLLMs）的解码过程。该方法无需额外训练，通过动态评估生成过程中的置信度，智能提前终止解码，从而显著提升推理速度。实验证明，SchED在多个任务和模型上均能实现约2至4倍的加速，同时保持接近原始模型的性能表现。相比现有方法，SchED在长文本生成中表现更为稳定且效果更优，极大提升了dLLMs在实际应用中的效率和实用性。,8
Exploring MLLM-Diffusion Information Transfer with MetaCanvas,基于MetaCanvas的MLLM-扩散信息传递探索,Multimodal LLM,Meta,https://arxiv.org/pdf/2512.11464,https://huggingface.co/papers/2512.11464,本文提出了MetaCanvas，一种轻量级框架，旨在充分发挥多模态大语言模型（MLLMs）在视觉生成中的推理和规划能力。不同于传统方法将MLLMs仅用作文本编码器，MetaCanvas使其直接在空间和时空潜在空间中进行推理，并与扩散生成模型紧密结合。通过在多种生成任务上的实验验证，MetaCanvas在布局精确性、属性绑定和复杂控制方面均优于现有方法，展示了将MLLMs作为潜在空间规划者以缩小多模态理解与生成差距的潜力。,8
V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions,V-REX：通过问题链评测探索性视觉推理能力,Multimodal LLM,Other,https://arxiv.org/pdf/2512.11995,https://huggingface.co/papers/2512.11995,本文提出了V-REX，一个用于评估视觉语言模型多步探索推理能力的测试套件。通过将复杂的视觉推理任务拆解为一系列连贯的问题链，V-REX分别考察模型在规划探索问题和逐步回答问题两个方面的表现。该方法不仅能细致分析模型在中间推理步骤中的能力，还揭示了当前模型在多步视觉推理上的不足和改进空间。V-REX为推动视觉语言模型在处理开放式复杂任务中的探索性思考提供了有效的评测工具。,8
SS4D: Native 4D Generative Model via Structured Spacetime Latents,SS4D：通过结构化时空潜变量的原生4D生成模型,Other,Shanghai AI Lab,https://arxiv.org/pdf/2512.14284,https://huggingface.co/papers/2512.14284,本文提出了SS4D，一种原生4D生成模型，能够直接从单目视频合成动态3D物体。与传统方法依赖3D或视频生成模型不同，SS4D通过结构化时空潜变量直接在4D数据上训练生成器，实现了高保真度、时间一致性和结构稳定性。为解决4D训练数据稀缺，方法基于预训练的单图像到3D模型，结合专门的时间层和时序压缩技术，支持长序列高效训练和推理。同时，设计了鲁棒的训练策略以应对遮挡和运动模糊。实验结果显示，SS4D在合成质量和效率上显著优于现有方法，适用于合成高质量动态3D内容。,8
End-to-End Training for Autoregressive Video Diffusion via Self-Resampling,通过自我重采样实现自回归视频扩散模型的端到端训练,Diffusion Model,ByteDance,https://arxiv.org/pdf/2512.15702,https://huggingface.co/papers/2512.15702,本文提出了一种名为“重采样强制”的新方法，用于端到端训练自回归视频扩散模型，以解决训练与推理阶段历史信息不一致导致的误差累积问题。该方法通过在训练时模拟模型推理中的错误，增强时间一致性，并引入无参数的历史信息路由机制以提高长视频生成效率。实验表明，该方法在保持生成质量的同时，显著提升了长时序视频的时间连贯性，且无需依赖教师模型，具备良好的扩展性和实用价值。,8
VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks,VenusBench-GD：面向多样化定位任务的多平台GUI综合基准,Agent,Other,https://arxiv.org/pdf/2512.16501,https://huggingface.co/papers/2512.16501,本文提出了VenusBench-GD，一套覆盖多平台、多语言的图形用户界面（GUI）定位基准，旨在解决现有数据集规模有限、领域单一的问题。该基准包含丰富多样的应用场景和界面元素，采用高质量的数据标注流程，并设计了分层任务体系，涵盖基础与高级六个子任务，全面评估模型的定位能力。实验表明，通用多模态模型在基础任务上表现优异，而高级任务仍需专用模型，且存在过拟合和鲁棒性不足的问题，凸显了多层次评测框架的重要性。,8
ModelTables: A Corpus of Tables about Models,ModelTables：关于模型的表格语料库,Other,Other,https://arxiv.org/pdf/2512.16106,https://huggingface.co/papers/2512.16106,本文提出了ModelTables，一个专注于AI模型性能与配置的结构化表格语料库，涵盖来自Hugging Face模型卡、GitHub README及相关论文的9万余张表，关联6万多个模型。该语料库通过论文引用、模型继承和共享训练数据三种信号构建多源关联标准，用于评估模型与表格的相关性。通过对表格检索任务的实验，展示了现有方法的性能及改进空间。ModelTables为模型性能数据的语义检索和结构化比较提供了首个大规模基准，有助于推动模型知识的系统化组织与高效利用。,8
Animate Any Character in Any World,在任意世界中驱动任意角色的动画生成,Embodied AI,Microsoft,https://arxiv.org/pdf/2512.17796,https://huggingface.co/papers/2512.17796,本文提出了AniX，一种能够在任意静态3D场景中驱动用户指定角色进行多样化动作和交互的视频生成方法。AniX支持通过自然语言指令控制角色的行为，包括行走、手势及与物体的互动，同时保持视频的时序连贯性和视觉一致性。该方法结合了静态场景的真实感与动态角色的自由动作，显著提升了动作表现的自然度和多样性。实验结果表明，AniX在视觉质量、角色一致性、动作可控性及长时间交互连续性方面均表现优异，展示了其在虚拟环境角色动画生成中的广泛应用潜力。,8
INTELLECT-3: Technical Report,INTELLECT-3：技术报告,Agent,Other,https://arxiv.org/pdf/2512.16144,https://huggingface.co/papers/2512.16144,本文介绍了INTELLECT-3，一款拥有1060亿参数的混合专家模型，采用大规模强化学习训练，展现了在数学、编程、科学和推理等多个领域的领先性能，优于许多更大规模的模型。作者同时开源了完整的训练基础设施，包括强化学习框架、训练流程及多样化的训练与评估环境。为支持大规模异步强化学习，团队开发了prime-rl框架，具备高效扩展性和对多轮交互及工具使用的支持。该工作为强化学习驱动的大型模型训练提供了完整且高效的解决方案。,8
Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations,从下一帧预测学习：自回归视频建模编码有效表征,Other,PKU,https://arxiv.org/pdf/2512.21004,https://huggingface.co/papers/2512.21004,本文提出了一种名为NExT-Vid的新型自回归视觉生成预训练框架，通过掩码下一帧预测方法联合建模图像和视频，有效利用时间信息。该方法引入上下文隔离的自回归预测器以分离语义表示与目标解码，并采用条件流匹配解码器提升生成质量和多样性。大规模实验表明，NExT-Vid在视觉表示学习任务中显著优于现有生成预训练方法，展示了其在捕捉视频语义信息和提升下游分类性能方面的优势。,8
From Word to World: Can Large Language Models be Implicit Text-based World Models?,从词到世界：大型语言模型能否作为隐式基于文本的世界模型？,Agent,Microsoft,https://arxiv.org/pdf/2512.18832,https://huggingface.co/papers/2512.18832,本文探讨了大型语言模型（LLM）能否作为隐式文本世界模型，帮助智能体在文本环境中提升学习效率。作者提出了一个三层评估框架，考察模型的准确性、一致性、扩展性及对智能体的实际帮助。通过五个代表性文本环境的实验，结果表明，经过充分训练的语言模型能够维持连贯的内部状态，且性能随着数据和模型规模提升而稳定增长。利用这些模型进行动作验证、合成轨迹生成和强化学习预热，智能体表现显著提升。但效果依赖于行为覆盖范围和环境复杂度，揭示了文本世界模型支持智能体学习的适用边界。,8
InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search,InSight-o3：通过广义视觉搜索赋能多模态基础模型,Multimodal LLM,Other,https://arxiv.org/pdf/2512.18745,https://huggingface.co/papers/2512.18745,本文提出了O3-Bench，一个用于评估多模态模型在复杂视觉细节推理任务中的新基准，针对当前模型在处理含密集图表和地图等实际场景中的推理能力不足的问题。为提升性能，作者设计了InSight-o3框架，结合视觉推理代理和视觉搜索代理，创新引入“广义视觉搜索”任务，能够定位描述中关系性、模糊或概念性的视觉区域。通过专门训练的多模态大模型，该方法显著提升了多模态系统的推理表现，为构建更强大的开放式视觉推理系统迈出重要一步。,8
VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs,VL-LN基准：面向具身智能的长时目标导向导航与主动对话,Embodied AI,Other,https://arxiv.org/pdf/2512.22342,https://huggingface.co/papers/2512.22342,本文提出了一个面向实际应用场景的导航任务——交互式目标导航（IION），该任务要求智能体在导航过程中通过主动对话解决指令中的模糊和歧义问题。基于此，作者构建了大规模的视觉语言对话导航基准（VL-LN），包含4万多条长距离对话轨迹及自动评测协议，用于训练和评估具备对话能力的导航模型。实验结果表明，利用VL-LN训练的模型在导航性能上显著优于传统方法，验证了该基准在推动具备主动交互能力的智能导航研究中的有效性和实用价值。,8
OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding,OmniAgent：用于全模态音视频理解的音频引导主动感知智能体,Agent,Other,https://arxiv.org/pdf/2512.23646,https://huggingface.co/papers/2512.23646,本文提出了OmniAgent，一种基于音频引导的主动感知智能体，旨在提升音视频多模态理解的精细度。与传统依赖固定流程的被动模型不同，OmniAgent通过动态规划，主动调用多种工具，聚焦任务相关的音频线索，辅助定位关键时间段并指导视觉推理。实验结果显示，OmniAgent在多个音视频理解基准测试中，准确率超越现有领先模型10%至20%，显著提升了跨模态对齐和细粒度推理能力，推动了多模态智能理解的发展。,8
YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection,YOLO-Master：基于专家混合模型加速的专用Transformer用于增强实时检测,Other,Tencent,https://arxiv.org/pdf/2512.23273,https://huggingface.co/papers/2512.23273,本文提出了YOLO-Master，一种基于YOLO架构的新型实时目标检测框架。该方法通过引入一种高效稀疏专家混合模块，能够根据场景复杂度动态分配计算资源，实现计算的自适应调整。核心设计包括一个轻量级的动态路由网络，促进专家间的互补专长并智能激活最相关的专家，从而提升检测性能的同时降低计算开销。实验结果表明，YOLO-Master在多个大型数据集上均优于现有方法，特别是在复杂密集场景中表现突出，且保持了实时检测的速度优势。,8
Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models,数字孪生AI：从大语言模型到世界模型的机遇与挑战,Agent,Other,https://arxiv.org/pdf/2601.01321,https://huggingface.co/papers/2601.01321,本文提出了一个涵盖建模、同步、干预和自主管理四个阶段的统一框架，系统阐述了人工智能技术在数字孪生全生命周期中的应用。通过结合物理模型与数据驱动方法，特别是大型语言模型和生成式AI，数字孪生从被动模拟工具转变为具备推理、沟通和自主决策能力的智能系统。论文还综述了数字孪生在多个领域的应用现状，分析了其在扩展性、可解释性和可信性等方面面临的挑战，并指出了未来推动数字孪生智能化和负责任发展的关键方向。,8
CoV: Chain-of-View Prompting for Spatial Reasoning,CoV：用于空间推理的链视角提示方法,Embodied AI,Other,https://arxiv.org/pdf/2601.05172,https://huggingface.co/papers/2601.05172,本文提出了一种名为Chain-of-View（CoV）的推理框架，旨在提升视觉语言模型在三维环境中回答空间相关问题的能力。CoV通过主动选择与问题相关的视角，并结合动态调整摄像机位置的策略，实现对场景的逐步细化观察，克服了传统模型视角固定、信息有限的限制。无需额外训练，CoV在多个数据集和模型上均显著提升了空间推理性能，验证了其作为一种通用且高效的三维空间理解方法的有效性。,8
Orient Anything V2: Unifying Orientation and Rotation Understanding,Orient Anything V2：统一的方向与旋转理解,Other,Shanghai AI Lab,https://arxiv.org/pdf/2601.05573,https://huggingface.co/papers/2601.05573,本文提出了Orient Anything V2，一种用于统一理解物体三维方向和旋转的基础模型。相比于前作仅识别唯一前脸方向，V2支持多种旋转对称性物体并能直接预测相对旋转。该模型通过生成多样化的三维数据、智能标注系统、对称性感知的分布拟合以及多帧结构实现性能提升。实验表明，Orient Anything V2在多个公开基准上实现了领先的零样本方向估计、六自由度姿态估计和对称性识别效果，展现出良好的泛化能力，促进了方向估计在机器人、自动驾驶等领域的应用。,8
"Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",相同论断，不同判断：多语言金融虚假信息检测中情景诱导偏见的基准测试,LLM,Other,https://arxiv.org/pdf/2601.05403,https://huggingface.co/papers/2601.05403,本文提出了MFMD-Scen，一种用于评估大型语言模型在多语言金融虚假信息检测中行为偏见的综合基准。通过与金融专家合作，构建了涵盖角色、个性、地区、族群和宗教信仰等复杂经济场景，并开发了包含英语、中文、希腊语和孟加拉语的多语言金融虚假信息数据集。基于该基准，系统评测了22个主流模型，结果显示无论商业还是开源模型均存在显著的行为偏见。该研究为理解和改进金融领域多语言虚假信息检测中的模型公正性和可靠性提供了重要工具和数据支持。,8
BizFinBench.v2: A Unified Dual-Mode Bilingual Benchmark for Expert-Level Financial Capability Alignment,BizFinBench.v2：面向专家级金融能力对齐的统一双模双语基准,LLM,Other,https://arxiv.org/pdf/2601.06401,https://huggingface.co/papers/2601.06401,本文提出了BizFinBench.v2，这是首个基于中美真实股市数据的大规模金融评测基准，涵盖多场景下的在线评估与专家级问答。该基准通过分析真实用户查询，设计了十项核心任务，包含近3万条高质量问答，旨在弥补现有评测多依赖模拟数据且缺乏实时性的不足。实验显示，尽管先进模型如ChatGPT-5表现突出，但仍与金融专家存在差距，在线任务中DeepSeek-R1表现最佳。BizFinBench.v2为评估和提升大语言模型在金融领域的实用能力提供了重要工具和参考。,8
TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning,TourPlanner：一种基于约束门控强化学习的竞争共识框架用于旅行规划,Agent,Other,https://arxiv.org/pdf/2601.04698,https://huggingface.co/papers/2601.04698,本文提出了TourPlanner，一种面向旅行规划的综合框架，旨在解决现有方法中候选兴趣点筛选效率低、多路径推理能力不足以及难以同时满足硬性和软性约束的问题。TourPlanner通过个性化召回与空间优化策略构建候选集，采用多路径竞争共识推理提升方案探索能力，并引入基于门控机制的强化学习动态平衡约束优先级。实验证明，该方法在规划合理性和用户偏好匹配度上均显著优于现有技术，推动了智能旅行规划的发展。,8
Dr. Zero: Self-Evolving Search Agents without Training Data,Dr. Zero：无需训练数据的自我进化搜索智能体,Agent,Meta,https://arxiv.org/pdf/2601.07055,https://huggingface.co/papers/2601.07055,本文提出了Dr. Zero，一种无需训练数据即可实现自我进化的大型语言模型搜索代理框架。该方法通过一个反馈循环，使模型自主生成多样且逐渐复杂的问题来训练自身，从而不断提升推理和搜索能力。为提高训练效率，作者引入了分组策略优化技术，有效降低计算资源消耗。实验表明，Dr. Zero在无需人工标注数据的情况下，能够达到甚至超越传统监督方法的性能，展示了语言模型通过自我演进实现复杂推理能力的潜力。,8
Forest Before Trees: Latent Superposition for Efficient Visual Reasoning,Forest Before Trees：用于高效视觉推理的潜在叠加方法,Multimodal LLM,Other,https://arxiv.org/pdf/2601.06803,https://huggingface.co/papers/2601.06803,本文提出了一种名为Laser的新型视觉推理方法，通过动态窗口对未来语义进行对齐，避免了传统方法中因逐点预测导致的信息丢失和语义提前崩溃问题。Laser采用“先整体后局部”的认知策略，在保持全局特征的同时逐步细化细节，提升了模型的推理能力和解释性。实验结果表明，Laser在六个基准测试中显著优于现有潜在推理方法，且推理效率大幅提升，展现出良好的泛化能力。该工作为视觉推理领域提供了高效且稳定的新思路。,8
Motion Attribution for Video Generation,视频生成中的运动归因,Other,Other,https://arxiv.org/pdf/2601.08828,https://huggingface.co/papers/2601.08828,本文提出了Motive，一种基于梯度的视频生成运动归因框架，专注于识别训练视频中对运动表现影响最大的片段。通过引入运动加权的损失掩码，Motive有效区分了运动动态与静态视觉信息，实现了运动特征的高效计算。该方法不仅帮助理解数据如何影响生成视频的时序动态，还指导了数据筛选，显著提升了视频生成的运动流畅性和物理合理性。在文本驱动的视频生成任务中，使用Motive选出的关键数据使模型获得了74.1%的用户偏好提升，展示了其在细粒度数据优化和运动质量提升方面的重要价值。,8
TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts,TAG-MoE：面向统一生成式专家混合模型的任务感知门控,Diffusion Model,Tencent,https://arxiv.org/pdf/2601.08881,https://huggingface.co/papers/2601.08881,本文提出了TAG-MoE，一种将任务语义信息注入专家混合模型（MoE）路由的新框架，旨在解决统一图像生成与编辑模型中的任务干扰问题。通过引入层级任务语义标注和预测对齐正则化，模型能够根据任务的高层意图调整专家选择，从而实现更有效的任务专门化。实验结果显示，TAG-MoE显著提升了生成图像的质量和一致性，克服了传统密集模型在多任务处理中的性能瓶颈，推动了图像生成与编辑技术的统一发展。,8
Inference-time Physics Alignment of Video Generative Models with Latent World Models,基于潜在世界模型的视频生成模型推理时物理对齐,Other,Meta,https://arxiv.org/pdf/2601.10553,https://huggingface.co/papers/2601.10553,本文提出了一种基于潜在世界模型的推理阶段物理对齐方法，用于提升视频生成模型的物理合理性。通过引入WMReward，将物理一致性作为奖励信号，在生成过程中搜索并引导多个候选路径，实现了测试时计算资源的有效利用。实验证明，该方法显著改善了多种条件下的视频物理合理性，并在ICCV 2025物理智能挑战赛中获得第一名，超越现有最佳水平。该研究展示了利用潜在世界模型提升视频生成物理真实性的潜力，对增强生成模型的实用性和可靠性具有重要意义。,8
EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge,EvasionBench：通过多模型共识与LLM作为裁判检测金融问答中的回避性回答,LLM,Other,https://arxiv.org/pdf/2601.09142,https://huggingface.co/papers/2601.09142,本文提出了EvasionBench，一个用于识别财报电话会议中回避性回答的大规模数据集，包含3万训练样本和千余高质量人工标注测试样本。作者设计了基于多个先进语言模型之间意见分歧的多模型标注框架，通过引入“裁判”模型解决标注冲突，有效挖掘难分样本，提升了模型的泛化能力和准确率。所训练的Eva-4B模型在保持低推理成本的同时，准确率达81.3%，显著优于基线，推动了财务问答中回避检测的自动化和透明度提升。,8
AI & Human Co-Improvement for Safer Co-Superintelligence,AI与人类共进化以实现更安全的共超智能,AGI,Meta,https://arxiv.org/pdf/2512.05356,https://huggingface.co/papers/2512.05356,本文提出了一种以人机协同提升为核心的AI发展策略，主张通过人类研究者与AI系统的紧密合作，共同推进AI研究，从构思到实验全流程协作。相比单纯追求AI自主自我改进，这种合作模式更切实可行且安全，有助于加速AI技术进步，同时降低潜在风险。论文强调通过人机共进实现“共超级智能”，不仅提升AI能力，也增强人类研究水平，从而推动更安全、更高效的AI发展路径。,7
ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation,ReCamDriving：无LiDAR的摄像头控制新轨迹视频生成,Other,Other,https://arxiv.org/pdf/2512.03621,https://huggingface.co/papers/2512.03621,本文提出了ReCamDriving，一种基于纯视觉信息的相机控制新轨迹视频生成框架。该方法利用密集且完整的三维场景渲染，结合两阶段训练策略，实现了对相机视角的精准控制和结构一致性。通过引入基于三维场景的跨轨迹数据整理，解决了训练与测试间的差异问题，支持从单目视频中扩展多轨迹监督。基于此，作者构建了包含11万余对平行轨迹视频的ParaDrive数据集。大量实验表明，ReCamDriving在相机可控性和生成质量上均达到了领先水平。,7
SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning,SPARK：面向无参考强化学习的逐步过程感知奖励,Agent,Amazon,https://arxiv.org/pdf/2512.03244,https://huggingface.co/papers/2512.03244,本文提出了SPARK，一种三阶段框架，通过生成器和验证器自动产生用于训练过程奖励模型的合成数据，实现无需参考答案的强化学习。该方法在数学推理任务中，利用多重独立验证的步骤级反馈，训练出的奖励模型表现优于依赖真实标签的传统方法。最终，结合链式思维验证和格式约束，SPARK在多个数学基准测试中显著提升了模型准确率，展示了在缺乏明确答案领域中，参考自由强化学习的有效性和潜力。,7
"See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",看、听与理解：多模态大语言模型中视听人类语音理解的基准测试,Multimodal LLM,Other,https://arxiv.org/pdf/2512.02231,https://huggingface.co/papers/2512.02231,本文提出了AV-SpeakerBench，一个专注于说话者为中心的视听融合理解的基准测试，包含3212道多项选择题，旨在评估多模态大语言模型对视频中“谁说话”、“说了什么”及“何时发生”的细粒度推理能力。该基准通过精心设计的问题和专家标注，确保跨模态信息的准确对齐。实验结果显示，Gemini系列模型在该任务上表现领先，尤其是Gemini 2.5 Pro。AV-SpeakerBench为推动未来多模态系统在视听语言理解方面的深入发展提供了坚实基础。,7
EtCon: Edit-then-Consolidate for Reliable Knowledge Editing,EtCon：面向可靠知识编辑的编辑-再整合方法,LLM,Other,https://arxiv.org/pdf/2512.04753,https://huggingface.co/papers/2512.04753,本文提出了一种名为“Edit-then-Consolidate”的知识编辑框架，旨在解决大规模语言模型在知识更新过程中容易过拟合新信息且缺乏有效整合的问题。该方法通过有针对性的微调限制模型偏移，避免损害原有能力；随后通过策略优化阶段，将新知识更好地融入模型生成过程。实验结果表明，该框架在实际应用中显著提升了知识编辑的可靠性和泛化能力，有助于实现更高效且稳定的模型知识更新。,7
Rethinking Chain-of-Thought Reasoning for Videos,重新思考视频的Chain-of-Thought推理,Multimodal LLM,Other,https://arxiv.org/pdf/2512.09616,https://huggingface.co/papers/2512.09616,"本文针对视频推理中链式思维（Chain-of-Thought, CoT）方法存在的计算资源消耗大和推理链条冗长的问题，提出了一种高效的后训练和推理框架。该框架通过压缩视觉输入和生成简洁的推理过程，实现了推理效率的大幅提升，同时保持了在多个视频理解任务上的竞争性能。研究表明，较短且精炼的推理链条结合视觉信息压缩，能够有效替代传统长链式推理，避免了对人工标注和监督微调的依赖，推动了视频多模态模型的实用性和可持续发展。",7
"DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",DrivePI：面向统一自动驾驶理解、感知、预测与规划的空间感知4D多模态大语言模型,Multimodal LLM,Other,https://arxiv.org/pdf/2512.12799,https://huggingface.co/papers/2512.12799,本文提出了DrivePI，一种融合点云、多视角图像和语言指令的空间感知4D多模态大语言模型，用于实现自动驾驶中的统一空间理解、三维感知、运动预测和路径规划。该模型通过端到端训练，能够同时处理多任务，显著提升了感知和决策的准确性与安全性。实验结果表明，DrivePI在多个自动驾驶基准测试中优于现有视觉-语言-动作及专用视觉-动作模型，展示了其在自动驾驶领域多模态融合与综合能力上的先进水平。代码已开源，便于后续研究和应用。,7
Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection,迈向具身视觉：学习视觉驱动的主动视角选择,Embodied AI,Other,https://arxiv.org/pdf/2512.13250,https://huggingface.co/papers/2512.13250,本文提出了Visually Grounded Active View Selection（VG-AVS）任务和框架，旨在使视觉语言模型（VLMs）具备主动选择最有信息量视角的能力，从而提升视觉问答的表现。该方法仅依赖当前图像信息，无需场景记忆或外部知识，通过构建合成数据集和结合监督微调与强化学习优化，实现了在合成和真实场景中的良好泛化能力。将VG-AVS集成到现有系统中，可显著提高下游问答的准确性，推动了具备移动视角能力的智能视觉系统的发展。,7
A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning,A4-Agent：一种用于零样本可供性推理的智能体框架,Agent,Other,https://arxiv.org/pdf/2512.14442,https://huggingface.co/papers/2512.14442,本文提出了A4-Agent，一种无需训练的三阶段框架，用于基于语言指令预测物体的可交互区域。该方法将任务分解为：利用生成模型想象交互场景（Dreamer）、通过视觉语言模型确定交互部位（Thinker）、以及精确定位交互区域（Spotter）。通过整合多个预训练模型的优势，A4-Agent实现了零样本推理，在多个基准测试中显著优于现有的有监督方法，且在真实环境中表现出强大的泛化能力，推动了具身智能中可操作性理解的发展。,7
VABench: A Comprehensive Benchmark for Audio-Video Generation,VABench：一个用于音视频生成的综合基准测试,Multimodal LLM,PKU,https://arxiv.org/pdf/2512.09299,https://huggingface.co/papers/2512.09299,本文提出了VABench，一个针对音视频生成模型的综合评测框架，涵盖文本到音视频、图像到音视频及立体音视频三大任务类型。VABench设计了15个评估维度，系统衡量音视频内容的相似性、同步性及唇语一致性等关键指标，覆盖动物、音乐、环境等七类内容场景。该基准填补了现有评测体系中缺乏同步音视频综合评价的空白，推动了多模态生成技术的标准化评估和发展。,7
Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs,Hearing to Translate：将语音模态集成到大语言模型中的有效性研究,Multimodal LLM,Other,https://arxiv.org/pdf/2512.16378,https://huggingface.co/papers/2512.16378,本文提出了“Hearing to Translate”测试套件，首次系统评估了五种先进的语音大语言模型（SpeechLLMs）与十六种结合多语言大语言模型的直接及级联语音翻译系统的性能。通过覆盖多语言、多场景及复杂语音条件的广泛测试，研究发现传统的级联系统在整体翻译质量和可靠性上仍优于当前SpeechLLMs，后者仅在部分场景表现相当。结果强调了无论是在模型内部还是流水线中集成大语言模型，对于实现高质量语音翻译的重要性。该工作为语音翻译技术的发展提供了全面的基准和深入的见解。,7
Bolmo: Byteifying the Next Generation of Language Models,Bolmo：下一代语言模型的字节化,LLM,Other,https://arxiv.org/pdf/2512.15586,https://huggingface.co/papers/2512.15586,本文提出了Bolmo，一种基于字节级别的语言模型家族，通过将已有的子词级模型转换为字节级模型，实现了对字符的更细粒度理解和更高的效率。Bolmo采用专门设计的架构，支持在极低的训练成本下完成模型转换，并在多个任务上表现出优于同规模字节级模型且接近原子词模型的性能。该方法还兼顾了推理速度和后续训练的便利性，推动了字节级语言模型在实际应用中的可行性与竞争力。,7
"MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",MobileWorld：在智能体-用户交互及MCP增强环境中的自主移动智能体基准测试,Agent,Alibaba,https://arxiv.org/pdf/2512.19432,https://huggingface.co/papers/2512.19432,本文提出了MobileWorld，一个比现有AndroidWorld更具挑战性的移动任务基准，涵盖201个任务和20个应用，真实反映了复杂的跨应用操作、模糊用户指令及混合工具使用场景。MobileWorld强调长任务链和用户交互，支持新的任务类型如代理-用户互动及模型上下文协议（MCP）增强任务。实验结果显示当前模型在该基准上的表现显著下降，揭示了用户交互和MCP调用的挑战，指出了未来提升移动智能代理能力的关键方向。,7
How Much 3D Do Video Foundation Models Encode?,视频基础模型编码了多少三维信息？,Other,Other,https://arxiv.org/pdf/2512.19949,https://huggingface.co/papers/2512.19949,本论文探讨了视频基础模型在仅通过大量二维视频数据训练后，是否能自发具备三维理解能力。作者提出了首个模型无关的评估框架，通过浅层读出方法量化多种三维属性，系统评估了多款视频基础模型的三维感知水平。研究结果表明，当前最先进的视频生成模型在三维物体和场景理解上表现出强大能力，甚至超越了专门训练的三维专家模型。该工作为利用二维视频数据构建可扩展的三维模型提供了重要参考和新思路。,7
Monadic Context Engineering,Monad上下文工程,Agent,Other,https://arxiv.org/pdf/2512.22431,https://huggingface.co/papers/2512.22431,本文提出了一种名为“单子上下文工程”（Monadic Context Engineering，MCE）的新型智能体架构方法，旨在解决当前自主智能体在状态管理、错误处理和并发执行中的脆弱性问题。MCE利用数学上的代数结构，将智能体的工作流程视为计算上下文，内在管理状态传递、错误短路和异步执行等复杂问题。该方法支持顺序与并行操作的有序组合，并通过层叠结构实现功能模块的系统整合，提升智能体的鲁棒性和扩展性。此外，论文还扩展了该框架，支持通过元编程动态生成和管理子智能体工作流，推动更复杂的智能体协同与自动化。,7
Nested Browser-Use Learning for Agentic Information Seeking,面向自主信息检索的嵌套浏览器使用学习,Agent,Alibaba,https://arxiv.org/pdf/2512.23647,https://huggingface.co/papers/2512.23647,本文提出了一种名为Nested Browser-Use Learning（NestBrowse）的新方法，旨在提升信息检索智能体的浏览能力。传统信息检索智能体主要依赖API调用和URL抓取，难以充分利用网页中的丰富信息。NestBrowse通过引入一种嵌套结构，将浏览器操作与页面探索分离，简化了智能体的决策过程，实现了更有效的深层网页信息获取。实验证明，NestBrowse在复杂的信息检索任务中表现优异，展现出较高的效率和灵活性，推动了智能体在真实浏览环境中的应用发展。,7
SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling,SurgWorld：通过世界建模从视频中学习手术机器人策略,Embodied AI,Other,https://arxiv.org/pdf/2512.23162,https://huggingface.co/papers/2512.23162,本文提出了SurgWorld，一种基于世界模型的手术机器人学习方法，旨在解决手术机器人训练中数据稀缺的问题。研究团队构建了包含详细动作描述的SATA数据集，并利用先进的物理世界模型生成多样且逼真的手术视频。通过逆向动力学模型从合成视频中推断伪运动数据，生成配对的视觉与动作训练数据。实验表明，利用这些增强数据训练的机器人策略在真实手术机器人平台上显著优于仅依赖真实示范的数据。该方法为利用丰富的无标签手术视频实现自主手术技能学习提供了有效途径，推动了手术机器人策略的泛化与数据效率提升。,7
Confidence Estimation for LLMs in Multi-turn Interactions,多轮交互中大型语言模型的置信度估计,LLM,Other,https://arxiv.org/pdf/2601.02179,https://huggingface.co/papers/2601.02179,本文首次系统研究了大型语言模型在多轮对话中的置信度估计问题，提出了基于每轮校准和置信度单调性两个关键指标的评估框架。为此，作者设计了新的评测指标和“提示-猜测”数据生成方法，揭示现有置信度估计技术在多轮交互中表现不足。论文进一步提出了一种基于模型输出概率的新探针方法，显著提升了置信度的可靠性。该研究为构建更可信赖的对话系统提供了理论基础和实践指导。,7
Choreographing a World of Dynamic Objects,编排动态物体的世界,Embodied AI,Other,https://arxiv.org/pdf/2601.04194,https://huggingface.co/papers/2601.04194,本文提出了CHORD，一种通用的生成框架，能够从二维视频中提取运动信息，合成包含多物体交互的四维（3D+时间）动态场景。不同于传统依赖类别规则的图形方法和需要大量数据的学习方法，CHORD无需特定类别知识或大规模数据，通过蒸馏技术从视频中捕捉丰富的运动特征，实现了对多样动态物体的高效建模。实验表明该方法在生成复杂动态场景和机器人操作策略方面表现出显著优势，具备广泛的适用性和扩展潜力。,7
SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices,SnapGen++：释放扩散变换器在边缘设备上的高效高保真图像生成能力,Diffusion Model,Other,https://arxiv.org/pdf/2601.08303,https://huggingface.co/papers/2601.08303,本文提出了SnapGen++，一种专为移动和边缘设备设计的高效扩散变换器框架。通过引入紧凑的模型架构、灵活的训练机制及知识引导的蒸馏方法，该框架在资源受限环境下实现了与大型变换器相当的高质量图像生成。该方法不仅显著降低了计算和内存需求，还支持动态调整模型规模以适配不同硬件，具备实时生成能力，推动了高保真图像生成技术在多样化设备上的实用应用。,7
ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios,ViDoRe V3：复杂真实场景下检索增强生成的综合评估,Multimodal LLM,Other,https://arxiv.org/pdf/2601.08620,https://huggingface.co/papers/2601.08620,"本文提出了ViDoRe V3，一个涵盖多种文档类型和语言的多模态检索增强生成基准，旨在评估模型在复杂真实场景下处理视觉元素（如表格、图表、图片）和跨文档信息综合的能力。该基准包含来自10个专业领域的约2.6万页文档和3,099条多语言人工验证查询，并附有高质量的检索相关性和视觉定位标注。实验证明，视觉检索和后期交互模型显著提升性能，但当前系统在处理非文本信息和开放式查询上仍存在不足。该基准为推动多模态检索生成技术进步提供了重要资源。",7
LSRIF: Logic-Structured Reinforcement Learning for Instruction Following,LSRIF：面向指令跟随的逻辑结构化强化学习,LLM,Other,https://arxiv.org/pdf/2601.06431,https://huggingface.co/papers/2601.06431,本论文提出了一种逻辑结构强化学习框架LSRIF，用于提升大语言模型的指令执行能力。该方法通过构建包含并行、顺序和条件逻辑结构的训练数据集LSRInstruct，并设计针对不同逻辑结构的奖励机制，有效捕捉指令中的逻辑依赖关系。实验结果表明，LSRIF显著提升了模型在指令遵循和推理任务中的表现，且分析显示该方法促进了模型注意力机制对逻辑约束的聚焦，增强了模型对复杂指令的理解与执行能力。,7
M3DR: Towards Universal Multilingual Multimodal Document Retrieval,M3DR：迈向通用多语言多模态文档检索,Multimodal LLM,Other,https://arxiv.org/pdf/2512.03514,https://huggingface.co/papers/2512.03514,本文提出了M3DR，一种面向多语言多模态文档检索的框架，旨在突破现有系统以英语为中心的局限，实现跨语言和跨模态的高效对齐。通过利用合成多语言文档数据和对比训练，M3DR能够学习统一的文本与图像表示，支持多种视觉语言模型架构。作者在22种语言上验证了该方法的稳定性和适应性，并构建了涵盖单语、多语及混合语言场景的综合评测基准。所提出的模型在跨语言检索任务中实现了约150%的性能提升，展示了其广泛应用潜力。,6
VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning,VG-Refiner：通过智能体强化学习实现工具精炼的指称定位推理,Agent,THU,https://arxiv.org/pdf/2512.06373,https://huggingface.co/papers/2512.06373,本文提出了VG-Refiner框架，针对现有视觉推理模型在工具输出不可靠时易产生错误推理的问题，设计了一种两阶段的“思考-再思考”机制，使模型能够主动分析并修正工具反馈中的错误。通过引入专门的奖励机制和新的评估指标，VG-Refiner显著提升了指称和定位任务的准确性和纠错能力，同时保持了预训练模型的整体性能。该方法在多个基准测试中表现优异，展示了在多模态视觉推理中有效整合和优化工具输出的潜力。,6
Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning,超越Token级监督：通过强化学习释放基于解码的回归潜力,LLM,Other,https://arxiv.org/pdf/2512.06533,https://huggingface.co/papers/2512.06533,本文提出了一种基于强化学习的方法，提升了解码式回归模型在数值预测任务中的表现。传统方法依赖逐个预测单位的目标，难以准确反映连续数值的整体特征，限制了预测的精度和泛化能力。通过将预测过程视为决策过程，并引入序列级的奖励信号，作者的方法有效增强了模型的全局数值一致性。实验证明，该方法在多种回归任务中优于现有技术，显著提升了采样效率和预测精度，展示了解码式回归在通用数值预测中的潜力和优势。,6
DeepCode: Open Agentic Coding,DeepCode：开放式自主编码,Agent,Other,https://arxiv.org/pdf/2512.07921,https://huggingface.co/papers/2512.07921,本文提出了DeepCode，一种完全自主的代码生成框架，专注于将复杂文档（如科学论文）高质量转化为完整代码库。通过优化信息传递，DeepCode结合源代码压缩、结构化索引、知识注入和错误纠正四大策略，有效解决了大语言模型在处理信息过载与上下文限制时的矛盾。实验证明，DeepCode在多个基准测试中超越了主流商业工具和顶尖博士专家，显著提升了文档到代码的合成质量，为自动化科学研究复现和软件开发开辟了新路径。,6
COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision,COREA：通过双向3D到3D监督实现可重光3D高斯与符号距离场的粗到细3D表示对齐,Other,Other,https://arxiv.org/pdf/2512.07107,https://huggingface.co/papers/2512.07107,本文提出了COREA，一种创新的统一框架，首次实现了可光照调节的3D高斯表示与有符号距离场（SDF）的联合学习。通过一种由粗到细的双向三维对齐策略，COREA直接在三维空间中优化几何结构，显著提升了几何重建的精度和光照分解的稳定性。此外，密度控制机制有效平衡了几何细节与资源消耗。实验结果表明，COREA在新视角合成、网格重建和物理光照渲染任务中均表现优异，展示了其在三维重建与光照领域的广泛应用潜力。,6
"UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",UniUGP：统一理解、生成与规划的端到端自动驾驶框架,Agent,ByteDance,https://arxiv.org/pdf/2512.09864,https://huggingface.co/papers/2512.09864,本文提出了UniUGP，一种统一的端到端自动驾驶框架，通过融合视觉语言模型和视频生成技术，实现对复杂场景的理解、未来视频预测和轨迹规划。作者构建了多个包含推理与规划标注的专用数据集，设计了分阶段训练策略，使模型能够利用多帧观测和语言指令，生成具备连贯推理过程、物理一致性轨迹及合理未来场景的视频。实验结果表明，UniUGP在感知、推理和决策方面表现优异，且在应对长尾复杂驾驶场景时具备较强的泛化能力。,6
WonderZoom: Multi-Scale 3D World Generation,WonderZoom：多尺度三维世界生成,Other,Other,https://arxiv.org/pdf/2512.09164,https://huggingface.co/papers/2512.09164,本文提出WonderZoom，一种能够从单张图像生成多尺度3D场景的新方法。该方法通过引入适应不同空间尺度的高斯表面元素和逐步细节合成器，实现了从宏观环境到微观细节的连续生成和实时渲染。用户可交互式放大场景任意区域，自动生成此前不存在的细节内容，保持跨尺度一致性。实验结果表明，WonderZoom在生成质量和内容对齐度上显著优于现有单尺度3D生成模型，推动了多尺度3D世界的创建与探索。,6
MeshSplatting: Differentiable Rendering with Opaque Meshes,MeshSplatting：基于不透明网格的可微渲染,Other,Other,https://arxiv.org/pdf/2512.06818,https://huggingface.co/papers/2512.06818,本文提出了MeshSplatting，一种基于网格的可微分渲染方法，通过联合优化几何形状和外观，实现高质量的新视角合成。该方法通过限制性三角剖分保证网格连通性，提升表面一致性，生成适用于实时3D引擎的光滑不透明网格。相比现有技术，MeshSplatting在保持更高视觉质量的同时，训练速度提升两倍，内存使用减少一半，促进了神经渲染与交互式三维图形的融合，便于在游戏和AR/VR等应用中实现实时场景交互。,6
VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer,VLSA：具备即插即用安全约束层的视觉-语言-动作模型,Embodied AI,"THU, Alibaba",https://arxiv.org/pdf/2512.11891,https://huggingface.co/papers/2512.11891,本文提出了一种名为AEGIS的视觉-语言-动作安全架构（VLSA），通过引入可插拔的安全约束层，有效提升机器人操作任务中的安全性和执行效果。该架构基于控制屏障函数，能与现有视觉-语言-动作模型无缝集成，保证避免碰撞的同时保持任务指令的准确执行。为验证方法有效性，作者构建了包含多样复杂场景的安全关键基准测试SafeLIBERO。实验结果显示，AEGIS在障碍物规避率和任务成功率上分别提升了59.16%和17.25%，显著优于现有方法。论文还公开了相关代码和数据，推动后续研究发展。,6
Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models,Sparse-LaViDa：稀疏多模态离散扩散语言模型,Multimodal LLM,Other,https://arxiv.org/pdf/2512.14008,https://huggingface.co/papers/2512.14008,本文提出了Sparse-LaViDa，一种加速多模态掩码离散扩散模型（MDM）推理的新方法。通过动态减少推理过程中不必要的掩码标记，并引入专门的寄存标记保持信息完整，Sparse-LaViDa显著提升了模型采样速度，同时保证生成质量。该方法还设计了匹配训练和推理过程的注意力机制，确保一致性。基于先进的统一MDM模型LaViDa-O，Sparse-LaViDa在文本生成图像、图像编辑和数学推理等多项任务中实现了最高2倍的加速，展现出高效且实用的多模态生成能力。,6
In Pursuit of Pixel Supervision for Visual Pre-training,追求像素监督的视觉预训练,Other,Meta,https://arxiv.org/pdf/2512.15715,https://huggingface.co/papers/2512.15715,本文提出了Pixio，一种改进的基于像素的自监督视觉预训练模型，采用增强的掩码自编码器结构和更具挑战性的预训练任务。通过在20亿张网络图片上进行自我筛选训练，Pixio在多项实际应用中表现出色，包括单目深度估计、三维重建、语义分割和机器人学习，性能优于或匹配当前主流的潜变量空间方法。研究表明，直接利用像素级信息进行自监督学习是一种有效且稳定的替代方案，为视觉表征学习提供了新的思路。,6
SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories,SWE-Bench++：一个用于从开源仓库可扩展生成软件工程基准的框架,LLM,Other,https://arxiv.org/pdf/2512.17419,https://huggingface.co/papers/2512.17419,本文提出了SWE-Bench++，一个自动化框架，用于从开源GitHub项目中生成大规模、多语言的软件工程代码任务基准。该框架通过实时抓取拉取请求，覆盖了11种编程语言中的错误修复和功能请求，生成可复现且基于执行的测试任务。SWE-Bench++包含超过一万一千个实例，显著提升了任务规模和多样性。实验表明，当前最强模型在该基准上的表现仍有提升空间，且基于该数据集的微调能有效提升多语言代码生成能力。该工作为评估和改进代码生成模型提供了一个更全面、可扩展的测试平台。,6
StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models,StageVAR：面向视觉自回归模型的阶段感知加速方法,Other,Other,https://arxiv.org/pdf/2512.16483,https://huggingface.co/papers/2512.16483,本文提出了StageVAR，一种针对视觉自回归模型的阶段感知加速框架。通过系统分析发现，生成过程中的早期阶段对保持图像的语义和结构至关重要，应完整保留；而后期阶段主要负责细节调整，可以通过剪枝或近似计算加速处理。StageVAR利用这一特点，无需额外训练即可插拔式地优化后期计算，显著提升生成速度（最高3.4倍加速）且几乎不损失图像质量。该方法相比现有加速技术更高效，展示了基于阶段重要性设计的视觉自回归模型加速新思路。,6
DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation,DramaBench：一个用于戏剧剧本续写的六维度评估框架,LLM,Other,https://arxiv.org/pdf/2512.19012,https://huggingface.co/papers/2512.19012,本文提出了DramaBench，一个首创的大规模评测框架，用于多维度评价戏剧剧本续写质量。该框架涵盖格式规范、叙事效率、角色一致性、情感深度、逻辑连贯性和冲突处理六个独立维度，结合规则分析与大语言模型标注，实现客观且可复现的评估。通过对8个先进语言模型的1103个剧本进行全面测试，并辅以统计显著性分析和人工验证，证明了各维度的独立性和评测的可靠性。DramaBench为剧本续写模型提供具体改进建议，推动创意写作评测标准化发展。,6
VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation,VA-π：用于像素感知自回归生成的变分策略对齐,Other,Other,https://arxiv.org/pdf/2512.19680,https://huggingface.co/papers/2512.19680,本文提出了VA-π，一种轻量级的后训练框架，通过直接优化像素空间目标，解决了自回归视觉生成模型中生成图像质量与离散令牌序列不匹配的问题。VA-π将生成器与令牌器的对齐视为变分优化问题，结合像素重建和序列建模，利用基于像素重建质量的内在奖励对生成策略进行强化学习优化，无需重新训练令牌器或依赖外部奖励。该方法在少量数据和短时间调优下显著提升了图像质量指标，验证了其高效且实用的图像生成性能提升能力。,6
SlideTailor: Personalized Presentation Slide Generation for Scientific Papers,SlideTailor：面向科学论文的个性化演示幻灯片生成,Agent,Other,https://arxiv.org/pdf/2512.20292,https://huggingface.co/papers/2512.20292,本文提出了SlideTailor，一种基于用户隐式偏好生成个性化科学论文演示幻灯片的框架。该系统通过用户提供的论文-幻灯片示例对和视觉模板，自动提取内容与风格偏好，逐步生成可编辑的幻灯片，避免了繁琐的偏好描述。同时引入链式口语机制，使幻灯片内容与口头讲解紧密匹配，提升展示效果。为支持该任务，作者构建了包含多样用户偏好的评测数据集和指标。大量实验验证了SlideTailor在满足个性化需求和生成质量上的有效性，显著简化了演示文稿制作流程。,6
An Information Theoretic Perspective on Agentic System Design,基于信息论视角的Agentic系统设计,Agent,Other,https://arxiv.org/pdf/2512.21720,https://huggingface.co/papers/2512.21720,本文从信息理论视角系统分析了多语言模型架构中“压缩器-预测器”设计模式，提出将压缩器视为信息传输的噪声通道，通过互信息指标量化压缩质量，独立于具体任务预测下游性能。实验证明，较大压缩器不仅更准确且更高效，传递的信息密度更大，且扩展压缩器比扩展预测器更有效。该方法在实际应用中显著降低了API调用成本，同时保持了接近最先进模型的性能，指导了高效且经济的多模型系统设计。,6
Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting,分位数渲染：在三维高斯点渲染中高效嵌入高维特征,Other,Other,https://arxiv.org/pdf/2512.20927,https://huggingface.co/papers/2512.20927,本文提出了一种名为Quantile Rendering（Q-Render）的新型渲染方法，有效解决了基于3D高斯点阵表示中高维特征渲染效率低下的问题。与传统方法不同，Q-Render通过稀疏采样对渲染贡献最大的高斯点，实现了高效且高保真的特征嵌入。基于此，作者设计了具备良好泛化能力的Gaussian Splatting Network（GS-Net），显著提升了开放词汇分割任务的性能。实验证明，该方法在多个数据集上优于现有技术，且在512维特征渲染上实现了约43倍的实时加速，推动了三维场景理解与渲染的应用发展。,6
End-to-End Test-Time Training for Long Context,面向长上下文的端到端测试时训练方法,LLM,Other,https://arxiv.org/pdf/2512.23675,https://huggingface.co/papers/2512.23675,本文将长上下文语言建模视为持续学习问题，而非单纯依赖架构设计。作者采用标准的滑动窗口Transformer架构，并通过测试时的下一词预测不断更新模型权重，实现对上下文的动态压缩。同时，训练阶段引入元学习优化模型初始化，使测试时学习更高效。该端到端的测试时训练方法（TTT-E2E）在保持与全注意力机制相当的性能的同时，推理延迟恒定且显著低于全注意力，尤其在超长上下文（如12.8万词）时速度提升2.7倍。实验验证了该方法在大规模模型和长上下文下的优越扩展性。,6
Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process,[翻译]Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process,Other,Other,https://arxiv.org/pdf/2512.23988,https://huggingface.co/papers/2512.23988,无法生成摘要。,6
Pretraining Frame Preservation in Autoregressive Video Memory Compression,[翻译]Pretraining Frame Preservation in Autoregressive Video Memory Compression,Other,Other,https://arxiv.org/pdf/2512.23851,https://huggingface.co/papers/2512.23851,本文提出了一种名为PFP的神经网络结构，旨在将长视频压缩为较短的上下文表示，同时在预训练阶段专注于保持任意时刻单帧的高频细节。该方法能够将20秒的视频压缩至约5000长度的上下文中，支持随机帧的高保真恢复。预训练模型可直接微调用于自回归视频生成的记忆编码，实现在保持较低上下文成本的同时，延长视频历史记忆且减少画质损失。实验验证了该框架的有效性，并探讨了不同网络设计的权衡。,6
Guiding a Diffusion Transformer with the Internal Dynamics of Itself,利用自身内部动力学引导扩散Transformer,Diffusion Model,Other,https://arxiv.org/pdf/2512.24176,https://huggingface.co/papers/2512.24176,"本文提出了一种名为“内部引导”（Internal Guidance, IG）的简洁有效策略，用于提升扩散变换器模型的图像生成质量。该方法通过在训练过程中对中间层施加辅助监督，并在采样时结合中间层和深层输出，显著提高了训练效率和生成效果。实验证明，IG在多个基线模型上均取得了显著性能提升，尤其是在ImageNet 256×256数据集上，结合现有指导方法可实现当前最先进的生成质量。该工作为扩散模型的优化提供了一种新颖且实用的思路。",6
Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow,Dream2Flow：通过3D物体流桥接视频生成与开放世界操作,Embodied AI,Other,https://arxiv.org/pdf/2512.24766,https://huggingface.co/papers/2512.24766,本文提出了Dream2Flow，一种将视频生成模型与机器人操作结合的新框架。该方法通过从生成的视频中提取三维物体运动信息，将复杂的物体状态变化转化为可执行的机器人动作指令，实现了无需专门示范的零样本操作。Dream2Flow有效解决了机器人物理执行与视觉预测之间的差异，支持多种类型物体的操作。仿真和真实环境实验验证了其作为视频模型与开放世界机器人操作桥梁的通用性和可扩展性，为机器人在未知环境中完成多样化任务提供了新的思路。,6
WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks,WebGym：利用真实任务扩展视觉网页智能体的训练环境规模,Agent,Microsoft,https://arxiv.org/pdf/2601.02439,https://huggingface.co/papers/2601.02439,本文提出了WebGym，一个规模最大且开源的视觉网页代理训练环境，包含近30万个真实且多样化的任务，旨在提升代理在未见网站上的泛化能力。通过设计高效的异步采样系统，加速强化学习训练过程，实现了4-5倍的速度提升。基于WebGym训练的视觉语言模型在全新测试集上的成功率显著提高，超过了多种专有模型，展示了其在真实网页任务中训练稳健且通用策略的潜力。,6
E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models,E-GRPO：高熵步骤驱动流模型的有效强化学习,Diffusion Model,THU,https://arxiv.org/pdf/2601.00423,https://huggingface.co/papers/2601.00423,本文提出了一种名为E-GRPO的强化学习方法，旨在提升基于流模型的图像生成效果。该方法通过聚合低熵的采样步骤，形成高熵采样步骤，从而增强探索能力并缓解奖励信号稀疏和模糊的问题。同时，引入多步组内归一化优势估计，提升策略优化的稳定性和效率。实验结果表明，E-GRPO在不同奖励设置下均能有效改进生成质量，展示了其在视觉生成任务中提高人类偏好对齐的潜力。,6
MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics,MDAgent2：用于分子动力学中的代码生成与知识问答的大型语言模型,AI4Science,PKU,https://arxiv.org/pdf/2601.02075,https://huggingface.co/papers/2601.02075,本文提出了MDAgent2，一种面向分子动力学领域的端到端大语言模型框架，能够实现专业知识问答和自动代码生成。通过构建高质量领域数据集，并采用多阶段训练策略，作者训练出两个适应分子动力学任务的模型。创新地引入了基于模拟结果的闭环强化学习方法，实现模型自我优化。此外，开发了集代码生成、执行和自我纠错于一体的多智能体系统，并设计了首个针对LAMMPS代码生成与问答的评测基准。该工作展示了大语言模型在工业级科学模拟中的应用潜力，为自动化科学计算提供了重要方法基础。,6
Agentic Rubrics as Contextual Verifiers for SWE Agents,Agentic Rubrics作为软件工程智能体的上下文验证器,Agent,Other,https://arxiv.org/pdf/2601.04171,https://huggingface.co/papers/2601.04171,本文提出了一种名为Agentic Rubrics的方法，用于软件工程智能体的高效验证。该方法通过让专家智能体基于代码库上下文生成核查清单，从而无需实际执行代码即可评估代码修改的质量。实验证明，Agentic Rubrics在多个大型模型上相较传统验证手段有显著提升，且评分结果与真实测试高度一致，同时还能发现测试未覆盖的问题。该方法实现了验证过程的可扩展性和细粒度判别，为软件工程智能体的训练和推理提供了更可靠的支持。,6
AnyDepth: Depth Estimation Made Easy,AnyDepth：轻松实现深度估计,Other,PKU,https://arxiv.org/pdf/2601.02760,https://huggingface.co/papers/2601.02760,本文提出了AnyDepth，一种轻量且高效的单目深度估计框架。该方法采用DINOv3作为视觉编码器，结合设计简洁的变换器解码器，有效减少模型参数和计算量，同时提升深度估计精度。通过引入基于质量的样本筛选策略，优化训练数据质量，进一步提高模型泛化能力。实验结果表明，AnyDepth在多个基准测试中优于现有复杂模型，实现了精度与效率的良好平衡，为零样本深度估计提供了实用且通用的解决方案。,6
VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory,VLingNav：具身导航中的自适应推理与视觉辅助语言记忆,Embodied AI,"ByteDance, PKU",https://arxiv.org/pdf/2601.08665,https://huggingface.co/papers/2601.08665,本文提出了VLingNav，一种结合语言驱动认知的机器人导航模型，旨在提升复杂环境中的导航能力。该模型通过自适应推理机制，在需要时动态切换快速直觉执行与深度规划；同时引入视觉辅助的语言记忆模块，实现对环境信息的长期记忆与利用，避免重复探索。作者构建了规模最大的带推理标注导航数据集，并结合专家引导的强化学习，显著提升导航性能。实验表明，VLingNav在多项导航任务中表现优异，且能实现零样本迁移至真实机器人，展现出强大的跨领域和跨任务泛化能力。,6
End-to-End Video Character Replacement without Structural Guidance,端到端视频角色替换方法：无需结构引导的MoCha框架,Other,Alibaba,https://arxiv.org/pdf/2601.08587,https://huggingface.co/papers/2601.08587,本文提出了MoCha，一种无需逐帧结构指导即可实现视频角色替换的端到端方法。该方法仅需单帧遮罩作为输入，突破了传统依赖骨骼或深度等结构信息的限制，显著提升了在复杂场景下的适用性和视觉一致性。通过引入条件感知的位置编码和强化学习后训练策略，MoCha有效增强了面部身份特征表现。此外，作者设计了三种专用训练数据集，解决了配对视频数据匮乏的问题。实验结果表明，MoCha在多个指标上优于现有技术，具备广泛的应用潜力。,6
PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary,PRL：过程奖励学习提升大语言模型的推理能力并拓宽推理边界,LLM,Other,https://arxiv.org/pdf/2601.10201,https://huggingface.co/papers/2601.10201,本文提出了Process Reward Learning（PRL）方法，通过将强化学习的目标分解为多个中间步骤，为大型语言模型的推理过程提供更细粒度的监督信号。与传统只依赖最终结果奖励的方式不同，PRL能够在推理过程中引导模型更有效地探索，从而提升其推理能力和适用范围。理论上，PRL等价于在最大化奖励的同时加入模型间的约束，保证优化过程的稳定性。实验结果表明，PRL显著提升了模型在复杂推理任务中的表现，且具有良好的泛化能力。,6
Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding,主动视频感知：用于智能体长视频理解的迭代证据寻求,Agent,Other,https://arxiv.org/pdf/2512.05774,https://huggingface.co/papers/2512.05774,本文提出了一种名为Active Video Perception（AVP）的新方法，用于提升长视频理解的效率和准确性。与传统依赖无差别生成字幕的方法不同，AVP通过一个循环的“计划-观察-反思”过程，主动选择与查询相关的视频片段进行分析，动态判断是否已获得足够信息回答问题。该方法显著减少了计算资源消耗，同时在多个长视频理解基准测试中实现了优异表现，准确率提升5.7%，推理时间和输入数据量分别降低至18.4%和12.4%。这一框架为长视频内容的智能分析提供了更高效且精准的解决方案。,5
OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation,OmniSafeBench-MM：用于多模态越狱攻击-防御评估的统一基准与工具箱,Multimodal LLM,"ByteDance, Alibaba",https://arxiv.org/pdf/2512.06589,https://huggingface.co/papers/2512.06589,本文提出了OmniSafeBench-MM，一个统一的多模态“越狱”攻击与防御评估工具箱。该平台整合了多种攻击方法和防御策略，涵盖丰富的风险领域和用户意图类型，建立了多维度的评估体系，能够细致衡量模型响应的危害程度、意图一致性及细节水平。通过对多款开源及闭源多模态大模型的广泛测试，揭示了其安全漏洞。OmniSafeBench-MM为多模态模型的安全研究提供了标准化、可复现的基础，推动相关领域的系统性进展。,5
Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization,[翻译]Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization,Other,Other,https://arxiv.org/pdf/2512.10955,https://huggingface.co/papers/2512.10955,无法生成摘要。,5
Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation,基于跟踪的结构提取：蒸馏结构保持运动用于视频生成,Diffusion Model,Other,https://arxiv.org/pdf/2512.11792,https://huggingface.co/papers/2512.11792,本文提出了一种新方法，通过从自回归视频跟踪模型中提取结构保持的运动先验，提升视频生成中运动的真实性和结构一致性。该方法引入双向特征融合模块和局部特征对齐损失，有效保持了物体的拓扑结构和局部运动关系。实验结果显示，所训练的视频生成模型在标准测试集和用户评价中均显著优于现有方法，生成的视频在保留结构和运动连贯性方面表现更佳，特别适用于生成复杂的关节和变形对象如人类和动物的动态视频。,5
Image Diffusion Preview with Consistency Solver,基于ConsistencySolver的图像扩散预览,Diffusion Model,DeepMind,https://arxiv.org/pdf/2512.13592,https://huggingface.co/papers/2512.13592,本文提出了一种名为Diffusion Preview的新方法，通过快速低步数采样生成图像预览，提升用户交互体验。核心创新是ConsistencySolver，一种基于多步数方法并通过强化学习优化的轻量级高阶求解器，显著提高低步数生成图像的质量和预览与最终结果的一致性。实验表明，该方法在减少近一半用户交互时间的同时，保持了生成图像的高质量，且比现有加速技术更高效。该方案适用于需要高效预览和细化的图像生成任务。,5
GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation,GenieDrive：基于4D占据引导的视频生成的物理感知驾驶世界模型,Embodied AI,Other,https://arxiv.org/pdf/2512.12751,https://huggingface.co/papers/2512.12751,本文提出了GenieDrive，一种基于4D占用空间的物理感知驾驶视频生成框架。通过先预测包含丰富三维结构和动态信息的4D占用状态，GenieDrive为后续视频生成提供物理基础。采用变分自编码器压缩占用信息，并引入互控注意力机制提升控制对占用演变的建模能力，实现了更高的预测准确率和更快的推理速度。此外，利用归一化多视角注意力生成多视角一致且高质量的驾驶视频。实验结果表明，GenieDrive在物理一致性、视频质量和多视角控制方面均显著优于现有方法。,5
"Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological ""Censorship""",美学对齐风险同化：图像生成与奖励模型如何强化美的偏见与意识形态“审查”,Diffusion Model,Other,https://arxiv.org/pdf/2512.11883,https://huggingface.co/papers/2512.11883,本文研究了当前图像生成及评价模型在美学偏好上的偏见，发现这些模型倾向于生成符合传统美学标准的作品，难以满足用户对“反美学”图像的需求，尤其在艺术和批判性表达中限制了用户自主性和美学多样性。通过构建多样化的美学数据集，作者验证了生成模型和奖励模型对非主流美学的系统性惩罚，揭示了潜在的“审美同化”与意识形态“审查”风险。此研究提醒设计者关注模型对用户意图的尊重，促进更包容的美学表达。,5
Spherical Leech Quantization for Visual Tokenization and Generation,用于视觉标记化与生成的球面Leech量化,Other,Other,https://arxiv.org/pdf/2512.14697,https://huggingface.co/papers/2512.14697,本文提出了一种基于Leech格的非参数量化方法——球面Leech量化（Λ24-SQ），通过统一的格编码视角整合了多种量化技术。该方法利用高对称性和均匀分布特性，简化了训练流程，并在图像标记和压缩任务中实现了更优的重构质量与压缩效率平衡，优于现有最佳方法BSQ。此外，Λ24-SQ在视觉自回归生成模型中也表现出显著提升，展现了其在大规模视觉表示学习中的广泛应用潜力。,5
WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory,WAY：基于全球AIS轨迹的船舶目的地估计,Other,Other,https://arxiv.org/pdf/2512.13190,https://huggingface.co/papers/2512.13190,本文提出了一种名为WAY的深度学习方法，用于基于全球自动识别系统（AIS）数据准确预测船舶的长周期目的地。该方法通过将船舶航迹重构为嵌套序列结构并结合空间网格，有效减少了数据的时空偏差，同时保持了细节信息。WAY架构包括多通道特征表示和专门设计的序列处理模块，提升了信息整合能力。此外，作者引入了一种梯度随机屏蔽技术，防止训练过程中的偏差积累。实验证明，该方法在长期目的地预测任务中优于传统模型，具备良好的实际应用潜力。,5
VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?,VTCBench：视觉-语言模型能否通过视觉-文本压缩理解长上下文？,Multimodal LLM,Tencent,https://arxiv.org/pdf/2512.15649,https://huggingface.co/papers/2512.15649,本文提出了首个针对视觉文本压缩（VTC）技术的基准测试VTCBench，系统评估视觉语言模型在理解压缩后长文本信息中的表现。研究涵盖信息检索、推理和长时记忆三大任务，此外还设计了多样化的测试场景。实验结果表明，尽管模型能较好地识别文本内容，但在捕捉长距离关联和上下文依赖方面表现不足。该工作揭示了当前视觉语言模型在处理高密度压缩信息时的局限，为未来提升模型的长文本理解能力和扩展性提供了重要参考。,5
Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets,Nano Banana Pro 是低级视觉全能选手吗？基于14项任务和40个数据集的综合评估,Diffusion Model,Other,https://arxiv.org/pdf/2512.15110,https://huggingface.co/papers/2512.15110,本文系统评估了商业文本生成图像模型Nano Banana Pro在14种低层视觉任务上的表现，涵盖40个数据集。研究发现，Nano Banana Pro在无需微调的情况下，通过简单文本提示即可实现高质量的视觉效果，甚至在主观感受上优于专门设计的模型。然而，由于生成模型的随机性，其在传统基于参考的量化指标上表现较弱，难以达到领域专家模型的像素级一致性。该工作揭示了Nano Banana Pro作为零样本低层视觉任务解决方案的潜力，同时指出其在精确度方面仍存在提升空间。,5
Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification,关键差异：用于能力缺口发现与修正的模型审计,Multimodal LLM,Other,https://arxiv.org/pdf/2512.16921,https://huggingface.co/papers/2512.16921,本文提出了AuditDM，一种基于强化学习的自动化框架，用于发现和修正多模态大语言模型（MLLM）中的能力缺陷。通过训练一个审计模型生成具有挑战性的问题和图像，AuditDM能够揭示不同模型间的显著差异和具体弱点，进而为模型改进提供无须额外标注的训练数据。实验证明，该方法在多个先进模型上发现了20多种失败类型，针对这些问题的微调显著提升了模型性能，甚至使小规模模型超越更大规模模型。该研究为模型诊断和优化提供了新的有效途径。,5
Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language,Insight Miner：用于跨领域对齐的时间序列分析数据集与自然语言,Multimodal LLM,Other,https://arxiv.org/pdf/2512.11251,https://huggingface.co/papers/2512.11251,本文提出了Insight Miner，一种面向多领域时间序列数据的多模态模型，能够生成高质量且包含专业知识的时间序列描述。为支持该模型，作者构建了首个通用领域的时间序列与自然语言对齐数据集TS-Insights，包含10万条时间序列窗口及其对应描述。通过一种新颖的工作流程，结合统计特征提取和GPT-4生成，Insight Miner在时间序列描述任务上优于现有先进模型。该研究为利用大规模多模态模型进行时间序列分析提供了新思路，推动了让语言模型直接理解时间序列数据的进展。,5
Meta-RL Induces Exploration in Language Agents,Meta-RL促使语言智能体进行探索,Agent,Other,https://arxiv.org/pdf/2512.16848,https://huggingface.co/papers/2512.16848,本文提出了LaMer，一种基于元强化学习的框架，旨在提升大型语言模型代理在强化学习任务中的探索能力和适应性。LaMer通过跨回合训练促进长期奖励优化，并利用反思机制实现无梯度的策略调整，使代理能在测试时主动探索并快速适应环境反馈。实验证明，LaMer在多个任务上显著优于传统强化学习方法，且在面对更复杂或未见过的任务时表现出更强的泛化能力。该研究展示了元强化学习在引导语言代理主动探索和增强环境适应性方面的有效性。,5
Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital,是否一致？迈向风险投资中的自主法律智能体,Agent,Other,https://arxiv.org/pdf/2512.18658,https://huggingface.co/papers/2512.18658,本论文聚焦风险投资中法律尽职调查中的核心任务——资本结构核对，旨在实现该流程的自动化。当前大型语言模型虽在法律文本分析上表现优异，但面对多文档推理和严格证据追踪的资本结构核对任务仍力不从心。作者将该任务定义为法律人工智能的现实基准，评估现有智能系统表现，并提出一种基于“世界模型”的架构，推动核对自动化发展，进而为应用型法律智能奠定基础，提升尽职调查的效率与准确性。,5
FaithLens: Detecting and Explaining Faithfulness Hallucination,FaithLens：检测与解释可信性幻觉,LLM,"THU, PKU",https://arxiv.org/pdf/2512.20182,https://huggingface.co/papers/2512.20182,本文提出了FaithLens，一种高效且经济的模型，用于检测大型语言模型生成内容中的不真实信息（即“忠实性幻觉”）。FaithLens通过利用先进语言模型合成带解释的训练数据，并结合规则强化学习优化，不仅能准确判断内容是否真实，还能给出清晰的解释。实验表明，FaithLens在12个不同任务上优于现有先进模型如GPT-4.1，兼具较高的可信度和效率，提升了语言模型在实际应用中的可靠性。,5
Scaling Laws for Code: Every Programming Language Matters,代码的规模定律：每种编程语言都很重要,LLM,Other,https://arxiv.org/pdf/2512.13472,https://huggingface.co/papers/2512.13472,本论文首次系统研究了多编程语言代码大模型的规模效应，发现不同编程语言在预训练中对模型性能影响显著，解释型语言（如Python）比编译型语言（如Rust）更能从增加模型规模和数据中获益。研究还揭示了多语言联合训练的协同效应，尤其是语法相似语言间的互助作用，并提出了将代码及其翻译并行配对的预训练策略以提升跨语言能力。最后，论文提出了一种基于语言特性和协同关系的训练数据分配方法，在相同计算预算下显著提升了多语言模型的整体表现。,5
SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios,SWE-EVO：长时程软件演进场景下编码智能体的基准测试,Agent,Other,https://arxiv.org/pdf/2512.18470,https://huggingface.co/papers/2512.18470,本文提出了SWE-EVO，一个针对长周期、多步骤软件演进任务的评测基准，旨在反映现实软件开发中跨多个文件和多次迭代的复杂修改需求。基准基于七个成熟开源Python项目的版本历史，包含48个需跨平均21个文件进行修改的任务，并通过严格的测试套件验证。实验结果显示，尽管先进模型如GPT-5表现优异，但在该基准上的解决率仅为21%，远低于单一问题任务的65%，揭示了当前AI编码代理在持续、多文件推理方面的显著不足。论文还提出了细化进展的Fix Rate指标，推动对复杂软件演进能力的更精确评估。,5
SVBench: Evaluation of Video Generation Models on Social Reasoning,SVBench：基于社会推理的视频生成模型评估,Agent,"THU, Shanghai AI Lab, PKU",https://arxiv.org/pdf/2512.21507,https://huggingface.co/papers/2512.21507,本文提出了SVBench，这是首个专门评估视频生成模型社会推理能力的基准。基于心理学经典范式，SVBench涵盖七大社会认知维度，如意图推断、联合注意和社会规范等。作者设计了无需训练的代理系统，自动生成多样化测试场景并通过视觉语言模型进行评价。大规模实验显示，尽管当前视频生成技术在视觉效果和文本匹配上表现优异，但在理解和生成符合社会逻辑的行为方面存在显著不足。该工作为提升视频生成模型的社会智能提供了系统化评测工具和研究方向。,5
SWE-RM: Execution-free Feedback For Software Engineering Agents,SWE-RM：面向软件工程智能体的无执行反馈机制,Agent,Alibaba,https://arxiv.org/pdf/2512.21919,https://huggingface.co/papers/2512.21919,本文提出了SWE-RM，一种基于执行无关反馈的奖励模型，旨在提升软件工程智能体的训练效果。传统依赖单元测试的反馈方式存在反馈稀疏且难以区分不同执行轨迹的问题，而SWE-RM通过更细粒度的信号改善了这一点。论文系统分析了训练数据规模、策略混合及数据来源等因素对模型表现的影响，设计出具备良好分类准确率和校准能力的混合专家架构模型。实验结果显示，SWE-RM显著提升了软件工程智能体在测试时扩展和强化学习中的性能，刷新了开源模型的性能纪录。,5
Evaluating Parameter Efficient Methods for RLVR,RLVR中参数高效方法的评估,LLM,Other,https://arxiv.org/pdf/2512.23165,https://huggingface.co/papers/2512.23165,本文系统评估了在可验证奖励强化学习（RLVR）框架下的参数高效微调方法。通过对12种以上方法在数学推理任务中的比较，研究发现一些结构变体（如DoRA、AdaLoRA、MiSS）显著优于常用的LoRA方法；而基于奇异值分解初始化的策略存在性能崩溃，且极端的参数压缩会严重限制模型的推理能力。论文通过消融和扩展实验验证了这些结论，为优化RLVR中的参数高效训练提供了重要指导，推动了该领域的深入探索。,5
SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time,SpaceTimePilot：跨越时空的动态场景生成渲染,Diffusion Model,Other,https://arxiv.org/pdf/2512.25075,https://huggingface.co/papers/2512.25075,本文提出了SpaceTimePilot，一种视频生成模型，能够独立控制视频中的摄像机视角和时间动态，实现对动态场景的空间与时间的解耦与自由探索。通过引入时间嵌入机制和创新的时间扭曲训练策略，模型在没有现成配对数据的情况下学习时间控制。此外，作者还设计了改进的摄像机条件机制和首个覆盖完整空间时间轨迹的合成数据集Cam×Time，提升了控制的精度。实验结果表明，该方法在真实和合成数据上均能高效生成连贯且可控的视频，推动了动态场景生成技术的发展。,5
JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation,[翻译]JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation,Other,Other,https://arxiv.org/pdf/2512.22905,https://huggingface.co/papers/2512.22905,无法生成摘要。,5
Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers,面向呼吸音分类的几何感知优化：基于SAM优化的音频频谱变换器提升灵敏度,Other,Other,https://arxiv.org/pdf/2512.22564,https://huggingface.co/papers/2512.22564,本论文针对呼吸音分类中数据量有限、噪声大及类别不平衡的问题，提出了一种基于几何感知优化的音频频谱变换器方法。通过引入Sharpness-Aware Minimization（SAM）技术，模型在训练时不仅降低损失，还优化了损失曲面的形状，使其更具泛化能力，有效提升了对未见患者的识别效果。同时，采用加权采样策略缓解类别不平衡。该方法在ICBHI 2017数据集上取得了68.10%的最佳分类性能和68.31%的敏感度，显著优于现有模型，增强了临床筛查的可靠性。,5
Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems,锻造空间智能：面向自主系统的多模态数据预训练路线图,Multimodal LLM,Alibaba,https://arxiv.org/pdf/2512.24385,https://huggingface.co/papers/2512.24385,本文针对自动驾驶车辆和无人机等自主系统，提出了一个多模态数据预训练的综合框架，旨在通过融合摄像头、激光雷达等多种传感器数据，实现对环境的统一感知和空间智能。论文系统梳理了不同传感器特性与学习方法的关系，构建了涵盖单模态到统一多模态的预训练范式分类，支持三维目标检测和语义占用预测等复杂任务。同时探讨了文本信息与占用表示的结合，以促进开放环境下的感知与规划。最后，指出了计算效率和模型扩展性的关键挑战，并规划了通用多模态基础模型的发展路线，为实际部署奠定基础。,5
Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking,Figure It Out：通过主动视觉思维提升推理前沿,Multimodal LLM,Tencent,https://arxiv.org/pdf/2512.24297,https://huggingface.co/papers/2512.24297,本论文提出了FIGR，一种通过端到端强化学习将主动视觉思维融入多轮推理的方法。FIGR在解决复杂几何和空间关系问题时，通过动态生成图形并利用视觉表示辅助推理，有效捕捉文本难以表达的全局结构信息。实验表明，FIGR在多个数学推理基准测试中显著优于纯文本推理方法，提升了模型的稳定性和准确性，展示了图形引导的多模态推理在复杂问题解决中的潜力和优势。,5
KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs,KV-Embedding：通过解码器单向大语言模型内部KV重路由实现无训练文本嵌入,LLM,Other,https://arxiv.org/pdf/2601.01046,https://huggingface.co/papers/2601.01046,本文提出了KV-Embedding，一种无需训练即可从冻结的大型语言模型中提取文本表示的方法。该方法通过重新利用模型内部每层最后一个位置的关键-值（KV）状态，将其作为前缀输入，从而使所有词元能够在单次前向传播中访问全序列上下文，克服了传统解码器模型因因果注意力机制导致的上下文访问受限问题。通过自动选择合适层数，KV-Embedding在多个模型和任务上表现优于现有无训练基线，且支持长文本输入，展示了内部状态操作在表示学习中的潜力和高效性。,5
CPPO: Contrastive Perception for Vision Language Policy Optimization,CPPO：用于视觉语言策略优化的对比感知方法,Multimodal LLM,Other,https://arxiv.org/pdf/2601.00501,https://huggingface.co/papers/2601.00501,本文提出了CPPO，一种用于视觉语言模型微调的新方法。CPPO通过检测模型输出中感知相关的关键部分，利用对比感知损失提升模型在多模态推理中的表现。与以往依赖额外模型或人工标注来区分感知和推理的做法不同，CPPO通过分析输入图像扰动引起的输出熵变化，自动识别感知信息，并针对性地优化。这种方法不仅提升了训练效率和效果，还增强了模型对图像信息的敏感性和稳定性，推动了视觉语言模型在复杂任务中的应用。,5
EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning,EpiQAL：用于增强对齐与推理的流行病学问答大语言模型基准测试,LLM,Microsoft,https://arxiv.org/pdf/2601.03471,https://huggingface.co/papers/2601.03471,本文提出了EpiQAL，一个针对流行病学领域的大型语言模型问答能力的全新基准测试。EpiQAL包含三个子集，分别评估模型对事实回顾、多步骤推理和结论重构的能力，数据均来源公开文献。实验发现当前模型在流行病学推理上表现有限，尤其是多步骤推理最具挑战性，且模型规模并非性能决定因素。该基准为提升模型在基于证据的流行病学推理和结论生成方面提供了细致的诊断工具，推动相关技术的发展。,5
Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing,Re-Align：面向上下文图像生成与编辑的结构化推理引导对齐,Multimodal LLM,Tencent,https://arxiv.org/pdf/2601.05124,https://huggingface.co/papers/2601.05124,本文提出了Re-Align，一种针对图像生成与编辑任务的统一框架，旨在解决当前模型理解能力难以有效转化为图像生成的问题。核心创新在于引入结构化推理引导的对齐方法，通过“上下文链式思维”明确区分语义指导与参考关联，提升对用户意图的准确理解。同时，结合强化学习训练，通过设计代理奖励函数优化生成结果与推理文本的一致性。大量实验表明，Re-Align在多种图像生成与编辑任务中表现优于同规模现有方法，显著增强了模型的实用性和生成质量。,5
One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling,一样本统治一切：强化学习扩展中的极端数据效率,LLM,Alibaba,https://arxiv.org/pdf/2601.03111,https://huggingface.co/papers/2601.03111,本文提出了一种名为“多学科学习”的新框架，通过精心设计的单一训练样本，显著提升大语言模型在多个领域（如物理、化学、生物）的推理能力。研究表明，单个高质量的数学推理样本不仅能跨学科带来性能提升，且经过融合多学科元素的合成样本效果更佳，超越了使用大量自然样本的训练结果。该工作挑战了强化学习中对大规模数据的依赖，强调样本设计的质量和精准性对于激发模型推理潜力的重要性，推动了训练样本工程化的理念。,5
SmartSearch: Process Reward-Guided Query Refinement for Search Agents,SmartSearch：基于过程奖励引导的搜索智能体查询优化,Agent,Other,https://arxiv.org/pdf/2601.04888,https://huggingface.co/papers/2601.04888,本文提出了SmartSearch，一种针对基于大语言模型的搜索代理的改进框架。该框架通过引入过程奖励机制对中间搜索查询质量进行细粒度监督，并结合查询优化策略，有效提升了查询的准确性和检索效果。通过设计三阶段的渐进式学习流程，搜索代理能够逐步掌握优化查询的能力。实验结果表明，SmartSearch在搜索效率和查询质量上均显著优于现有方法，展现出较强的实用价值和推广潜力。,5
"JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",JudgeRLVR：先判定，后生成的高效推理方法,LLM,PKU,https://arxiv.org/pdf/2601.08468,https://huggingface.co/papers/2601.08468,本文提出了JudgeRLVR，一种先判断后生成的两阶段强化学习方法，用于提升大语言模型在数学推理中的效率和准确性。通过首先训练模型辨别有效解答，再基于该判断能力进行生成优化，JudgeRLVR有效减少了冗长的试错过程，提升了推理质量。实验证明，该方法在同领域和跨领域数学任务中均显著提高了准确率，同时大幅缩短了生成长度，展现出更优的性能与泛化能力。,5
EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs,EpiCaR：知道你不知道的事对提升大语言模型推理能力的重要性,LLM,Other,https://arxiv.org/pdf/2601.06786,https://huggingface.co/papers/2601.06786,本文提出了一种名为EpiCaR的训练方法，旨在提升大型语言模型的推理能力和自我认知能力。传统方法虽能提高准确率，但常导致模型过度自信，难以正确表达不确定性。EpiCaR通过联合优化推理性能与校准效果，使模型不仅学会推理，更能判断何时应信赖自身推理结果。实验证明，该方法在多个模型和任务上均优于现有技术，显著提升了准确性和可靠性，同时减少了推理计算量，展现出较强的泛化能力和实用价值。,5
Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering,通过稀疏扩散与三维渲染实现静态场景的高效摄像机控制视频生成,Diffusion Model,Other,https://arxiv.org/pdf/2601.09697,https://huggingface.co/papers/2601.09697,本文提出了一种高效的静态场景视频生成方法，通过扩散模型生成稀疏关键帧，结合三维重建与渲染技术合成完整视频。该方法通过将关键帧提升为三维表示并渲染中间视角，实现生成成本在多帧间分摊，保证几何一致性。系统还能根据摄像机轨迹自适应调整关键帧数量，简化计算。实验表明，所提方法在生成20秒视频时速度提升超过40倍，同时保持了高质量和时间稳定性，为实时交互场景下的视频生成提供了实用解决方案。,5
LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning,LaViT：对齐潜在视觉思维以实现多模态推理,Multimodal LLM,Other,https://arxiv.org/pdf/2601.10129,https://huggingface.co/papers/2601.10129,本文提出了LaViT，一种通过对视觉语义和注意力路径的自回归重构，促进多模态模型中视觉思维对齐的新方法。该方法解决了现有模型在知识蒸馏过程中存在的“感知差距”问题，即学生模型虽能复制教师文本输出，却关注不同的视觉区域，依赖语言先验而非真实视觉信息。LaViT通过引入逐步感知门控机制，有效提升了视觉定位能力和推理性能，在复杂任务中实现了显著提升，且使体积较小的模型表现优于更大规模的开源及商业模型，推动了多模态推理技术的发展。,5
RigMo: Unifying Rig and Motion Learning for Generative Animation,RigMo：统一绑定与运动学习的生成动画框架,Other,Other,https://arxiv.org/pdf/2601.06378,https://huggingface.co/papers/2601.06378,本文提出了RigMo，一种统一的生成框架，能够直接从原始网格序列中同时学习骨骼结构（rig）和运动，无需人工标注。RigMo通过两个紧凑的潜在空间分别编码变形的结构信息和动态变化，实现了具有明确骨骼和连贯运动的3D动画生成。该方法支持端到端推断，提升了动画的可解释性和物理合理性。实验证明，RigMo在重建精度和跨类别泛化能力上优于现有自动绑定和变形方法，开创了结构感知且高效的动态三维建模新范式。,5
From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model,从片段到场景：通过视觉-语言模型实现自动驾驶中的时间理解,Multimodal LLM,Other,https://arxiv.org/pdf/2512.05277,https://huggingface.co/papers/2512.05277,本文提出了针对自动驾驶视频中时间理解难题的专属基准测试——TAD，包含近6000个问答对和7类任务，专门评估视觉-语言模型对动态场景的理解能力。通过对多种现有模型的测试，发现其表现普遍不理想，主要因细粒度动作理解不足。为提升性能，作者设计了两种无需训练的新方法：基于思维链的Scene-CoT和融合自我视角时序认知地图的TCogMap，显著提高了模型准确率。该工作填补了自动驾驶时间理解评测的空白，推动该领域未来研究发展。,4
TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models,TreeGRPO：基于树状优势的GRPO用于扩散模型的在线强化学习后训练,Diffusion Model,Other,https://arxiv.org/pdf/2512.08153,https://huggingface.co/papers/2512.08153,本文提出了TreeGRPO，一种基于树结构的强化学习框架，显著提升了扩散模型后训练的效率。通过将去噪过程视为搜索树，TreeGRPO能够共享和重用噪声样本的公共部分，生成多个候选路径，实现更高的样本利用率和更细粒度的奖励分配。此外，该方法支持多子节点并行更新，减少计算开销。实验证明，TreeGRPO在训练速度上比现有方法快约2.4倍，同时在性能与效率的权衡上表现更优，为视觉生成模型的强化学习对齐提供了高效且可扩展的解决方案。,4
From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs,从下一个Token到下一个Block：扩散大语言模型的原则性适应路径,LLM,PKU,https://arxiv.org/pdf/2512.06776,https://huggingface.co/papers/2512.06776,本文提出了一种将传统自回归语言模型（AR）逐步适配为块状扩散语言模型（Block-Diffusion）的系统方法。该方法通过引入上下文因果注意力机制、辅助自回归损失及逐步增大生成块大小，实现了并行生成与保留预训练知识的平衡。基于此，作者构建的NBDiff-7B模型在多个任务上优于同类7B参数级别扩散模型，显示出高效利用计算资源且无需从零训练扩散模型的潜力，为提升大规模语言模型的生成速度和性能提供了新的思路。,4
Learning Unmasking Policies for Diffusion Language Models,扩散语言模型的解掩码策略学习,Diffusion Model,Other,https://arxiv.org/pdf/2512.09106,https://huggingface.co/papers/2512.09106,本文提出了一种基于强化学习的方法，用于优化掩码离散扩散语言模型中选择替换令牌的采样策略。相比传统的启发式方法，该方法通过将采样过程建模为马尔可夫决策过程，并设计轻量级策略网络，实现了在保持生成质量的同时提升生成速度。实验表明，所学策略在全扩散生成中优于现有启发式策略，且具备一定的跨模型和长序列泛化能力，但在领域外数据上的表现有所下降。该研究为提升扩散语言模型的推理效率和质量提供了新的思路。,4
Towards a Science of Scaling Agent Systems,迈向智能体系统规模化科学,Agent,DeepMind,https://arxiv.org/pdf/2512.08296,https://huggingface.co/papers/2512.08296,本文提出了一个量化框架，系统研究多智能体系统的规模扩展规律，揭示了智能体数量、协调结构、模型能力与任务特性之间的关系。通过在多个不同任务和架构上的大规模实验，作者发现了工具协调权衡、能力饱和以及拓扑结构对错误放大的影响等关键现象。该框架能基于任务属性预测最优协调策略，有效指导多智能体系统设计，提升了对智能体规模扩展原理的理解和应用，为构建高效智能体系统提供了科学依据。,4
MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification,MoRel：基于锚点中继的双向融合与分层致密化的无闪烁长距离4D运动建模,Other,Other,https://arxiv.org/pdf/2512.09270,https://huggingface.co/papers/2512.09270,本文提出了MoRel，一种创新的4D高斯点云动态场景渲染方法，针对长距离动态视频中存在的内存爆炸和时间闪烁问题，采用关键帧锚点的双向融合机制，实现了时间上一致且内存高效的运动建模。同时，引入基于特征变化的分层加密策略，提升渲染质量。作者还构建了包含大范围动态运动的新数据集SelfCap_LR，验证了方法在长距离动态场景下的稳定性与扩展性。MoRel在保证渲染质量的同时，有效控制了内存使用，推动了动态场景的实时高质量重建。,4
The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality,FACTS排行榜：大型语言模型事实准确性的综合基准测试,LLM,Other,https://arxiv.org/pdf/2512.10791,https://huggingface.co/papers/2512.10791,本文提出了The FACTS Leaderboard，一个综合性的在线评测平台，系统地衡量大型语言模型在多种场景下生成事实准确文本的能力。该平台包含四个子评测：基于图像的问题回答、闭卷事实问答、利用搜索接口的信息检索以及基于文档的长文本生成。通过自动评判模型回答，综合四个子榜单的表现，提供对模型整体事实准确性的全面评价。该基准套件公开维护，支持外部参与，促进语言模型事实性研究的标准化和进步。,4
X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale,[翻译]X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale,Other,Other,https://arxiv.org/pdf/2512.04537,https://huggingface.co/papers/2512.04537,无法生成摘要。,4
LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator,LEO-RobotAgent：一种通用语言驱动的具身机器人智能体,Agent,Other,https://arxiv.org/pdf/2512.10605,https://huggingface.co/papers/2512.10605,本文提出了LEO-RobotAgent，一种基于大语言模型的通用机器人智能代理框架。该框架结构简洁，支持多种类型机器人（如无人机、机械臂和轮式机器人）在不同场景下自主完成复杂任务，具备良好的泛化能力和鲁棒性。通过模块化工具集和人机交互机制，LEO-RobotAgent提升了人机意图理解和协作效率，降低了人机交互门槛。实验证明该框架在多平台任务执行中表现出色，展示了其在机器人任务规划和智能操作领域的广泛应用潜力。,4
"Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}","[翻译]Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}",Other,Other,https://arxiv.org/pdf/2512.02901,https://huggingface.co/papers/2512.02901,无法生成摘要。,4
EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models,EVOLVE-VLA：基于环境反馈的视觉-语言-动作模型测试时训练框架,Embodied AI,Other,https://arxiv.org/pdf/2512.14666,https://huggingface.co/papers/2512.14666,本文提出了EVOLVE-VLA，一种基于环境反馈的测试时训练框架，旨在提升视觉-语言-动作模型的适应能力。该方法通过自主估计任务进展，替代传统训练中依赖的人工奖励信号，实现了在极少甚至无任务示范的情况下，模型通过与环境交互持续自我优化。实验结果显示，EVOLVE-VLA在长任务、单次学习及跨任务泛化上均显著优于传统监督微调方法，且具备错误恢复和策略创新等新能力。该工作推动了机器人智能从静态模仿向动态自适应的转变。,4
Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in,Zoom-Zero：通过时间缩放的强化粗到细视频理解框架,Multimodal LLM,Other,https://arxiv.org/pdf/2512.14273,https://huggingface.co/papers/2512.14273,本文提出了Zoom-Zero，一种用于视频问答任务的粗到细时间定位框架。该方法通过先粗略定位相关视频片段，再聚焦关键帧进行细粒度验证，有效提升了时间定位的准确性和答案质量。核心创新包括引入放大准确性奖励以验证定位结果，以及基于关键token的奖励分配策略，解决了现有方法在多重奖励信号处理上的不足。实验表明，Zoom-Zero在多个数据集上显著提升了时间定位和问答性能，尤其在长视频理解中表现出更好的细节保留和全局上下文兼顾能力。,4
Janus: Disaggregating Attention and Experts for Scalable MoE Inference,Janus：为可扩展MoE推理解耦注意力与专家模块,LLM,Other,https://arxiv.org/pdf/2512.13525,https://huggingface.co/papers/2512.13525,本文提出了Janus，一种针对大规模混合专家模型（MoE）推理的高效系统。Janus通过将注意力模块和专家模块分离部署在不同GPU子集群，实现了模块的独立管理和弹性扩展。系统设计包括自适应的双阶段通信方案、轻量级调度器以均衡专家负载，以及精细的资源管理策略，从而显著提升了推理吞吐量和响应速度。实验结果表明，Janus在满足延迟要求的同时，单GPU吞吐量最高提升3.9倍，显著优于现有方法。,4
"Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",Efficient-DLM：从自回归到扩散语言模型及其速度提升,Diffusion Model,Other,https://arxiv.org/pdf/2512.14067,https://huggingface.co/papers/2512.14067,本文提出了一种将预训练的自回归语言模型高效转换为扩散语言模型的方法，显著提升了生成速度同时保持任务准确性。作者通过分析现有方法在注意力机制和掩码策略上的不足，设计了块状注意力模式和位置依赖的掩码策略，有效保留了原模型权重分布并缩小训练与测试的差距。基于此，构建的Efficient-DLM系列模型在准确率和吞吐量上均优于当前主流模型，实现了更高效且准确的语言生成，推动了扩散语言模型的实际应用。,4
TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration,TAT：面向多任务的自适应Transformer用于一体化医学图像修复,Other,ByteDance,https://arxiv.org/pdf/2512.14550,https://huggingface.co/papers/2512.14550,本文提出了一种任务自适应Transformer（TAT）框架，旨在解决医疗图像恢复中多任务共享模型面临的任务干扰和任务不平衡问题。通过引入任务特定权重生成和动态损失权重调整两大策略，TAT有效缓解了不同任务间的梯度冲突和优化难度差异。实验结果表明，该方法在PET合成、CT去噪和MRI超分辨率等多项任务中均取得了领先性能，展示了其在多模态医疗图像恢复中的广泛适用性和优越效果。,4
CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives,CRISP：基于单目视频的接触引导Real2Sim与平面场景基元,Embodied AI,Other,https://arxiv.org/pdf/2512.14696,https://huggingface.co/papers/2512.14696,本文提出了CRISP，一种从单目视频中恢复可用于仿真的人体动作和场景几何结构的方法。通过将场景点云拟合为平面简化模型，并结合人体与场景接触信息，CRISP有效重建了干净且符合物理规律的环境几何。该方法显著降低了动作追踪失败率（从55.2%降至6.9%），并提升了强化学习仿真效率（提高43%）。实验验证了其在多种真实视频中的鲁棒性，推动了机器人和增强现实领域中从视频到仿真的应用发展。,4
SCOPE: Prompt Evolution for Enhancing Agent Effectiveness,SCOPE：通过提示演化提升智能体效能,Agent,Other,https://arxiv.org/pdf/2512.15374,https://huggingface.co/papers/2512.15374,本文提出了SCOPE，一种通过自动演化提示语来提升大型语言模型代理（LLM agents）在复杂动态环境中管理上下文能力的方法。SCOPE将上下文管理视为在线优化问题，结合执行轨迹自动调整提示，采用双流机制平衡即时错误修正和长期策略演进，并引入视角驱动探索以扩展策略覆盖范围。实验证明，SCOPE在无需人工干预的情况下，显著提升了任务成功率，展示了其在提升智能代理适应性和效率方面的潜力。,4
Understanding and Improving Hyperbolic Deep Reinforcement Learning,理解与改进超曲率深度强化学习,Agent,Other,https://arxiv.org/pdf/2512.14202,https://huggingface.co/papers/2512.14202,本文针对强化学习中利用双曲几何特征空间面临的优化难题，分析了核心操作梯度导致训练不稳定的原因。基于此，提出了新的双曲强化学习算法Hyper++，通过稳定的价值函数训练、特征正则化控制嵌入范数及优化友好的网络层设计，有效提升了训练稳定性和性能。在多个强化学习基准测试中，Hyper++表现优于现有双曲及欧几里得方法，且显著缩短了训练时间，展示了其在复杂环境中捕捉层级结构和关系信息的优势。,4
FrontierCS: Evolving Challenges for Evolving Intelligence,FrontierCS：面向不断进化智能的前沿挑战,Other,Other,https://arxiv.org/pdf/2512.15699,https://huggingface.co/papers/2512.15699,本文提出了FrontierCS，一个包含156个开放式计算机科学问题的评测基准，涵盖算法和研究领域，问题的最优解未知但可客观评价。与传统基准不同，FrontierCS要求模型通过编写可执行程序来解决问题，配备专家参考解和自动评测工具。实验证明，当前模型在解决这些复杂问题上仍远落后于人类专家，且单纯增加推理资源难以缩小差距，模型倾向于生成可行代码而非高质量算法设计。FrontierCS为推动智能体在计算机科学前沿挑战中的能力提升提供了重要平台。,4
Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers,可训练的对数线性稀疏注意力用于高效扩散Transformer,Diffusion Model,PKU,https://arxiv.org/pdf/2512.16615,https://huggingface.co/papers/2512.16615,本文提出了一种名为Log-linear Sparse Attention（LLSA）的新型注意力机制，旨在提升扩散变换器（DiTs）处理超长序列的效率。通过引入层级化的Top-K选择策略和关键值增强方法，LLSA将传统的二次计算复杂度降低为对数线性，显著减少计算开销。该方法支持高效GPU训练，实现了在高分辨率图像生成任务中训练速度提升6倍，推理速度提升近28倍，同时保持生成质量。LLSA为扩散模型在长序列上的高效训练提供了有效解决方案。,4
Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision,Nemotron-Math：基于多模态监督的高效长上下文数学推理蒸馏,LLM,Other,https://arxiv.org/pdf/2512.15489,https://huggingface.co/papers/2512.15489,本文提出了Nemotron-Math，一个包含750万数学解题过程的大规模数据集，融合了竞赛题和社区问题，覆盖多种推理难度和风格，并支持Python工具辅助推理。该数据集通过多模式生成提升了推理多样性和质量，显著优于现有数据集。论文还提出了一种高效的长上下文训练策略，实现了128K长度训练速度提升2-3倍且准确率无明显下降。Nemotron-Math为数学推理模型提供了多样且高质量的监督，推动了模型在复杂数学问题上的性能达到最新水平。,4
3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework,3D-RE-GEN：基于生成框架的室内场景三维重建,Other,Other,https://arxiv.org/pdf/2512.17459,https://huggingface.co/papers/2512.17459,本文提出了3D-RE-GEN，一种从单张室内图片重建可编辑3D场景的生成框架。该方法通过组合多种先进模型，实现了对场景中物体的准确分解、重建和合理布局，同时生成完整背景以增强空间约束和光照效果。创新性地将遮挡物体的重建视为图像编辑任务，并引入4自由度优化确保物体与地面合理对齐。3D-RE-GEN显著提升了单图3D场景重建的质量和实用性，满足视觉特效和游戏开发中对可修改3D模型的需求。,4
Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface,Real2Edit2Real：通过3D控制界面生成机器人示范,Embodied AI,PKU,https://arxiv.org/pdf/2512.19402,https://huggingface.co/papers/2512.19402,本文提出了Real2Edit2Real框架，通过结合三维场景重建与编辑，以及多条件视频合成，有效生成多样化的机器人操作示范。该方法利用多视角RGB图像重建场景几何，基于点云进行三维编辑并调整机器人姿态，确保物理深度一致性，进而合成新的多视角操作视频。实验证明，仅用少量真实示范生成的数据即可训练出性能媲美甚至优于大量真实数据的策略，数据效率提升10-50倍。该框架具备高度灵活性和扩展性，有望成为统一的机器人示范数据生成工具。,4
Multi-hop Reasoning via Early Knowledge Alignment,通过早期知识对齐实现多跳推理,LLM,Other,https://arxiv.org/pdf/2512.20144,https://huggingface.co/papers/2512.20144,本文提出了一种名为早期知识对齐（EKA）的新方法，用于提升大型语言模型在多跳推理任务中的检索和推理效率。EKA通过在规划推理步骤前，将模型与相关检索知识进行对齐，增强了模型的知识基础，显著提高了检索准确率，减少了错误传播，提升了整体性能和计算效率。实验结果表明，EKA作为一种无需额外训练的推理策略，具有良好的泛化能力和适应性，推动了迭代检索增强生成系统的发展，促进了结构化推理与高效探索的结合。,4
Streaming Video Instruction Tuning,Streaming Video Instruction Tuning（流式视频指令微调）,Multimodal LLM,Tencent,https://arxiv.org/pdf/2512.21334,https://huggingface.co/papers/2512.21334,本文提出了Streamo，一种面向实时视频流的多功能大语言模型助手，能够完成实时解说、动作理解、事件描述、时间定位和时效性问答等多种任务。为支持这一能力，作者构建了包含46.5万条指令的大规模训练数据集Streamo-Instruct-465K，覆盖丰富的时间上下文和多任务监督，实现统一训练。实验表明，Streamo在连续视频流中具备强大的时间推理和交互响应能力，填补了离线视频模型与实时多模态助手之间的空白，推动了智能视频理解向实时应用的进步。,4
ProGuard: Towards Proactive Multimodal Safeguard,ProGuard：面向主动多模态安全防护,Multimodal LLM,Shanghai AI Lab,https://arxiv.org/pdf/2512.23573,https://huggingface.co/papers/2512.23573,本文提出了ProGuard，一种面向多模态安全的主动防护系统，能够识别并描述超出训练分布的安全风险，而无需调整模型结构。研究团队构建了一个包含8.7万条文本、图像及其组合样本的平衡数据集，并基于此通过强化学习训练了视觉语言模型，实现高效且简洁的风险推理。ProGuard在二元安全分类上表现接近闭源大模型，在风险类别识别上显著优于现有开源模型，尤其在主动检测和描述新型安全风险方面提升显著，展示了其在多模态内容安全防护中的重要应用价值。,4
Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation,Robo-Dopamine：用于高精度机器人操作的通用过程奖励建模,Embodied AI,PKU,https://arxiv.org/pdf/2512.23703,https://huggingface.co/papers/2512.23703,本文针对机器人强化学习中设计有效奖励函数的难题，提出了Robo-Dopamine方法。该方法通过构建一个基于多视角输入、具备步骤感知能力的通用奖励模型（GRM），实现对细粒度操作进展的准确评估；并引入一种理论上合理的奖励塑形策略，避免了传统方法中的语义陷阱，提升了策略优化的稳定性和效率。大量仿真与真实机器人实验表明，GRM在奖励评估上达到领先水平，结合Dopamine-RL框架后显著加速了策略学习，且具备良好的任务适应性和泛化能力。,4
Factorized Learning for Temporally Grounded Video-Language Models,[翻译]Factorized Learning for Temporally Grounded Video-Language Models,Other,Other,https://arxiv.org/pdf/2512.24097,https://huggingface.co/papers/2512.24097,无法生成摘要。,4
BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts,[翻译]BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts,Other,Other,https://arxiv.org/pdf/2512.24885,https://huggingface.co/papers/2512.24885,无法生成摘要。,4
FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation,FlowBlending：面向快速高保真视频生成的阶段感知多模型采样策略,Diffusion Model,Other,https://arxiv.org/pdf/2512.24724,https://huggingface.co/papers/2512.24724,本文提出了FlowBlending，一种针对视频生成中不同阶段模型容量需求差异的多模型采样策略。该方法在关键的早期和后期阶段使用大模型以保证生成质量，而在中间阶段采用小模型以减少计算量。通过简单的阶段划分标准和速度差异分析，FlowBlending实现了在保持视觉效果和时间连贯性的同时，推理速度提升1.65倍，计算量减少57%以上。此外，该方法可与现有加速技术结合，进一步提升效率，显著推动了高质量视频生成的快速实现。,4
Diversity or Precision? A Deep Dive into Next Token Prediction,多样性还是精确性？对下一令牌预测的深入探讨,LLM,Tencent,https://arxiv.org/pdf/2512.22955,https://huggingface.co/papers/2512.22955,本文深入探讨了大语言模型中下一词预测的训练策略，揭示了预训练模型输出分布对后续强化学习效果的关键影响。作者将传统的交叉熵损失视为一种单步策略梯度优化，提出了一种融合强化学习思想的广义预训练目标，通过奖励设计在多样性与精准性之间实现平衡。研究发现，相较于追求更高的分布多样性，采用更加精准的先验分布能为强化学习提供更优的探索空间，从而显著提升模型的推理能力。该工作为优化语言模型训练提供了新的视角和方法。,4
COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs,COMPASS：用于评估大语言模型组织特定政策一致性的框架,LLM,Other,https://arxiv.org/pdf/2601.01836,https://huggingface.co/papers/2601.01836,本文提出了COMPASS框架，首次系统评估大型语言模型在企业环境中对特定组织政策的遵守情况。通过设计近6000条测试查询，涵盖日常合规和对抗性边缘案例，作者在八个行业场景中检验了七款主流模型。结果显示，模型能较好处理合法请求（准确率超过95%），但在拒绝违规请求方面表现较差，仅能阻止13%至40%的违规行为。研究揭示当前模型在关键政策执行上的不足，强调COMPASS作为保障组织AI安全的重要评估工具的价值。,4
Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion,通过协同引导与协同融合实现稳定的半监督遥感图像分割,Other,Other,https://arxiv.org/pdf/2512.23035,https://huggingface.co/papers/2512.23035,本文提出了一种稳定的半监督遥感图像分割框架Co2S，通过融合视觉语言模型和自监督模型的先验信息，有效缓解了伪标签偏移问题。该方法采用双学生结构，结合文本嵌入和可学习查询实现显式与隐式的语义协同引导，提升语义一致性。同时，设计了全局与局部特征融合策略，增强了分割的精细度。大量实验验证了该方法在多种数据集和场景下的优越性能，展示了其在减少标注负担和提升分割准确性方面的潜力。,4
DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies,DiffProxy：基于扩散生成密集代理的多视角人体网格恢复,Diffusion Model,Other,https://arxiv.org/pdf/2601.02267,https://huggingface.co/papers/2601.02267,本文提出了DiffProxy，一种基于扩散生成模型的多视角人体网格恢复框架。DiffProxy通过生成多视角一致且像素对齐的人体代理，实现了从合成数据训练到真实场景的有效泛化，解决了真实数据标注不完善和合成数据域差异的问题。其核心创新包括多条件生成机制、灵活的局部细节增强模块以及不确定性感知的测试时优化策略。该方法在多个真实数据集上实现了领先性能，特别在遮挡和部分视角等复杂场景中表现出强大的零样本泛化能力。,4
Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy,通过System-2策略实现大型语言模型中大规模计数的机制可解释性,LLM,Other,https://arxiv.org/pdf/2601.02989,https://huggingface.co/papers/2601.02989,本文针对大型语言模型（LLMs）在大规模计数任务中因架构限制导致的准确性下降问题，提出了一种受到人类系统2思维启发的测试时策略。该策略将复杂的计数任务拆分为多个较小且独立的子任务，模型分别解决后再汇总结果。通过实验和因果分析，作者揭示了该方法的内部机制，包括潜在计数的计算与存储、专用注意力机制的传递以及最终的结果聚合。该方法显著提升了LLMs在大规模计数任务上的表现，提供了对模型推理过程的深入理解和一种通用的改进思路。,4
"Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",Muses：无训练条件下设计、组合与生成不存在的奇幻3D生物,Other,Other,https://arxiv.org/pdf/2601.03256,https://huggingface.co/papers/2601.03256,本文提出了Muses，一种无需训练即可前向生成奇幻3D生物的新方法。不同于依赖复杂零件优化或二维图像重建的传统技术，Muses基于3D骨骼结构，通过图约束推理合理设计并组合多样元素，进而生成结构连贯且风格统一的高质量3D模型。该方法不仅提升了视觉真实感和文本描述的一致性，还支持灵活的3D对象编辑，展示了在创造虚构生物方面的显著优势和广泛应用潜力。,4
OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs,OpenRT：面向多模态大语言模型的开源红队测试框架,Multimodal LLM,Shanghai AI Lab,https://arxiv.org/pdf/2601.01592,https://huggingface.co/papers/2601.01592,本文提出了OpenRT，一种开源的多模态大语言模型安全评估框架。该框架通过模块化设计，支持多维度、多策略的对抗测试，实现了对多种先进模型的系统化安全检测。实验证明，即使是领先模型在面对复杂、多轮攻击时仍存在显著安全漏洞，平均攻击成功率高达49.14%。OpenRT提供了一个可扩展、持续维护的平台，有助于推动大语言模型安全性的标准化和提升，促进其在关键应用中的安全可靠部署。,4
RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models,RedBench：用于大型语言模型全面红队测试的通用数据集,LLM,Other,https://arxiv.org/pdf/2601.03699,https://huggingface.co/papers/2601.03699,本文提出了RedBench，一个统一且全面的评估数据集，旨在检测大型语言模型（LLM）在多领域、多类型攻击下的安全风险。RedBench整合了37个现有基准数据集，包含近3万条攻击和拒绝样本，采用标准化的风险分类体系，覆盖22类风险和19个应用领域。该数据集及其开源评测工具为系统性评估和比较不同模型的脆弱性提供了有力支持，有助于推动更安全可靠的语言模型研发和实际应用。,4
Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts,为什么大语言模型还不是科学家：来自四次自主科研尝试的经验教训,Agent,Other,https://arxiv.org/pdf/2601.03315,https://huggingface.co/papers/2601.03315,本文通过四次尝试，系统评估了利用大型语言模型（LLM）自动生成机器学习研究论文的可行性。研究发现，尽管有一次成功完成并被学术会议接受，但大多数尝试因多种常见问题失败，如模型偏向训练数据、执行压力导致偏差、长任务中的记忆衰减、过早宣告成功、领域知识不足及实验设计不合理。基于这些经验，作者提出了四条设计原则，旨在指导构建更稳健的AI科学家系统，推动自主科学发现的发展。所有相关资源已公开，促进后续研究。,4
OpenTinker: Separating Concerns in Agentic Reinforcement Learning,OpenTinker：智能体强化学习中的关注点分离,Agent,Other,https://arxiv.org/pdf/2601.07376,https://huggingface.co/papers/2601.07376,本文提出了OpenTinker，一个针对大型语言模型代理的强化学习基础设施。该系统通过将算法设计、执行管理和代理与环境的交互分离，打破了传统单一的端到端强化学习流程，实现了模块化、可组合的组件设计。OpenTinker配备了集中调度器，统一管理训练与推理任务，支持多种训练方式并优化资源利用。此外，论文还探讨了多代理训练的扩展设计，并通过多个实际案例验证了该框架在复杂代理学习场景中的有效性，降低了强化学习系统的开发和复用难度。,4
UM-Text: A Unified Multimodal Model for Image Understanding,UM-Text：一种用于图像理解的统一多模态模型,Multimodal LLM,Other,https://arxiv.org/pdf/2601.08321,https://huggingface.co/papers/2601.08321,本文提出了UM-Text，一种统一的多模态模型，能够根据自然语言指令理解图像内容并进行视觉文本编辑。该模型通过视觉语言模型处理指令和参考图像，实现文本内容和布局的风格一致设计；结合多种条件信息的编码器提升文本生成的准确性和协调性。为增强训练效果，设计了区域一致性损失和分阶段训练策略。此外，作者构建了大规模多样化的视觉文本数据集UM-DATA-200K。实验结果表明，UM-Text在多个公开数据集上表现优异，显著提升了视觉文本编辑的自动化和质量。,4
VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding,VideoLoom：用于联合时空理解的视频大语言模型,Multimodal LLM,Other,https://arxiv.org/pdf/2601.07290,https://huggingface.co/papers/2601.07290,本文提出了VideoLoom，一种统一的视频大语言模型，旨在实现对视频中空间和时间信息的联合理解。为提升模型的细粒度定位能力，作者构建了包含时间和空间标注的人类视频数据集LoomData-8.7k，并设计了涵盖时间、空间及组合问题的评测基准LoomBench。实验证明，VideoLoom在多项空间和时间任务上表现优异，显著推动了视频多模态理解技术的发展，提供了一个通用且高效的解决方案。,4
The AI Hippocampus: How Far are We From Human Memory?,AI海马体：我们距离人类记忆还有多远？,Multimodal LLM,PKU,https://arxiv.org/pdf/2601.09113,https://huggingface.co/papers/2601.09113,本文系统梳理了大型语言模型及多模态模型中的记忆机制，提出了隐式、显式和智能体三种记忆范式。隐式记忆指模型内部参数中蕴含的知识，显式记忆通过外部存储增强信息交互，智能体记忆则支持自主代理的长期规划和协作。论文还探讨了多模态环境下记忆的整合及相关挑战，如容量限制和信息一致性。该综述为构建更具适应性和上下文理解能力的智能系统提供了理论框架和未来研究方向。,4
Geometric Stability: The Missing Axis of Representations,几何稳定性：表征的缺失轴,Other,Other,https://arxiv.org/pdf/2601.09173,https://huggingface.co/papers/2601.09173,本文提出了“几何稳定性”这一新的评估维度，用以衡量表示在扰动下结构的稳固性，弥补了传统相似性指标只能反映内容一致性但忽视结构鲁棒性的不足。作者设计了Shesha框架，在多个领域和配置中验证了稳定性与相似性之间几乎无关且机制不同。研究表明几何稳定性在安全监控、模型可控性和选择等方面具有显著优势，并能应用于生物系统分析。该工作为表示分析提供了必要且互补的视角，促进对模型及系统内部结构可靠性的全面理解。,4
Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments,Flow Equivariant World Models：用于部分观测动态环境的记忆机制,Embodied AI,Other,https://arxiv.org/pdf/2601.01075,https://huggingface.co/papers/2601.01075,本文提出了一种名为“流等变世界模型”的新方法，将自我运动和外部物体运动统一视为参数化的流动变化，从而实现对动态环境的稳定表示。该方法利用运动的对称性结构，有效捕捉部分观察到的环境动态，显著提升了在二维和三维视频世界建模任务中的表现，特别是在视野外的可预测动态场景下。实验结果表明，该模型在长时间预测中表现优越，展现了良好的泛化能力。该研究为数据高效、结构化的智能体环境建模提供了新的思路。,4
Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL,临床文本到SQL中的患者相似性队列推理,LLM,Other,https://arxiv.org/pdf/2601.09876,https://huggingface.co/papers/2601.09876,本文提出了CLINSQL基准，用于评估文本到SQL转换模型在真实临床电子健康记录（EHR）中的表现。该基准包含633个专家标注的任务，强调多表关联、时间推理及基于患者相似性的队列分析，要求生成可执行且临床相关的查询。研究测试了22种模型，结果显示尽管有进步，但现有模型在复杂临床场景下的准确性仍不足以达到临床应用标准。CLINSQL为推动临床文本到SQL的可靠自动化分析提供了重要资源和评价框架。,4
Deriving Character Logic from Storyline as Codified Decision Trees,基于故事线推导角色逻辑的编码决策树,Agent,Other,https://arxiv.org/pdf/2601.10080,https://huggingface.co/papers/2601.10080,本文提出了一种基于大规模叙事数据构建角色行为模型的新方法——编码决策树（CDT）。该方法通过将角色行为表示为条件规则树，实现了行为模型的可执行性和可解释性，支持在不同情境下准确调用相应规则。CDT通过迭代生成、验证和细化场景-行为规则，提升了模型的透明度和可维护性。实验证明，CDT在多个基准测试中显著优于传统人工编写和自动生成的行为模型，能够为角色扮演代理提供更稳定、一致的行为基础。,4
V-DPM: 4D Video Reconstruction with Dynamic Point Maps,V-DPM：基于动态点图的4D视频重建,Other,Other,https://arxiv.org/pdf/2601.09499,https://huggingface.co/papers/2601.09499,本文提出了V-DPM框架，将动态点图（Dynamic Point Maps）扩展应用于视频输入，实现了对动态场景的高精度4D重建。该方法不仅恢复了场景的三维形状和摄像机参数，还精确捕捉了每个点的三维运动，突破了传统点图仅适用于静态场景或图像对的限制。通过在已有静态3D重建模型VGGT基础上引入少量合成数据训练，V-DPM有效提升了动态场景下的重建性能，达到了当前最先进的效果，为动态视频的三维重建提供了实用且高效的解决方案。,4
ProPhy: Progressive Physical Alignment for Dynamic World Simulation,ProPhy：用于动态世界模拟的渐进物理对齐框架,Other,Other,https://arxiv.org/pdf/2512.05564,https://huggingface.co/papers/2512.05564,本文提出了ProPhy，一种分阶段的物理对齐框架，用于提升动态视频生成的物理一致性和真实感。ProPhy通过语义专家和细化专家两个模块，分别从文本描述和视频细节中提取物理先验，实现对物理规律的细粒度建模。同时，引入了物理对齐策略，将视觉语言模型的物理推理能力融入视频生成过程。实验结果表明，ProPhy在复杂动态场景下生成的视频更加符合物理规律，显著优于现有方法，推动了物理感知世界模拟的发展。,3
Group Representational Position Encoding,群表示位置编码,LLM,THU,https://arxiv.org/pdf/2512.07805,https://huggingface.co/papers/2512.07805,本文提出了GRAPE（Group Representational Position Encoding），一种基于群作用的统一位置编码框架。GRAPE结合了两类机制：乘法旋转和加性偏置，分别扩展了现有的RoPE和ALiBi方法。乘法旋转通过矩阵指数映射实现相对位置编码，保持范数不变且支持跨子空间特征耦合；加性偏置则通过低秩幺元作用实现，兼容流式缓存和相对位置关系。该框架为长序列模型提供了统一且灵活的位置编码设计空间，涵盖并超越了多种现有技术，提升了位置编码的表达能力和适用性。,3
Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning,Decouple to Generalize：面向数据稀缺视觉-语言推理的上下文优先自我进化学习,Multimodal LLM,Shanghai AI Lab,https://arxiv.org/pdf/2512.06835,https://huggingface.co/papers/2512.06835,本文提出了DoGe，一种针对数据稀缺环境下视觉-语言模型的双重解耦学习框架。该方法通过将上下文学习与问题求解分离，先聚焦于理解问题背景，再逐步进行任务解决，有效缓解了传统强化学习中因数据不足导致的训练不稳定和奖励欺骗问题。同时，DoGe引入动态课程学习策略，扩展领域知识库并迭代更新训练样本，提升数据多样性和模型泛化能力。实验结果表明，DoGe在多个基准测试中均优于现有方法，为实现自我进化的大规模视觉-语言模型提供了可行路径。,3
"Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation","Ground Slow, Move Fast：一种用于通用视觉-语言导航的双系统基础模型",Embodied AI,"Shanghai AI Lab, THU",https://arxiv.org/pdf/2512.08186,https://huggingface.co/papers/2512.08186,本文提出了DualVLN，一种结合高层推理与低层动作执行的双系统视觉语言导航模型。系统2基于大规模预训练模型，负责慢速生成中期路径目标，实现全局规划；系统1则快速响应，利用系统2提供的目标信息生成平滑准确的动作轨迹，实现实时控制。该设计有效解决了传统端到端方法动作碎片化和延迟高的问题，在多项导航基准测试和真实动态环境中表现出优越的泛化能力和适应性，推动了视觉语言导航技术向更实用方向发展。,3
Modular Neural Image Signal Processing,模块化神经图像信号处理,Other,Other,https://arxiv.org/pdf/2512.08564,https://huggingface.co/papers/2512.08564,本文提出了一种模块化的神经图像信号处理（ISP）框架，能够对原始图像数据进行分阶段处理并生成高质量的显示图像。该框架通过模块化设计，实现了对渲染过程各中间步骤的精细控制，提升了渲染准确性、系统扩展性和对未知相机的适应能力。此外，作者基于此框架开发了一个用户交互式照片编辑工具，支持多样化的图像风格调整和无限次后期重渲染。该方法参数规模适中，且在多个测试集上表现出竞争力的效果，展示了其在灵活性和实用性方面的优势。,3
Efficiently Reconstructing Dynamic Scenes One D4RT at a Time,高效重建动态场景：逐个D4RT进行,Other,DeepMind,https://arxiv.org/pdf/2512.08924,https://huggingface.co/papers/2512.08924,本文提出了D4RT，一种基于统一变换器架构的高效模型，用于从视频中重建动态四维场景。D4RT通过一种创新的查询机制，避免了传统方法中对每帧密集解码和多任务解码器的高计算开销，实现了对任意时空点的灵活三维位置探测。该方法结构轻量且易扩展，显著提升了训练和推理效率，在多种四维重建任务中优于现有技术，推动了动态场景理解与重建的发展。,3
SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting,SUCCESS-GS：面向高效静态与动态高斯点绘制的紧凑性与压缩技术综述,Other,Other,https://arxiv.org/pdf/2512.07197,https://huggingface.co/papers/2512.07197,本文系统综述了3D和4D高斯点云（Gaussian Splatting）技术中提升效率的关键方法，重点聚焦于参数压缩和结构重组两大方向，以降低存储和计算资源需求，同时保持高质量的三维重建效果。通过分类总结现有技术、评测指标及数据集，论文首次提供了统一的视角，全面展示了静态与动态场景下高斯点云的紧凑表示与加速渲染策略。此外，文章分析了当前方法的局限性，并展望了未来实现可扩展、实时和高效三维表示的研究方向，推动该领域应用于虚拟现实、自动驾驶等实际场景。,3
IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting,IF-Bench：基于生成视觉提示的多模态大语言模型红外图像基准测试与增强,Multimodal LLM,Other,https://arxiv.org/pdf/2512.09663,https://huggingface.co/papers/2512.09663,本文提出了IF-Bench，这是首个专门用于评估多模态大语言模型（MLLMs）对红外图像理解能力的高质量基准，包含499张红外图像和680个精心设计的问答对，覆盖图像理解的多个关键方面。基于该基准，作者系统测试了40余种开源和闭源模型，分析了模型规模、结构及推理方式对红外图像理解的影响。此外，论文创新性地提出了一种无需训练的生成式视觉提示方法，通过将红外图像转换为语义和空间上对齐的可见光图像，有效缓解了域差异问题，显著提升了模型性能。该工作为红外图像多模态理解提供了重要工具和方法。,3
TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression,TED-4DGS：基于时间激活与嵌入变形的4DGS压缩方法,Other,Other,https://arxiv.org/pdf/2512.05446,https://huggingface.co/papers/2512.05446,本文提出了TED-4DGS，一种结合时间激活和嵌入式变形的新型动态三维高斯点云压缩方法。该方法通过为稀疏锚点赋予可学习的时间激活参数，实现对动态场景中物体出现和消失的精准建模，并利用轻量级时间嵌入查询共享变形库，提升变形表达能力。同时，采用隐式神经表示的先验模型和自回归结构优化压缩性能。实验表明，TED-4DGS在多个真实动态场景数据集上实现了更优的画质与压缩率平衡，显著提升了动态三维场景的压缩效率和质量。,3
Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task,面向视频问答任务的工具增强时空推理,Multimodal LLM,THU,https://arxiv.org/pdf/2512.10359,https://huggingface.co/papers/2512.10359,本文提出了一种增强多模态大语言模型的视频时空推理框架，通过引入丰富且多样化的视频工具包，提升模型对视频中空间关系和时间动态的理解能力。该框架通过合理调度时空工具，逐步定位视频关键区域，有效避免了工具调用的捷径问题。实验结果显示，该方法在多个视频问答基准上显著提升了性能，展示了其在构建智能视频分析助手方面的重要潜力。代码已开源，便于后续研究和应用。,3
H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos,H2R-Grounder：一种无需配对数据的人类交互视频到物理约束机器人视频的转换范式,Embodied AI,Other,https://arxiv.org/pdf/2512.09406,https://huggingface.co/papers/2512.09406,本文提出了一种无需配对数据的新方法，将普通的人类物体交互视频转换为具有物理真实性的机器人操作视频。通过去除训练视频中的机器人臂部并添加简易视觉标记，模型学会在测试时根据人类动作生成对应的机器人动作，保持动作连续性和背景一致性。该方法无需人机配对视频，仅依赖未配对的机器人视频，极大简化了训练过程。实验结果显示，该方法生成的机器人视频更真实且符合物理规律，为利用大量未标注的人类视频提升机器人学习能力提供了新思路。,3
Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents,Fed-SE：面向隐私受限多环境大语言模型智能体的联邦自我进化框架,Agent,Other,https://arxiv.org/pdf/2512.08870,https://huggingface.co/papers/2512.08870,本文提出了Fed-SE，一种面向隐私受限环境中多样化任务的大型语言模型（LLM）智能体的联邦自我进化框架。该方法通过在本地对高质量交互轨迹进行参数高效微调，实现稳定的模型更新；在全局则采用低秩子空间聚合，有效分离不同环境的特性，减少负迁移。实验证明，Fed-SE在五个异构环境中较传统联邦学习提升了约18%的任务成功率，展示了其在保护隐私前提下实现跨环境知识共享和智能体能力提升的潜力。,3
ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning,ReViSE：基于自反性学习的统一模型中面向推理的视频编辑,Multimodal LLM,Other,https://arxiv.org/pdf/2512.09924,https://huggingface.co/papers/2512.09924,本文提出了ReViSE框架，旨在提升视频编辑模型在理解和推理基础上的编辑能力。针对现有数据集不足及模型推理与编辑脱节的问题，作者设计了包含物理合理性和因果动态推理的新任务——Reason-Informed Video Editing（RVE），并构建了相应的评测基准RVE-Bench。ReViSE通过内部视觉语言模型的自我反馈机制，实现生成与评估的统一，显著提升了编辑的准确性和视觉质量。实验结果表明，该方法在推理驱动的视频编辑任务中较现有方法提升了32%的综合表现。,3
Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge,Vision-Language-Action模型的任务适应：2025年BEHAVIOR挑战赛冠军方案,Embodied AI,Other,https://arxiv.org/pdf/2512.06951,https://huggingface.co/papers/2512.06951,本文提出了一种视觉-动作策略，在2025年BEHAVIOR挑战赛中荣获第一名。该策略针对50个多样化的家庭任务，结合双手操作、导航和环境感知，显著提升了任务执行的效率和准确性。核心创新包括引入相关噪声以优化训练过程，实现动作序列的平滑衔接；采用可学习的混合层注意力机制和阶段跟踪方法以解决状态歧义问题。实验结果表明，该方法在公开和私有排行榜上均取得26%的综合评分，展示了其在复杂长时序任务中的优越性能和适应能力。,3
CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare,CLINIC：评估医疗领域多语言大语言模型的可信性,LLM,Other,https://arxiv.org/pdf/2512.11437,https://huggingface.co/papers/2512.11437,本文提出了CLINIC，一个面向医疗领域的多语言信任度评估基准，涵盖真确性、公平性、安全性、鲁棒性和隐私五个关键维度。通过对15种语言、18项任务的系统测试，CLINIC揭示了现有语言模型在事实准确性、偏见控制、隐私保护和抗攻击能力方面存在显著不足。该基准为提升多语言环境下医疗语言模型的可靠性和安全性奠定了基础，推动其在全球多样化医疗场景中的应用。,3
Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems,因果评判者评估：用于大语言模型系统的校准替代指标,LLM,Other,https://arxiv.org/pdf/2512.11150,https://huggingface.co/papers/2512.11150,本文提出了Causal Judge Evaluation（CJE）框架，旨在提升大语言模型评估的准确性和效率。针对当前使用AI模型作为评判者时存在的评分未校准、置信区间失效以及权重不稳定等问题，CJE通过奖励校准、权重稳定化和不确定性感知推断三大技术手段加以解决。实验证明，CJE在大规模对话数据上实现了接近人工标注的排序准确率，同时显著降低评估成本。该方法为大规模模型评估提供了更可靠且经济的替代方案。,3
RecTok: Reconstruction Distillation along Rectified Flow,RecTok：沿校正流的重建蒸馏,Diffusion Model,PKU,https://arxiv.org/pdf/2512.13421,https://huggingface.co/papers/2512.13421,本文提出了RecTok，一种通过丰富扩散模型中前向流的语义信息和增强重建能力的新方法，成功突破了高维视觉令牌器在生成质量上的限制。RecTok通过语义蒸馏和重建对齐蒸馏两大创新，使训练过程中的流匹配具备更丰富的语义表达，从而提升图像重建和生成效果。实验结果显示，RecTok在多种设置下均实现了当前最佳的生成性能，且随着潜在空间维度增加，性能持续提升，显著加快了训练收敛速度。该方法为扩散模型的高维潜在空间设计提供了新的思路。,3
Towards Interactive Intelligence for Digital Humans,迈向数字人交互智能,Embodied AI,Other,https://arxiv.org/pdf/2512.13674,https://huggingface.co/papers/2512.13674,本文提出了“互动智能”这一数字人新范式，赋予数字人个性化表达、自适应交互及自我进化能力。为实现这一目标，作者设计了Mio框架，集成了认知推理与实时多模态表现，通过五个核心模块协同工作，实现流畅且连贯的互动体验。论文还建立了新的评测标准，实验证明该框架在多项指标上优于现有方法，推动数字人从简单模仿向智能交互迈进，具有重要的理论和应用价值。,3
CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models,CAPTAIN：用于文本到图像扩散模型中记忆缓解的语义特征注入,Diffusion Model,Other,https://arxiv.org/pdf/2512.10655,https://huggingface.co/papers/2512.10655,本文提出了CAPTAIN，一种无需重新训练的框架，用于缓解文本到图像扩散模型中的记忆复现问题。CAPTAIN通过在图像生成的去噪阶段直接修改潜在特征，采用频率噪声初始化减少早期的记忆复制倾向，定位并注入语义对齐的非记忆参考图像特征，从而有效抑制模型对训练样本的重复生成。实验证明，CAPTAIN在大幅降低记忆复现的同时，保持了生成图像对输入文本的高度一致性和良好的视觉质量，为扩散模型的安全合规应用提供了实用解决方案。,3
What matters for Representation Alignment: Global Information or Spatial Structure?,表示对齐的关键因素：全局信息还是空间结构？,Diffusion Model,Other,https://arxiv.org/pdf/2512.10794,https://huggingface.co/papers/2512.10794,本文研究了视觉表示对生成模型训练中表示对齐效果的关键影响因素，重点比较了全局语义信息与空间结构的作用。通过对27种不同视觉编码器的大规模实证分析，发现空间结构——即图像局部特征之间的关系——比全局语义性能更能提升生成效果。基于此，作者提出了两项简单改进，强化空间信息的传递，显著加快了训练收敛速度。该工作挑战了传统观点，为更有效地利用预训练视觉表示指导生成模型训练提供了新的思路。,3
FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering,FrameDiffuser：基于G-Buffer条件的神经前向帧渲染扩散模型,Diffusion Model,Other,https://arxiv.org/pdf/2512.16670,https://huggingface.co/papers/2512.16670,本文提出了FrameDiffuser，一种基于G-buffer数据的自回归神经渲染框架，能够生成具有时间一致性的高质量真实感图像。该方法通过结合结构引导和时间连贯性机制，实现了在长序列中稳定渲染，适用于交互式应用场景。与现有单帧独立生成或计算开销过大的视频模型相比，FrameDiffuser在保证渲染速度的同时，显著提升了光照、阴影和反射的真实感。通过针对特定环境的训练，模型在一致性和效率上表现优异，展示了其在实时神经渲染中的潜力和实用价值。,3
Bidirectional Normalizing Flow: From Data to Noise and Back,双向归一化流：从数据到噪声再回归,Other,THU,https://arxiv.org/pdf/2512.10953,https://huggingface.co/papers/2512.10953,本文提出了一种名为双向正则流（BiFlow）的生成模型框架，突破了传统正则流必须具备精确可逆变换的限制。BiFlow通过学习一个反向模型来近似从噪声到数据的映射，允许更灵活的模型设计和训练方法。实验证明，在ImageNet数据集上，BiFlow相比传统基于因果解码的正则流显著提升了生成质量，同时采样速度提升了百倍。该方法在正则流领域达到了最先进的性能，并在单次评估生成方法中表现竞争力，推动了经典生成模型的发展。,3
A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos,面向长视频的全模态推理与工具使用的基准与智能体框架,Multimodal LLM,Other,https://arxiv.org/pdf/2512.16978,https://huggingface.co/papers/2512.16978,本文提出了LongShOTBench，一套针对长时多模态视频理解的诊断基准，涵盖开放式问题、对话及多模态推理任务，结合视频、语音和环境音频，支持细致且可追踪的评估。该基准通过人工验证确保数据质量和可复现性，填补了现有数据集在时长和多模态复杂性兼顾上的不足。论文还设计了LongShOTAgent，一种基于预处理、搜索和迭代优化的智能系统，用于分析长视频。实验显示现有多模态大模型在该基准上表现有限，凸显长视频理解的挑战。LongShOTBench为提升多模态模型的长时视频理解能力提供了实用评价平台。,3
Name That Part: 3D Part Segmentation and Naming,Name That Part：3D 部件分割与命名,Other,Other,https://arxiv.org/pdf/2512.18003,https://huggingface.co/papers/2512.18003,本文提出了ALIGN-Parts，一种创新的3D物体部件语义分割与命名方法。该方法通过将隐式3D部件表示与文本描述对齐，实现了开放词汇的部件识别，克服了现有数据集中部件定义不一致的问题。结合几何特征、多视角视觉信息及语言模型生成的语义知识，ALIGN-Parts能够高效地完成一次性分割与命名任务。该模型支持零样本匹配和置信度校准，促进了多数据集统一本体的构建，推动了3D部件标注的自动化和应用拓展。论文还引入了适用于命名分割的新评估指标。,3
CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion,CASA：通过自注意力实现高效视觉-语言融合的交叉注意力方法,Multimodal LLM,Other,https://arxiv.org/pdf/2512.19535,https://huggingface.co/papers/2512.19535,本文提出了CASA，一种结合自注意力机制的跨模态注意力方法，用于高效融合视觉与语言信息。传统将图像信息直接插入语言模型输入的方法虽然效果好，但计算和内存开销大，尤其在处理高分辨率图像和长时间多模态内容时。相比之下，CASA通过在跨模态注意力层中引入局部文本交互，显著提升了模型对细粒度视觉细节的理解能力，同时保持了跨注意力模型的高效性和可扩展性。实验证明，CASA在多个视觉理解任务中接近传统插入方法的性能，且更适合长上下文多模态应用。,3
QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models,QuantiPhy：评估视觉-语言模型物理推理能力的定量基准,Multimodal LLM,Other,https://arxiv.org/pdf/2512.19526,https://huggingface.co/papers/2512.19526,本文提出了QuantiPhy，一个首创的基准测试，旨在定量评估视觉语言模型对物理属性（如物体大小、速度和加速度）的推理能力。该数据集包含3300多个带有数值真值的视频文本实例，通过统一的提示和评分标准，公平衡量模型在物理量估计上的准确性。实验发现，尽管现有模型在表面上表现出合理的理解，但在数值推断上存在显著差距，且过于依赖预训练知识而非视觉和文本输入。QuantiPhy为推动视觉语言模型从模糊描述向精确物理理解迈进提供了重要工具。,3
Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems,Simulstream：用于流式语音转文本翻译系统评估与演示的开源工具包,Other,Other,https://arxiv.org/pdf/2512.17648,https://huggingface.co/papers/2512.17648,本文提出了simulstream，一个首个开源的统一评测与演示工具包，专为实时语音到文本翻译系统设计。该工具支持处理长时连续语音，兼容两种主流翻译策略——增量解码与重译，方便在同一平台上比较它们的翻译质量和响应速度。此外，simulstream还提供了交互式网页界面，便于展示和测试不同系统，弥补了现有工具不维护、功能有限的不足，推动了实时语音翻译技术的研究与应用。,3
GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training,GTR-Turbo：合并检查点作为Agentic VLM训练中的免费教师,Agent,"THU, Tencent, PKU",https://arxiv.org/pdf/2512.13043,https://huggingface.co/papers/2512.13043,本文提出了GTR-Turbo，一种高效的多模态视觉语言模型强化学习训练方法。该方法通过合并训练过程中生成的多个模型检查点，构建一个无需额外训练或查询昂贵教师模型的“免费”教师，从而指导后续训练。相比现有依赖大型预训练教师模型的方法，GTR-Turbo不仅避免了对特权模型的依赖，提升了训练稳定性，还显著提高了模型性能（提升10-30%准确率），同时大幅降低了训练时间和计算成本（分别减少50%和60%），在多样化视觉任务中展现出优越的实用价值。,3
Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation,Knot Forcing：驯服自回归视频扩散模型实现实时无限交互式人像动画,Diffusion Model,Alibaba,https://arxiv.org/pdf/2512.21734,https://huggingface.co/papers/2512.21734,本文提出了Knot Forcing，一种针对实时人像动画的流式生成框架，解决了现有自回归视频扩散模型在连续生成中出现的错误积累、运动不连贯和长时一致性差的问题。该方法通过分块生成结合全局身份信息缓存、邻近块重叠的时序节点模块以及动态更新参考帧时间坐标的机制，实现了高保真、时序一致且响应迅速的无限序列人像动画。Knot Forcing在普通消费级GPU上支持实时运行，显著提升了交互式虚拟助手和直播头像等应用的视觉稳定性和用户体验。,3
Bridging Your Imagination with Audio-Video Generation via a Unified Director,通过统一导演模型连接你的想象与视听生成,Multimodal LLM,ByteDance,https://arxiv.org/pdf/2512.23222,https://huggingface.co/papers/2512.23222,本文提出了UniMAGE，一种统一的导演模型，将视频脚本创作与关键帧生成整合于单一框架中，打破了传统方法中两者分离的局限。通过结合文本与图像生成的混合变换器架构，并采用“先交织学习再解耦训练”的策略，UniMAGE提升了故事叙述的逻辑连贯性和视觉关键帧的一致性。该模型使非专业用户能够基于简单提示，生成结构合理、画面统一的多镜头长视频，显著推动了AI驱动的音视频创作技术发展。,3
Valori: A Deterministic Memory Substrate for AI Systems,[翻译]Valori: A Deterministic Memory Substrate for AI Systems,Other,Other,https://arxiv.org/pdf/2512.22280,https://huggingface.co/papers/2512.22280,本文提出了Valori，一种基于定点运算的确定性AI内存系统，旨在解决当前基于浮点运算的向量嵌入存储和检索中存在的跨硬件平台结果不一致问题。Valori通过将内存视为可重放的状态机，实现了跨不同硬件架构的位级一致性，保障了内存状态、快照和搜索结果的可复现性。该方法提升了AI系统的可信度和审计能力，特别适用于对结果可验证性要求高的应用场景。Valori的开源实现为构建可靠的AI记忆基础提供了重要支持。,3
Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning,通过方向性解耦对齐驯服扩散强化学习中的偏好模式崩溃,Diffusion Model,"THU, Alibaba",https://arxiv.org/pdf/2512.24146,https://huggingface.co/papers/2512.24146,本文针对文本到图像扩散模型在强化学习过程中出现的“偏好模式崩溃”问题，即模型生成内容趋于单一且缺乏多样性，提出了一种新的解决方案。作者设计了一个评估基准DivGenBench来量化该现象，并提出了方向性解耦对齐（D²-Align）方法，通过调整奖励信号的方向，有效避免模型过度优化导致的单一输出。实验结果表明，该方法在提升与人类偏好一致性的同时，显著保持了生成图像的多样性，突破了以往方法在质量与多样性之间的权衡。,3
FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing,FFP-300K：面向通用视频编辑的首帧传播规模化方法,Other,Tencent,https://arxiv.org/pdf/2601.01720,https://huggingface.co/papers/2601.01720,本文提出了FFP-300K，一个包含30万对高质量720p视频的全新大规模数据集，专为视频编辑中的首帧传播任务设计，解决了现有数据集规模小、分辨率低和编辑多样性不足的问题。基于此，作者设计了一种无需运行时指导的新框架，通过自适应时空编码和自我蒸馏策略，有效保持首帧外观与视频运动的一致性。实验结果表明，该方法在多个指标上显著优于现有学术和商业模型，提升了视频编辑的稳定性和视觉质量。,3
MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning,MindWatcher：迈向更智能的多模态工具集成推理,Agent,Other,https://arxiv.org/pdf/2512.23412,https://huggingface.co/papers/2512.23412,本文提出了MindWatcher，一种集成多模态链式思维和交替思考的智能推理代理，能够自主决定并协调调用多种辅助工具，完成复杂的多步骤决策任务，无需人工提示。MindWatcher通过图像处理提升推理精度，配备了丰富的辅助工具和涵盖多领域的本地图像检索数据库，增强了小模型的识别能力。作者还构建了专门的评测基准和高质量训练数据，设计了高效的训练架构。实验表明，MindWatcher在工具调用和推理表现上优于更大规模模型，推动了自主智能代理的发展。,3
Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks,通过语言学习任务预训练提升语言模型的语言能力,LLM,Other,https://arxiv.org/pdf/2601.03448,https://huggingface.co/papers/2601.03448,本文提出了一种名为L2T的预训练框架，通过结合传统的下一词预测任务与结构化的语言学习任务，提升语言模型的语言理解能力。该方法模拟人类语言习得过程，将原始文本转化为明确的输入输出对，提供更直接的语言知识刺激。实验表明，L2T不仅加速了语言能力的获得，还在语言理解基准测试中表现优异，同时保持了模型在一般推理任务上的竞争力。该研究为增强语言模型的语言能力提供了新的有效途径。,3
ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing,ThinkRL-Edit：面向推理中心图像编辑的强化学习思考框架,Agent,ByteDance,https://arxiv.org/pdf/2601.03467,https://huggingface.co/papers/2601.03467,本文提出了ThinkRL-Edit，一种基于强化学习的图像编辑框架，专注于提升图像编辑中的视觉推理能力。该方法通过引入链式思维（Chain-of-Thought）机制，实现编辑前的多方案推理与验证，突破了以往仅依赖去噪随机性的探索限制。同时，采用无偏奖励融合策略和更稳定的二元检查表奖励，提升了训练的准确性和稳定性。实验证明，ThinkRL-Edit在保持编辑指令一致性的同时，显著增强了编辑结果的语义合理性和视觉连贯性，优于现有方法。,3
DocDancer: Towards Agentic Document-Grounded Information Seeking,DocDancer：迈向具代理性的文档驱动信息检索,Agent,"PKU, Shanghai AI Lab, Tencent",https://arxiv.org/pdf/2601.05163,https://huggingface.co/papers/2601.05163,本文提出了DocDancer，一种开源且端到端训练的文档问答智能体，将文档问答任务视为信息检索问题，设计了基于工具的框架以提升文档的探索和理解能力。针对高质量训练数据匮乏的问题，作者引入了“先探索后综合”的数据合成方法。实验证明，DocDancer在长文本理解基准测试中表现出色，且分析结果为智能体工具设计和合成数据的优化提供了有益启示。该工作推动了文档问答系统在开放环境下的实用性和透明度。,3
DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation,DR-LoRA：用于专家混合模型适配的动态秩LoRA,LLM,"THU, PKU",https://arxiv.org/pdf/2601.04823,https://huggingface.co/papers/2601.04823,本文提出了DR-LoRA，一种针对混合专家模型（MoE）动态调整低秩适配（LoRA）参数规模的方法。传统方法为所有专家分配相同的参数规模，忽视了专家在不同任务中的功能差异，导致资源分配不均。DR-LoRA通过专家重要性评分，依据任务需求自动调整各专家的参数规模，实现更高效的参数利用。实验证明，DR-LoRA在多个任务上优于固定参数分配的LoRA方案，提升了模型性能和适配效率。,3
Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning,记忆更重要：作为智能体搜索与推理逻辑地图的事件中心记忆,Agent,Other,https://arxiv.org/pdf/2601.04726,https://huggingface.co/papers/2601.04726,本文提出了CompassMem，一种基于事件的记忆框架，通过将经历分割为事件并构建事件图，明确表达事件之间的逻辑关系，从而实现结构化的记忆管理和检索。该方法突破了传统扁平化和简单相似度检索的局限，使智能代理能够在长时间跨度内有目标地导航记忆，支持更有效的推理和决策。实验证明，CompassMem在多个任务和模型上均显著提升了记忆检索和推理性能，推动了智能代理在复杂环境中的长期交互能力。,3
Over-Searching in Search-Augmented Large Language Models,搜索增强大语言模型中的过度搜索问题,LLM,Other,https://arxiv.org/pdf/2601.05503,https://huggingface.co/papers/2601.05503,本文系统分析了搜索增强大型语言模型中过度搜索的问题，即模型在不提升回答质量的情况下频繁调用搜索工具，导致计算资源浪费和错误信息产生。研究发现，搜索对可回答问题有助益，但对无法回答的问题反而降低了模型的谨慎性；复杂推理模型和多轮对话中该问题更为严重；检索内容的质量对结果影响显著。为衡量这一现象，作者提出了新的评估指标“每正确答案代价”。此外，论文探讨了缓解策略并发布了相关数据集，推动高效搜索增强模型的研究。,3
Structured Episodic Event Memory,结构化情节事件记忆,Agent,Alibaba,https://arxiv.org/pdf/2601.06411,https://huggingface.co/papers/2601.06411,本文提出了一种名为Structured Episodic Event Memory（SEEM）的分层记忆框架，结合图结构记忆和动态情节记忆，有效提升大型语言模型在长期交互中的叙事连贯性和推理能力。SEEM基于认知框架理论，将交互信息转化为结构化的事件框架，并通过关联融合与逆向溯源机制重建完整的叙事上下文。实验证明，该方法在多个基准测试中显著优于现有技术，为自主智能体的长期记忆管理提供了新的解决思路。,3
Are LLM Decisions Faithful to Verbal Confidence?,LLM的决策是否忠实于口头置信度？,LLM,Other,https://arxiv.org/pdf/2601.07767,https://huggingface.co/papers/2601.07767,本文提出了RiskEval框架，评估大型语言模型（LLM）在面对不同错误惩罚时是否能合理调整其放弃回答的策略。研究发现，尽管模型能表达对自身答案的信心，但在高惩罚环境下，它们并未表现出应有的风险意识，几乎不主动放弃回答，导致整体效用显著下降。结果表明，模型的口头置信度虽具一定校准性，但不足以保证其决策的可信性和风险敏感性，提示未来需增强模型将不确定性转化为合理决策的能力。,3
e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings,e5-omni：用于全模态嵌入的显式跨模态对齐,Multimodal LLM,Other,https://arxiv.org/pdf/2601.03666,https://huggingface.co/papers/2601.03666,本文提出了e5-omni，一种针对多模态嵌入模型的显式对齐方法，解决了传统模型中不同模态间相似度尺度不一致、负样本难度分布失衡及嵌入统计特性不匹配等问题。该方法通过模态感知的温度校准、可控负样本策略和批量白化正则化三项轻量级技术，提升了多模态嵌入的鲁棒性和匹配效果。实验结果表明，e5-omni在多个多模态数据集上优于现有基线方法，且具备良好的通用性和迁移能力，推动了多模态信息检索的发展。,3
Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale,野外智能体技能：大规模安全漏洞的实证研究,Agent,Other,https://arxiv.org/pdf/2601.10338,https://huggingface.co/papers/2601.10338,本文首次对AI代理技能这一新兴生态进行了大规模安全分析，收集并检测了超过4万份技能包。研究发现约26%的技能存在安全漏洞，主要包括提示注入、数据泄露、权限提升和供应链风险，其中数据泄露和权限提升最为常见。带有可执行脚本的技能漏洞风险显著更高。论文构建了漏洞分类体系，提出了高效的检测方法，并公开了相关数据和工具。研究强调了加强权限管理和安全审核的重要性，以防止技能被恶意利用。,3
Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning,Colon-X：从多模态理解到临床推理的智能结肠镜检查进展,Multimodal LLM,Other,https://arxiv.org/pdf/2512.03667,https://huggingface.co/papers/2512.03667,本文提出了Colon-X项目，致力于提升结肠镜检查中的多模态人工智能能力。研究构建了包含超过110万视觉问答条目的大型多模态数据集ColonVQA，涵盖76种临床发现和18项任务。通过系统评估现有多模态大模型的泛化性和稳定性，发现其临床表现尚不可靠。为此，团队设计了基于专家辩论标注的推理数据集ColonReason，并开发了推理驱动模型ColonR1，在有限数据下显著提升准确率，推动了从多模态理解向临床推理的关键转变。所有资源均公开，助力智能结肠镜领域发展。,2
SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs,SQ-format：一种面向大语言模型的统一稀疏量化硬件友好数据格式,LLM,ByteDance,https://arxiv.org/pdf/2512.05409,https://huggingface.co/papers/2512.05409,本文提出了一种名为SQ-format的统一稀疏量化数据格式，旨在提升大语言模型后训练量化的准确性与计算效率。该格式结合了稀疏矩阵的高精度加速和低精度矩阵乘法的优势，解决了现有低位宽量化与稀疏化方法在硬件支持上的局限。SQ-format特别适用于处理激活中的异常值，实现静态压缩，并在保持模型性能的同时显著提高推理速度。论文还探讨了支持该格式的硬件设计，为下一代AI加速器的发展提供了方向。,2
TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation,TimesNet-Gen：基于深度学习的站点特异性强震动生成,AI4Science,Other,https://arxiv.org/pdf/2512.04694,https://huggingface.co/papers/2512.04694,本文提出了一种名为TimesNet-Gen的深度学习模型，用于生成具有站点特征的强地震动记录。该方法通过时间域条件生成器和站点特定的潜在瓶颈，有效捕捉了不同地质条件对地震波形的影响。实验结果显示，TimesNet-Gen在站点特异性表现上优于基于频谱图的条件变分自编码器，能够更准确地模拟地震动的关键频率特征。该模型为地震风险评估提供了更精细的站点级地震动合成工具，有助于提升地震防灾减灾能力。代码已公开，便于进一步研究和应用。,2
Rethinking Training Dynamics in Scale-wise Autoregressive Generation,重新思考Scale-wise自回归生成中的训练动态,Other,Other,https://arxiv.org/pdf/2512.06421,https://huggingface.co/papers/2512.06421,本文针对视觉领域中按尺度逐步生成图像的自回归模型存在的训练与测试不匹配及不同尺度学习难度不均的问题，提出了一种名为自回归自我优化（SAR）的方法。该方法通过引入分阶段尺度展开和对比学生强制损失，使模型在训练时能更好地适应自身预测结果，提升生成质量。实验表明，SAR能显著降低生成误差且计算开销较低，展现出良好的效率和可扩展性，适合作为视觉自回归生成模型的后期优化手段。,2
VideoVLA: Video Generators Can Be Generalizable Robot Manipulators,VideoVLA：视频生成器可作为通用机器人操控器,Embodied AI,Microsoft,https://arxiv.org/pdf/2512.06963,https://huggingface.co/papers/2512.06963,本文提出了VideoVLA，一种基于多模态扩散变换器的视频生成模型，用于机器人操作任务中的动作和视觉结果预测。与传统依赖视觉语言理解模型的方法不同，VideoVLA利用预训练的视频生成模型同时预测动作序列及其未来视觉影响，实现了对新任务、新物体和不同操作技能的强大泛化能力。实验表明，高质量的视觉想象显著提升了动作预测的准确性和任务成功率，展示了视觉想象在机器人操作中的重要作用。该方法为机器人学习开辟了新的方向，推动了通用操作系统的发展。,2
One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation,一层足矣：预训练视觉编码器在图像生成中的适配,Diffusion Model,Other,https://arxiv.org/pdf/2512.07829,https://huggingface.co/papers/2512.07829,本文提出了一种名为FAE的简单高效框架，用于将预训练的视觉编码器适配到图像生成任务中。FAE通过仅使用一层注意力机制和两个独立的解码器，实现了从高维理解特征到低维生成潜空间的有效转换，兼顾了信息重建与生成需求。该方法兼容多种自监督编码器，并能应用于扩散模型和归一化流等生成模型。在多个图像生成基准测试中，FAE表现出色，既提升了生成质量，也加快了训练速度，展示了预训练视觉表示在图像生成领域的广泛适用性和潜力。,2
EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce,EcomBench：面向电商领域基础智能体的整体评估,Agent,Alibaba,https://arxiv.org/pdf/2512.08868,https://huggingface.co/papers/2512.08868,本文提出了EcomBench，一个专为电商领域设计的综合评测基准，旨在真实环境中评估智能代理的核心能力。与传统多聚焦于学术或模拟场景的评测不同，EcomBench基于全球领先电商平台的真实用户需求，涵盖多样任务和不同难度等级，重点考察代理在深度信息检索、多步推理及跨源知识整合等方面的表现。该基准为衡量智能代理在动态复杂电商环境中的实际应用能力提供了严谨且具代表性的测试平台，推动了智能代理技术向现实场景的有效转化。,2
TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels,TrackingWorld：基于世界中心单目几乎所有像素的三维跟踪,Embodied AI,Other,https://arxiv.org/pdf/2512.08358,https://huggingface.co/papers/2512.08358,本文提出了TrackingWorld，一种基于单目视频的世界中心三维像素跟踪方法。该方法通过引入跟踪放大器，将稀疏的二维轨迹高效扩展为密集轨迹，并通过优化框架同时估计相机位姿和三维坐标，实现对几乎所有像素的稠密三维跟踪。相比现有方法，TrackingWorld有效区分了相机运动与前景动态，且能跟踪视频中新出现的动态对象。大量合成及真实数据集实验验证了该方法在三维跟踪精度和密度上的显著提升，推动了单目三维运动分析的发展。,2
Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images,基于MRI图像的脑肿瘤分类与分割的新型深度学习架构,Other,Other,https://arxiv.org/pdf/2512.06531,https://huggingface.co/papers/2512.06531,本文提出了两种新颖的深度学习模型，分别用于脑肿瘤的分类和分割，显著提升了MRI图像中脑肿瘤的自动检测性能。分类模型SAETCN在包含三种常见脑肿瘤（胶质瘤、脑膜瘤和垂体瘤）及非肿瘤图像的数据集上实现了99.38%的准确率；分割模型SAS-Net则达到了99.23%的像素准确率。该研究为脑肿瘤的早期诊断提供了高效且可靠的计算机辅助方法，有助于减轻医生负担并促进精准医疗的发展。,2
LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning,LYNX：用于置信度控制推理的动态早停学习机制,LLM,Other,https://arxiv.org/pdf/2512.05325,https://huggingface.co/papers/2512.05325,本文提出了LYNX，一种基于模型内部状态的动态早停机制，用于提升大型推理模型的计算效率和准确性。LYNX通过识别生成过程中的自然提示词，训练轻量级探针预测何时停止推理，并利用无分布假设的置信控制方法避免过早退出。该方法仅需一次训练，便能跨多种任务和模型规模实现高效推理。在多个数学和常识问答基准测试中，LYNX显著减少推理所需的计算量，同时保持或提升准确率，优于现有早停技术，且无需额外辅助模型，支持用户自定义置信度控制，具有广泛应用潜力。,2
"Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS",超越统一模型：面向实时TTS的低延迟上下文感知音素化的面向服务方法,Other,Other,https://arxiv.org/pdf/2512.08006,https://huggingface.co/papers/2512.08006,本文提出了一种面向服务的实时语音合成框架，通过轻量级的上下文感知音素转换策略，有效提升了发音准确性和语言自然度，同时保证了低延迟性能。该方法将复杂的上下文处理模块与核心语音合成引擎解耦，实现高质量音素转换模型的实时应用。实验结果表明，该系统在保持响应速度的前提下，显著改善了语音的自然性和准确性，适用于离线及终端设备的实时文本转语音场景。,2
VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory,VideoSSM：结合混合状态空间记忆的自回归长视频生成,Diffusion Model,ByteDance,https://arxiv.org/pdf/2512.04519,https://huggingface.co/papers/2512.04519,本文提出了VideoSSM，一种结合自回归扩散模型与混合状态空间记忆的新方法，用于长视频生成。该模型通过融合全局的状态空间记忆和局部的上下文窗口，有效协调短期与长期信息，解决了长视频中常见的错误累积、运动漂移和内容重复问题。实验结果表明，VideoSSM在保持视频时序一致性和运动稳定性方面表现优异，支持多样化内容生成和交互式控制，展示了其在长视频生成领域的显著优势和良好扩展性。,2
Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale,Confucius Code Agent：工业规模的开源AI软件工程师,Agent,Meta,https://arxiv.org/pdf/2512.10398,https://huggingface.co/papers/2512.10398,本文提出了Confucius Code Agent（CCA），一个面向工业规模的软件工程AI编码代理，基于开源的Confucius SDK开发。该平台通过统一的协调机制、层级记忆系统和跨会话持续学习能力，实现了对大规模代码库的高效推理与复杂工具链的稳健管理。CCA在真实软件工程任务中表现优异，SWE-Bench-Pro测试中Resolve@1指标达到54.3%，显著优于现有开源编码代理。该工作为AI编码代理提供了透明、可扩展且易复现的基础，推动了从研究原型到生产级系统的转化与应用。,2
DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance,[翻译]DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance,Other,Other,https://arxiv.org/pdf/2512.10894,https://huggingface.co/papers/2512.10894,无法生成摘要。,2
CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images,CheXmask-U：基于标志点的X射线图像解剖分割不确定性量化,Other,Other,https://arxiv.org/pdf/2512.10715,https://huggingface.co/papers/2512.10715,本文提出了一种基于混合神经网络架构的胸部X光解剖标志点分割不确定性量化方法，通过两个互补指标评估模型预测的可靠性。实验表明，不确定性指标能有效反映图像质量下降并识别不可靠预测，支持异常样本检测。作者还公开了包含65万余张带不确定性标注的胸部X光标志点分割大规模数据集CheXmask-U，为相关研究提供重要资源。该工作为提升基于标志点的医学图像分割的稳健性和临床安全性提供了新思路。,2
Scaling Behavior of Discrete Diffusion Language Models,[翻译]Scaling Behavior of Discrete Diffusion Language Models,Other,Other,https://arxiv.org/pdf/2512.10858,https://huggingface.co/papers/2512.10858,无法生成摘要。,2
START: Spatial and Textual Learning for Chart Understanding,START：用于图表理解的空间与文本学习,Multimodal LLM,Amazon,https://arxiv.org/pdf/2512.07186,https://huggingface.co/papers/2512.07186,本文提出了START，一种结合空间布局和文本信息的图表理解方法，提升了多模态大语言模型对图表的解析能力。通过引入图表元素定位和图表代码生成，START不仅恢复了图表背后的数据结构，还捕捉了图表的视觉布局特征。为支持训练和评估，作者构建了包含真实图表及其对应代码的数据集，并设计了专门的空间理解基准测试。实验结果表明，START在多个模型和任务上均显著优于现有方法，推动了图表理解技术的发展。相关代码和数据将公开发布。,2
LitePT: Lighter Yet Stronger Point Transformer,LitePT：更轻量且更强大的点云变换器,Other,Other,https://arxiv.org/pdf/2512.13689,https://huggingface.co/papers/2512.13689,本文提出了LitePT，一种高效且轻量的3D点云处理骨干网络。该模型基于对卷积和注意力机制在不同层次作用的分析，采用卷积处理早期高分辨率层以提取低级几何信息，深层则切换为注意力机制以捕捉高级语义和上下文。同时引入了一种无需训练的三维位置编码方法PointROPE，防止空间信息丢失。LitePT相比当前最先进的Point Transformer V3，参数更少、运行更快、内存占用更低，且在多个任务和数据集上表现相当或更优，展示了设计简洁且高效的3D点云模型的新方向。,2
Directional Textual Inversion for Personalized Text-to-Image Generation,用于个性化文本到图像生成的方向性文本反演,Diffusion Model,Other,https://arxiv.org/pdf/2512.13672,https://huggingface.co/papers/2512.13672,本文提出了方向性文本反演（Directional Textual Inversion，DTI）方法，解决了现有文本反演技术在复杂提示词下表现不佳的问题。DTI通过固定嵌入向量的大小，仅优化其方向，有效避免了嵌入范数膨胀带来的性能下降，提升了生成图像对文本提示的忠实度，同时保持了对个性化主体的相似度。该方法还支持在不同概念间进行平滑且语义连贯的插值，增强了个性化文本到图像生成的灵活性和稳定性。实验结果表明，DTI在多项个性化任务中优于传统方法，展示了其在高质量文本引导图像生成中的潜力。,2
Inferring Compositional 4D Scenes without Ever Seeing One,在未见过任何实例的情况下推断组合式4D场景,Other,Other,https://arxiv.org/pdf/2512.05272,https://huggingface.co/papers/2512.05272,本文提出了COM4D，一种能够从单目视频中联合推断多对象4D场景结构及其时空配置的方法。该方法无需使用任何4D组合训练数据，而是通过分别学习静态多对象组合和单一动态对象的空间与时间注意力机制，在推理阶段将二者融合，实现了对复杂动态场景的完整重建。COM4D不仅保证了空间和时间上的一致性，还在4D对象和组合3D重建任务上取得了领先的效果，展示了纯数据驱动方法在复杂场景理解中的潜力。,2
DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders,DiffusionBrowser：基于多分支解码器的交互式扩散预览,Diffusion Model,Other,https://arxiv.org/pdf/2512.13690,https://huggingface.co/papers/2512.13690,本文提出了DiffusionBrowser，一种轻量级且模型无关的解码器框架，能够在视频扩散生成的去噪过程中实时生成多模态预览（包括RGB图像和场景内在属性），速度超过实时4倍，大幅提升用户交互体验。该方法不仅允许用户在任意中间步骤查看生成效果，还支持通过随机性重新注入和模态引导实现对生成过程的动态控制。此外，DiffusionBrowser还能揭示扩散模型中场景和对象细节的逐步构建过程，增强了生成过程的透明度和可控性，推动了视频扩散模型的实用性和交互性发展。,2
Few-Step Distillation for Text-to-Image Generation: A Practical Guide,面向文本到图像生成的少步蒸馏：实用指南,Diffusion Model,"THU, Alibaba",https://arxiv.org/pdf/2512.13006,https://huggingface.co/papers/2512.13006,本文首次系统地研究了将扩散模型蒸馏技术应用于文本到图像生成任务，针对从固定类别标签到自由文本提示的转变，分析了现有方法面临的关键挑战。作者基于强大的教师模型FLUX.1-lite，提出统一框架并给出实用的输入调整、网络设计和参数设置指导，显著提升了生成速度和资源效率，同时保持图像质量。该工作为实际应用中快速、高质量的文本图像生成提供了坚实基础，并开源了相关代码与预训练模型。,2
Flowception: Temporally Expansive Flow Matching for Video Generation,Flowception：用于视频生成的时间扩展流匹配,Other,Meta,https://arxiv.org/pdf/2512.11438,https://huggingface.co/papers/2512.11438,本文提出了Flowception，一种新颖的视频生成框架，结合了非自回归的离散帧插入与连续帧去噪机制。该方法有效缓解了传统自回归模型中的误差累积问题，同时相比全序列生成显著降低了训练计算成本。Flowception支持变长视频生成，并能联合学习视频长度与内容，提升了生成质量和效率。实验结果表明，Flowception在多个评价指标上优于现有方法，且能够灵活应用于图像到视频生成及视频插帧等任务，展示了其广泛的实用潜力。,2
RePo: Language Models with Context Re-Positioning,RePo：具有上下文重新定位机制的大语言模型,LLM,Other,https://arxiv.org/pdf/2512.14391,https://huggingface.co/papers/2512.14391,本文提出了RePo，一种用于大型语言模型的新型上下文重定位机制。RePo通过可微分的方式动态分配词元位置，减少了传统固定线性位置编码带来的额外认知负担，从而更有效地利用有限的记忆资源。实验证明，RePo显著提升了模型在处理噪声多、结构复杂及长上下文任务中的表现，同时保持了对短上下文任务的竞争力。该方法有助于模型更好地捕捉远距离且相关的信息，增强了对输入上下文内在结构的理解。,2
Hybrid Attribution Priors for Explainable and Robust Model Training,用于可解释且鲁棒模型训练的混合归因先验,LLM,PKU,https://arxiv.org/pdf/2512.14719,https://huggingface.co/papers/2512.14719,本文提出了一种名为Class-Aware Attribution Prior（CAP）的新框架，旨在提升小型语言模型的可解释性和鲁棒性。通过引导模型捕捉细粒度的类别差异，CAP生成更具区分性的解释信息，克服了现有归因方法常聚焦于相似类别共有关键词的问题。基于此，作者进一步设计了CAP Hybrid，将CAP与传统归因方法结合，提供更全面的监督信号。大量实验表明，该方法在多种数据规模和对抗环境下均有效提升了模型的解释能力和抗干扰性能。,2
Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation,Make-It-Poseable：用于3D人形角色动画的前馈潜空间姿态模型,Other,Other,https://arxiv.org/pdf/2512.16767,https://huggingface.co/papers/2512.16767,本文提出了Make-It-Poseable，一种新颖的前馈式3D人物姿态生成框架。该方法通过在潜在空间中直接操作角色的隐含表示，实现高质量且鲁棒的姿态调整，克服了传统自动绑定方法在权重预测和拓扑变化处理上的不足。核心组件为潜在姿态转换器，结合密集姿态表示以精确控制角色动作。此外，设计了潜在空间监督和自适应补全模块，提升了几何细节和适应复杂变形能力。该方法不仅显著提升了姿态生成质量，还自然支持零件替换和细节优化等3D编辑应用，具备广泛实用价值。,2
Coupled Variational Reinforcement Learning for Language Model General Reasoning,用于语言模型通用推理的耦合变分强化学习,LLM,Other,https://arxiv.org/pdf/2512.12576,https://huggingface.co/papers/2512.12576,本文提出了一种名为CoVRL的混合方法，通过结合变分推断和强化学习，有效提升语言模型的推理能力。与现有仅基于问题采样推理路径的方法不同，CoVRL通过耦合先验和后验分布，实现了推理过程与答案信息的紧密结合，促进了更高效的探索和推理连贯性。实验证明，CoVRL在数学和通用推理任务上较基础模型提升了12.4%，并超越了现有先进的无验证器强化学习方法，展示了其在增强语言模型整体推理能力上的潜力和实用价值。,2
Improving Recursive Transformers with Mixture of LoRAs,利用Mixture of LoRAs改进递归Transformer,LLM,Other,https://arxiv.org/pdf/2512.12880,https://huggingface.co/papers/2512.12880,本文提出了一种名为Mixture of LoRAs（MoL）的轻量级机制，通过在递归变换器的共享前馈网络中引入条件低秩适配器，有效恢复了因参数共享导致的层间表达能力下降。基于此，作者设计了现代化递归架构ModernALBERT，结合多种先进技术，在多个自然语言理解和检索任务上实现了紧凑模型的最佳性能，甚至超越了更大规模的全参数模型。此外，提出的专家合并方法在推理阶段压缩模型，兼顾效率与准确性。该工作展示了条件计算在保持模型紧凑性的同时提升表达能力的有效性。,2
Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space,心智中的推理：潜在空间中的动态多模态交织,Multimodal LLM,Other,https://arxiv.org/pdf/2512.12623,https://huggingface.co/papers/2512.12623,本文提出了一种动态多模态潜在推理框架（DMLR），通过在潜在空间中交织视觉感知与语言推理，实现更自然高效的跨模态理解。该方法借鉴人类思维中感知与推理的动态交互，利用置信度引导的优化策略不断调整推理过程中的潜在表示，同时动态注入最相关的视觉信息，提升了模型的推理深度和视觉感知能力。实验结果表明，DMLR在多个多模态推理任务上显著优于现有方法，且保持了较高的推理效率，展示了其在跨模态智能理解中的潜力和实用价值。,2
MineTheGap: Automatic Mining of Biases in Text-to-Image Models,MineTheGap：文本到图像模型中偏见的自动挖掘方法,Diffusion Model,Other,https://arxiv.org/pdf/2512.13427,https://huggingface.co/papers/2512.13427,本文提出了MineTheGap，一种基于遗传算法的自动化方法，用于发现文本生成图像模型在处理模糊提示时产生的偏见。该方法通过一个新颖的偏见评分机制，评估并筛选出能暴露模型偏见的文本提示，促进对模型输出多样性和公平性的深入理解。实验验证了该方法在识别和量化偏见方面的有效性，有助于推动文本到图像生成技术在社会责任和用户体验方面的改进。,2
MatSpray: Fusing 2D Material World Knowledge on 3D Geometry,MatSpray：在三维几何体上融合二维材质世界知识,Diffusion Model,Other,https://arxiv.org/pdf/2512.18314,https://huggingface.co/papers/2512.18314,本文提出了MatSpray框架，通过结合二维扩散模型生成的材质信息与三维高斯点云重建技术，实现了高质量的三维材质融合。该方法首先从多视角图像中预测物体的物理材质参数（如颜色、粗糙度和金属度），然后将这些二维材质映射精确投射到三维几何结构上，最后通过轻量级神经网络进一步优化材质细节和多视角一致性。实验结果表明，该方法在材质表现和视觉真实感方面优于现有技术，显著提升了三维场景的可重光照性和真实感，为游戏和影视等内容制作流程带来更高效、精准的资产创建方案。,2
Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives,从形式与自然语言视角理解大语言模型中的三段论推理,LLM,Other,https://arxiv.org/pdf/2512.12620,https://huggingface.co/papers/2512.12620,本文研究了大型语言模型（LLMs）在三段论推理中的表现，从形式逻辑和自然语言理解两个角度出发。通过对14个不同模型的测试，作者发现三段论推理能力在模型间表现不一，部分模型在符号推理上达到完美表现，但整体上未必体现人类推理中的复杂性和偏见，如信念偏差效应。研究指出，尽管部分LLMs展现出较强的形式推理能力，但这并不意味着它们真正模拟了人类的思维过程，强调了未来对模型推理机制更深入理解的重要性。,2
Active Intelligence in Video Avatars via Closed-loop World Modeling,通过闭环世界建模实现视频虚拟形象中的主动智能,Agent,Other,https://arxiv.org/pdf/2512.20615,https://huggingface.co/papers/2512.20615,本文提出了ORCA，一种赋予视频虚拟形象主动智能的框架，能够在不确定环境中自主完成多步任务。通过引入闭环的观察-思考-行动-反思机制，ORCA持续校验预测结果以保持状态准确性；并采用双系统结构实现战略规划与具体动作转换。作者还设计了L-IVA任务与基准，用于评估虚拟形象的目标导向规划能力。实验表明，ORCA显著提升了任务成功率和行为连贯性，推动了视频虚拟形象从被动动画向主动、目标驱动的智能行为发展。,2
Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation,基于多LLM的主题分析与双重可靠性度量：结合Cohen's Kappa与语义相似性进行定性研究验证,LLM,Other,https://arxiv.org/pdf/2512.20352,https://huggingface.co/papers/2512.20352,本文提出了一种基于大型语言模型（LLM）的多视角主题分析验证框架，通过结合两种可靠性指标——Cohen’s Kappa衡量标注一致性和余弦相似度评估语义一致性，提升定性研究的可靠性。该框架支持多次独立运行、灵活参数配置及多样化数据格式，实现共识主题的自动提取。实验证明，三款主流LLM在心理艺术疗法访谈数据上的表现均达到高一致性，验证了多次运行集成方法的有效性。该开源工具为AI辅助的定性研究提供了透明、灵活且结构无关的可靠性评估方法，推动了该领域的方法学发展。,2
Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents,Memory-T1：用于多会话智能体时间推理的强化学习框架,Agent,Other,https://arxiv.org/pdf/2512.20092,https://huggingface.co/papers/2512.20092,本文提出了Memory-T1，一种基于强化学习的框架，专注于多会话对话中的时间推理问题。通过先粗选再精炼的策略，Memory-T1有效筛选与时间相关的对话内容，并通过多层奖励机制优化答案准确性、证据匹配和时间一致性，解决了长对话历史中信息噪声和时间顺序混淆的问题。在Time-Dialog基准测试中，Memory-T1显著提升了模型性能，刷新了开源模型的最佳记录，并在处理超长对话时表现出更强的鲁棒性，体现了其在复杂多轮对话时间推理中的实用价值。,2
"A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication",用于通用3×3矩阵乘法的58加法、秩23方案,Other,Other,https://arxiv.org/pdf/2512.21980,https://huggingface.co/papers/2512.21980,"本文提出了一种用于通用非交换环上3×3矩阵乘法的新算法，实现了秩为23且仅需58次标量加法的方案，较此前最佳的60次加法有所提升。该方法通过自动化搜索结合特定图结构探索和贪心策略，成功消除了公共子表达式，且仅使用{-1,0,1}的系数，保证了算法的高效性和跨域适用性。整体标量运算次数也从83降至81，显著提升了小矩阵乘法的计算效率。",2
Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards,基于可验证奖励的强化学习中样本极性的再思考,LLM,Other,https://arxiv.org/pdf/2512.21625,https://huggingface.co/papers/2512.21625,本文系统研究了强化学习中正负样本（样本极性）对训练大规模推理模型的影响。作者发现，正样本有助于强化已有的正确推理路径，而负样本则促进模型探索新的推理方式。基于这一观察，提出了一种自适应且非对称的优势信号分配方法（A3PO），能够更精细地在不同极性的关键位置分配奖励信号。实验证明，该方法在多个推理任务上显著提升了模型性能，推动了强化学习在复杂推理能力提升中的应用。,2
Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis,引入TrGLUE和SentiTurca：土耳其语通用语言理解与情感分析的综合基准,Other,Other,https://arxiv.org/pdf/2512.22100,https://huggingface.co/papers/2512.22100,本文提出了TrGLUE和SentiTurca两个面向土耳其语的自然语言理解（NLU）和情感分析综合基准。TrGLUE涵盖多种NLU任务，基于土耳其语原生语料，采用结合大型语言模型自动标注与人工验证的半自动标注流程，保证数据的语言自然性和高质量。SentiTurca则专注于情感分析任务。作者还提供了基于变换器模型的微调与评测代码，旨在填补土耳其语NLP评测工具的空白，推动相关研究的发展。,2
KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta,KernelEvolve：面向Meta异构AI加速器的智能核编码扩展,Agent,Meta,https://arxiv.org/pdf/2512.23236,https://huggingface.co/papers/2512.23236,本文提出了KernelEvolve，一个自动化内核生成与优化框架，旨在解决深度学习推荐模型在多样化硬件架构上的高效训练与推理问题。该系统通过多层编程抽象和动态搜索策略，支持跨NVIDIA、AMD GPU及Meta自研AI加速器的异构硬件，显著缩短开发周期并提升性能，最高达传统方法的17倍。KernelEvolve不仅保证了算法的正确性和广泛适用性，还降低了新硬件的编程门槛，推动了大规模异构AI系统的高效部署与优化。,2
GraphLocator: Graph-guided Causal Reasoning for Issue Localization,GraphLocator：基于图的因果推理用于问题定位,Other,"ByteDance, PKU",https://arxiv.org/pdf/2512.22469,https://huggingface.co/papers/2512.22469,本文针对软件缺陷定位中描述与代码之间存在的语义差异问题，提出了一种基于因果推理的定位方法GraphLocator。该方法通过构建因果问题图，有效识别问题描述中的症状及其潜在原因，并动态拆解复杂问题对应的多个代码实体。实验结果表明，GraphLocator在多个真实数据集上显著提升了定位准确率和召回率，尤其在解决症状与根因不匹配及一对多代码实体关联问题上表现优异，提升了后续缺陷修复的效率和效果。该研究为自动化软件维护提供了新的思路和工具。,2
TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems,TESO：针对噪声黑箱问题的禁忌增强仿真优化,Other,Other,https://arxiv.org/pdf/2512.24007,https://huggingface.co/papers/2512.24007,本文提出了一种名为TESO的仿真优化新方法，专为处理带噪声、计算代价高且搜索空间复杂的黑箱问题设计。TESO结合了短期禁忌列表和长期精英记忆机制，有效避免搜索陷入循环并促进多样化，同时通过扰动优质解实现局部强化。该方法还引入了允许突破禁忌限制的准则，以平衡探索与利用。通过排队优化实例验证，TESO在性能和稳定性上优于现有基准方法，展示了其在复杂随机环境中优化的潜力和实用价值。,2
SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving,SWE-Lego：推动软件问题解决监督微调的极限,LLM,Other,https://arxiv.org/pdf/2601.01426,https://huggingface.co/papers/2601.01426,本文提出了SWE-Lego，一种专注于软件问题解决的监督微调方法，通过构建包含3.2万高质量任务实例和1.8万验证轨迹的混合数据集，结合改进的训练策略（包括错误屏蔽和难度分级课程），显著提升了模型的性能。实验表明，SWE-Lego在开源模型中实现了领先表现，且通过测试时扩展技术进一步增强了效果。该方法以轻量级的微调流程，推动了软件工程任务自动解决的性能极限，展示了高效且实用的模型优化路径。,2
Pearmut: Human Evaluation of Translation Made Trivial,Pearmut：简化翻译人类评估的平台,Other,Other,https://arxiv.org/pdf/2601.02933,https://huggingface.co/papers/2601.02933,本文介绍了Pearmut平台，一种简化多语言自然语言处理（NLP）中人工翻译评估的工具。Pearmut通过支持多种标准评估协议和灵活的学习策略，降低了人工评估的技术门槛，使其操作流程与自动评估同样便捷。平台特别针对机器翻译任务设计，支持文档级上下文、不同评估方式及质量控制机制，促进了可靠且高效的人类评估在模型开发中的常态化应用，解决了传统人工评估复杂且耗时的问题。,2
"Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",Guardians of the Hair：在深度、立体视觉与新视角中恢复软边界细节,Other,Other,https://arxiv.org/pdf/2601.03362,https://huggingface.co/papers/2601.03362,本文提出了HairGuard，一种针对3D视觉任务中细腻软边界（如头发）细节恢复的框架。通过利用图像抠图数据训练，设计深度修正网络精确识别并优化软边界深度，同时结合深度前向变形和生成填充技术，HairGuard有效保留高质量纹理并消除背景干扰。该方法可与现有深度模型无缝集成，显著提升单目深度估计、立体图像转换和新视角合成中软边界的表现，推动了3D视觉在细节复原方面的进步。,2
Memorization in 3D Shape Generation: An Empirical Study,3D形状生成中的记忆现象：一项实证研究,Diffusion Model,Other,https://arxiv.org/pdf/2512.23628,https://huggingface.co/papers/2512.23628,本文提出了一种评估框架，用于量化三维形状生成模型中的记忆现象，并系统研究了数据特性和模型设计对记忆程度的影响。通过对现有方法的分析和基于潜在向量集扩散模型的实验，发现记忆受数据模态、多样性及条件细粒度的影响，同时模型的指导强度和数据增强策略也能调节记忆水平。该研究不仅揭示了三维生成模型记忆行为的规律，还提出了有效降低记忆、提升生成多样性的简单方法，为防止训练数据泄露和提升模型泛化能力提供了实证支持。,2
ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting,ProFuse：用于开放词汇3D高斯点溅射的高效跨视图上下文融合,Other,Other,https://arxiv.org/pdf/2601.04754,https://huggingface.co/papers/2601.04754,本文提出了ProFuse，一种高效的开放词汇3D场景理解框架，基于3D高斯斑点技术实现跨视角语义融合。通过引入密集对应引导的预注册阶段，ProFuse准确初始化场景几何结构，并通过跨视角聚类构建3D上下文提案，实现全局语义特征的融合。该方法无需额外渲染监督或复杂优化，显著提升了语义一致性和几何精度，且语义附加速度比现有方法快两倍，展示了在开放词汇3D理解任务中的优越性能和实用价值。,2
Multi-Scale Local Speculative Decoding for Image Generation,用于图像生成的多尺度局部推测解码,Other,Other,https://arxiv.org/pdf/2601.05149,https://huggingface.co/papers/2601.05149,本文提出了一种名为多尺度局部推测解码（MuLo-SD）的新方法，用于加速自回归图像生成。该方法通过低分辨率草稿模型结合学习型上采样器生成候选图像片段，再由高分辨率目标模型并行验证，并引入局部拒绝与重采样机制，聚焦空间邻域高效修正错误。实验表明，MuLo-SD在保持图像语义一致性和视觉质量的同时，实现了最高1.7倍的加速，优于现有主流推测解码技术，显著提升了生成效率与效果的平衡。,2
PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference,PyramidalWan：将预训练视频模型构建为金字塔结构以实现高效推理,Diffusion Model,Other,https://arxiv.org/pdf/2601.04792,https://huggingface.co/papers/2601.04792,本文提出了一种将预训练视频扩散模型转化为金字塔结构的方法，通过低成本微调实现多分辨率分阶段处理，有效降低推理计算量，同时保持生成视频的质量不受影响。该方法克服了现有从零训练的金字塔视频模型视觉效果不足的问题。此外，论文还探索了多种步骤蒸馏策略，进一步提升推理效率。实验结果表明，该方案在计算资源和推理速度上均有显著优势，为高效视频生成提供了实用路径。,2
ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers,ReHyAt：用于视频扩散Transformer的递归混合注意力机制,Diffusion Model,Other,https://arxiv.org/pdf/2601.04342,https://huggingface.co/papers/2601.04342,本文提出了ReHyAt，一种结合软max注意力高精度与线性注意力高效性的递归混合注意力机制，显著降低了视频生成模型的计算复杂度，实现了内存使用的常数级别。与现有方法相比，ReHyAt在保持视频生成质量的同时，将训练成本降低了约100倍，提升了长视频序列的可扩展性和实际应用潜力。实验结果及用户偏好研究表明，ReHyAt在视频质量和计算效率上均达到领先水平，为未来高效视频生成模型提供了有效方案。,2
Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset,面向开放词汇工业缺陷理解的大规模多模态数据集研究,Multimodal LLM,Other,https://arxiv.org/pdf/2512.24160,https://huggingface.co/papers/2512.24160,本文提出了IMDD-1M，这是首个包含100万对工业缺陷图像与文本描述的大规模多模态数据集，涵盖60余种材料和400多种缺陷类型，配有专家验证的详细注释。基于该数据集，作者训练了专为工业场景设计的视觉-语言基础模型，具备多任务处理能力且能通过少量数据实现高效微调。该工作显著提升了工业缺陷检测和生成的智能化水平，为制造质量检验提供了更具适应性和解释性的解决方案，推动了制造业智能检测技术的发展。,2
Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing,Scaling Behavior Cloning提升因果推理能力：用于实时视频游戏玩法的开放模型,Agent,Other,https://arxiv.org/pdf/2601.04575,https://huggingface.co/papers/2601.04575,本文提出了一种基于行为克隆的开放式训练方法，通过大规模人类游戏数据和深度模型的联合扩展，实现了实时运行于消费级GPU上的3D视频游戏智能体。研究系统地分析了模型规模和训练数据量对游戏表现及因果推理能力的影响，发现增加模型深度和数据量能显著提升因果推理效果。所发布的数据集、代码和预训练模型为后续研究提供了宝贵资源，展示了行为克隆在复杂游戏环境中达到人类水平的潜力。,2
Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes,超越二元偏好：通过属性解耦将扩散模型对齐至细粒度标准,Diffusion Model,Other,https://arxiv.org/pdf/2601.04300,https://huggingface.co/papers/2601.04300,本文提出了一种基于细粒度层级评价标准的扩散模型对齐方法，突破了传统依赖简单二元偏好的限制。作者首先与领域专家合作构建了包含多个正负属性的层级评价体系，然后设计了两阶段对齐框架：通过监督微调注入领域知识到辅助模型，再利用复杂偏好优化同时提升正面属性概率并抑制负面属性概率。以绘画生成为例，实验表明该方法显著提升了生成质量和与专家标准的一致性，为细粒度标准下的模型对齐开辟了新路径。,2
GenCtrl -- A Formal Controllability Toolkit for Generative Models,GenCtrl——生成模型的形式可控性工具包,LLM,Other,https://arxiv.org/pdf/2601.05637,https://huggingface.co/papers/2601.05637,本文提出了GenCtrl，一个用于生成模型可控性分析的理论框架和工具包。通过将人机交互视为控制过程，作者设计了一个算法来估计模型在对话中的可控输出集合，并给出无分布假设下的误差界限保证。实验结果表明，生成模型的可控性较为脆弱且高度依赖具体情境，揭示了当前控制方法的局限性。该工作强调在尝试控制生成模型之前，需深入理解其可控性的基本限制，为生成模型的可靠应用提供了理论支撑和实用工具。,2
TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration,TCAndon-Router：用于多智能体协作的自适应推理路由器,Agent,Tencent,https://arxiv.org/pdf/2601.04544,https://huggingface.co/papers/2601.04544,本文提出了TCAndon-Router，一种适用于多智能体系统的自适应路由方法。该方法通过动态引入新智能体和生成自然语言推理链，有效解决了传统路由中难以扩展和智能体职责重叠导致的冲突问题。TCAndon-Router支持多智能体协同工作，独立生成答案后由专门的优化智能体整合，显著提升了路由准确率和系统鲁棒性。实验结果表明，该方法在公开和企业数据上均表现优异，推动了多智能体协作路由的可解释性和实用性研究。,2
Codified Foreshadowing-Payoff Text Generation,编码化的伏笔-回报文本生成,LLM,Other,https://arxiv.org/pdf/2601.07033,https://huggingface.co/papers/2601.07033,本文提出了一种名为CFPG的新框架，旨在解决大型语言模型在故事生成中难以实现前期铺垫与后续事件逻辑衔接的问题。通过将故事中的伏笔、触发条件和结果转化为可执行的因果关系，CFPG确保故事承诺不仅被提及，还能在时间和逻辑上得到合理兑现。实验表明，该方法在实现故事连贯性和情节完成度方面显著优于传统生成方式，推动了语言模型从表面流畅向真实叙事能力的提升。,2
"""TODO: Fix the Mess Gemini Created"": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",“待办事项：修复Gemini造成的混乱”：迈向理解由生成式AI引发的自我承认技术债务,LLM,Other,https://arxiv.org/pdf/2601.07786,https://huggingface.co/papers/2601.07786,本文通过分析公开代码库中引用大型语言模型（LLM）生成代码的注释，揭示了开发者在使用生成式AI辅助编程时，常自觉留下技术债务的证据。研究发现，延期测试、不完全适配和对AI代码理解不足是导致此类技术债务的主要原因。论文提出“生成式AI诱发的自认技术债务”（GIST）概念，帮助理解开发者在引入AI代码时对其正确性和行为表达不确定性的普遍现象，为未来改进AI辅助开发工具和实践提供了重要视角。,2
ShowUI-Aloha: Human-Taught GUI Agent,ShowUI-Aloha：人类指导的图形用户界面智能体,Agent,Other,https://arxiv.org/pdf/2601.07181,https://huggingface.co/papers/2601.07181,本文提出了ShowUI-Aloha，一种将无结构的人类屏幕录制转化为结构化GUI任务的完整系统。该系统通过录制用户操作、语义理解、任务规划和执行四个环节，实现了从观察人类演示到自动执行复杂桌面操作的闭环。ShowUI-Aloha有效解决了传统训练数据稀缺和演示数据无标注的问题，提升了自动化GUI任务的成功率，展示了基于人类示范构建通用GUI智能代理的可行路径，推动了人机交互自动化的发展。,2
Sci-Reasoning: A Dataset Decoding AI Innovation Patterns,Sci-Reasoning：解码人工智能创新模式的数据集,Agent,Other,https://arxiv.org/pdf/2601.04577,https://huggingface.co/papers/2601.04577,本文提出了Sci-Reasoning数据集，首次系统捕捉了人工智能领域高质量研究成果背后的思维模式和创新路径。通过结合社区验证和人工智能辅助的审核流程，数据集追踪了顶级会议论文与其关键前驱工作的具体推理关系，识别出15种主要的思考模式，其中三种策略占据半数以上。该数据集不仅揭示了科学创新的内在逻辑，也为定量分析科研进展和训练未来智能科研助手提供了重要资源。,2
How Do Large Language Models Learn Concepts During Continual Pre-Training?,大语言模型如何在持续预训练过程中学习概念？,LLM,Meta,https://arxiv.org/pdf/2601.03570,https://huggingface.co/papers/2601.03570,本文研究了大型语言模型在持续预训练过程中如何学习、保持和遗忘概念。通过分析模型内部与具体概念相关的计算子结构，作者发现概念的学习呈现阶段性变化，早期快速增长后逐渐稳定。同时，学习进步较大的概念更容易被遗忘，语义相近的概念之间干扰更强，不同概念间的知识转移能力存在差异。该研究从电路层面揭示了概念学习的动态机制，为设计更具解释性和鲁棒性的概念感知训练方法提供了理论支持。,2
"Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",文本、代码与视觉的对齐：一种用于文本到可视化的多目标强化学习框架,Multimodal LLM,Other,https://arxiv.org/pdf/2601.04582,https://huggingface.co/papers/2601.04582,本文提出了RL-Text2Vis，一种基于强化学习的文本到可视化生成框架，通过联合优化文本准确性、代码执行有效性和图表质量，显著提升了生成图表的语义对齐和清晰度。该方法采用多目标奖励机制，利用执行后反馈指导模型训练，在多个基准测试中优于现有闭源和开源模型，实现了代码执行成功率和图表质量的大幅提升。研究展示了该框架在跨领域数据集上的良好泛化能力，为自动生成高质量数据可视化提供了有效解决方案。代码已公开。,2
Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking,面向大语言模型事实核查的全面阶段性基准测试,LLM,Other,https://arxiv.org/pdf/2601.02669,https://huggingface.co/papers/2601.02669,本文提出了FactArena，一个自动化评估框架，全面测试大型语言模型在事实核查全过程中的表现，涵盖了从信息提取、证据检索到最终判断的各个环节。通过引入分阶段评测、统一的评判机制以及动态生成更具挑战性的测试样本，FactArena揭示了现有模型在整体事实核查能力上与单纯声明验证准确率之间的显著差距。该框架为深入理解和提升语言模型的事实推理能力提供了可靠工具，推动其在安全关键的事实核查应用中的可信部署。,2
DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing,DPWriter：基于多样化规划分支的强化学习用于创意写作,LLM,Other,https://arxiv.org/pdf/2601.09609,https://huggingface.co/papers/2601.09609,本文提出了一种基于强化学习的新框架DPWriter，旨在提升大型语言模型在创意写作中的内容多样性。该方法通过将生成过程分解为有计划的中间步骤，引入多样性规划分支策略和群体感知多样性奖励，有效促进模型在规划阶段产生更多不同的生成路径。实验结果表明，DPWriter在保持生成质量的同时，显著增强了文本输出的多样性，优于现有方法，提升了创意写作任务中模型的实用性和表现。,2
CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents,CaMeLs也能使用计算机：面向计算机使用智能体的系统级安全,Agent,Other,https://arxiv.org/pdf/2601.09923,https://huggingface.co/papers/2601.09923,本文针对自动化操作计算机界面的智能代理面临的安全威胁，提出了一种新颖的系统设计——单次规划（Single-Shot Planning）。该方法通过在接触潜在恶意内容前，由可信规划器生成完整的执行流程图，确保任务执行的控制流完整性，有效防止恶意指令注入攻击。尽管如此，论文指出仍需额外措施防范通过界面元素操控执行路径的攻击。实验证明，该设计在保障安全的同时，兼顾了性能表现，展示了智能计算机使用代理在安全与实用性之间的良好平衡。,2
From FLOPs to Footprints: The Resource Cost of Artificial Intelligence,从FLOPs到足迹：人工智能的资源成本,Other,Other,https://arxiv.org/pdf/2512.04142,https://huggingface.co/papers/2512.04142,本研究首次系统量化了人工智能训练过程中的物质资源消耗，聚焦于Nvidia A100 GPU的元素组成及其环境影响。通过分析GPU的材料构成及不同使用效率和寿命下的计算需求，揭示训练大型模型（如GPT-4）可能导致数吨有害金属的开采与废弃。研究表明，提高硬件利用率和延长使用寿命能显著减少资源消耗，强调了软硬件优化在降低环境负担中的重要性。论文呼吁将物质资源成本纳入AI可持续发展的讨论，推动技术进步与环境责任的协调发展。,1
Vector Quantization using Gaussian Variational Autoencoder,基于高斯变分自编码器的向量量化,Other,THU,https://arxiv.org/pdf/2512.06609,https://huggingface.co/papers/2512.06609,本文提出了一种名为Gaussian Quant（GQ）的新方法，实现了将带有特定约束的高斯变分自编码器（Gaussian VAE）无训练转换为向量量化变分自编码器（VQ-VAE）。GQ通过构建高斯噪声码本，并选择最接近后验均值的码本项，避免了传统VQ-VAE训练中的困难和不稳定问题。理论分析表明，当码本规模满足一定条件时，量化误差可控。实验结果显示，GQ在多种网络架构上超越了现有的VQ-VAE和高斯VAE离散化方法，且引入的目标散度约束（TDC）进一步提升了性能。该方法简化了离散表示学习，具有较高的实用价值。,1
DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue,DZ-TDPO：用于长上下文对话中可变状态跟踪的非破坏性时间对齐,LLM,Other,https://arxiv.org/pdf/2512.03704,https://huggingface.co/papers/2512.03704,本文提出了DZ-TDPO框架，针对长对话系统中用户意图随时间变化而与历史信息冲突的问题，设计了一种非破坏性的状态对齐方法。该方法结合动态约束和时间注意力机制，有效缓解了模型对过往信息的过度依赖，实现了更准确的状态更新。实验表明，DZ-TDPO在多轮对话任务中显著提升了性能，且具备良好的零样本泛化能力。此外，研究还揭示了模型规模与稳定性之间的权衡，验证了通过精细调控注意力权重而非破坏性参数更新，可以保持模型的整体能力。,1
Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games,Small-Gain Nash：可认证的微分博弈中纳什均衡的收缩性,Other,Other,https://arxiv.org/pdf/2512.06791,https://huggingface.co/papers/2512.06791,本论文提出了Small-Gain Nash（SGN）条件，通过设计一种加权块度量，将传统基于欧几里得几何的伪梯度单调性要求推广到更广泛的游戏场景。SGN利用局部曲率和玩家间耦合的界限，构建了一个收缩映射的几何框架，保证了梯度动态在该度量下的收敛性。该方法不仅提供了连续流的指数收敛保证，还推导出明确的步长限制，支持离散时间算法稳定收敛。实验证明，SGN能有效解决传统方法无法预测收敛的情况，并扩展至熵正则化策略梯度等复杂环境，提供了一套结构化、可计算的收敛性认证工具。,1
Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks,使用算子网络预测复杂几何形状上的时变流动,AI4Science,Other,https://arxiv.org/pdf/2512.04434,https://huggingface.co/papers/2512.04434,本文提出了一种基于深度算子网络的时变流场预测方法，能够高效准确地模拟复杂几何形状周围的非定常流动。该模型通过编码几何形状和流动历史，实现了对速度场的快速预测，单步误差约为5%，计算速度相比传统数值模拟提升了1000倍。研究还通过物理指标评估了模型的长期预测性能，分析了误差来源及解决方案。该方法为复杂流动的快速仿真和工程设计提供了有效工具，相关代码已公开，支持复现和后续研究。,1
SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos,SAM-Body4D：无需训练的视频中4D人体网格恢复框架,Other,Other,https://arxiv.org/pdf/2512.08406,https://huggingface.co/papers/2512.08406,本文提出了SAM-Body4D，一种无需额外训练即可从视频中恢复3D人体网格的框架。该方法通过生成和优化连续且身份一致的分割区域，有效解决了传统逐帧处理导致的时间不连贯和遮挡问题。利用改进的遮挡恢复模块，SAM-Body4D能够补全缺失部分，生成稳定且完整的全身三维网格轨迹。此外，设计的并行策略支持多人体高效处理。实验结果表明，该方法在复杂自然场景下表现出更好的时间稳定性和鲁棒性，推动了视频中人体三维重建的实用性和准确性。,1
MemLoRA: Distilling Expert Adapters for On-Device Memory Systems,MemLoRA：面向设备端存储系统的专家适配器蒸馏,Multimodal LLM,Other,https://arxiv.org/pdf/2512.04763,https://huggingface.co/papers/2512.04763,本文提出了MemLoRA及其视觉扩展MemLoRA-V，旨在通过专门设计的记忆适配器，提升小型语言模型和视觉语言模型在本地设备上的记忆能力和性能。该方法使模型能够高效执行知识提取、记忆更新和基于记忆的生成任务，避免依赖云端计算。实验证明，MemLoRA在文本任务上超越了规模远大的基线模型，MemLoRA-V在多模态视觉问答任务中表现显著优于传统基于图像描述的方法，展示了其在隐私保护和多模态理解场景中的应用潜力。,1
"Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation",Terrain Diffusion：基于扩散模型的Perlin噪声继任者用于无限实时地形生成,Diffusion Model,Other,https://arxiv.org/pdf/2512.08309,https://huggingface.co/papers/2512.08309,本文提出了Terrain Diffusion，一种基于扩散模型的新方法，用于实时生成无缝且无限扩展的虚拟地形。该方法通过创新的InfiniteDiffusion算法，实现了在保持种子一致性和常数时间随机访问的同时，生成具备大尺度地理结构和细节层次的逼真地形。结合分层模型和紧凑编码，Terrain Diffusion克服了传统程序噪声在真实性和大范围连贯性上的局限，支持高效且可控的整个星球级别地形合成，推动了程序化世界生成技术的发展。,1
SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images,SegEarth-OV3：探索SAM 3在遥感图像开放词汇语义分割中的应用,Other,Other,https://arxiv.org/pdf/2512.08730,https://huggingface.co/papers/2512.08730,本文首次尝试将Segment Anything Model 3（SAM 3）应用于遥感图像的开放词汇语义分割任务，且无需额外训练。通过融合SAM 3中语义分割头和实例头的结果，提升了地物覆盖的准确性；同时利用存在性评分过滤无关类别，有效减少误报。实验结果表明，该方法在多个遥感数据集上表现出良好性能，展示了SAM 3在遥感开放词汇语义分割中的潜力。相关代码已公开，便于后续研究和应用。,1
Pay Less Attention to Function Words for Free Robustness of Vision-Language Models,减少对功能词的关注以提升视觉-语言模型的免费鲁棒性,Multimodal LLM,Other,https://arxiv.org/pdf/2512.07222,https://huggingface.co/papers/2512.07222,本文提出了一种名为“功能词去注意”（FDA）的方法，以提升视觉-语言模型在面对跨模态对抗攻击时的鲁棒性。研究发现，功能词（如“是”、“的”等）容易使模型受到攻击影响，通过在注意力机制中减弱功能词的权重，FDA有效降低了攻击成功率，同时几乎不影响模型的正常性能。大量实验验证了该方法在多种任务、数据集和模型上的有效性和泛化能力。该工作为提升视觉-语言模型的安全性提供了一种简单且高效的解决方案。,1
Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication,临床对话的重塑：基于大语言模型的智能体范式在医疗交流中的应用,Agent,Other,https://arxiv.org/pdf/2512.01453,https://huggingface.co/papers/2512.01453,本论文探讨了医疗人工智能从传统生成式文本模型向具备自主决策能力的智能体架构的转变，重点分析了临床对话中既需自然交流的同理心，又需医学依据的严谨性。作者提出了一种基于知识来源和智能目标的分类框架，将现有方法归纳为四种典型模式，系统解析了各模式在规划、记忆、执行及协作等环节的设计权衡。研究不仅深化了对医疗AI认知结构的理解，还为未来构建安全、可靠且具备伦理保障的智能医疗系统提供了理论指导和实践参考。,1
GimbalDiffusion: Gravity-Aware Camera Control for Video Generation,GimbalDiffusion：面向视频生成的重力感知相机控制,Diffusion Model,Other,https://arxiv.org/pdf/2512.09112,https://huggingface.co/papers/2512.09112,本文提出了GimbalDiffusion框架，实现了基于重力方向的绝对坐标系下的精确摄像机控制，提升了文本驱动视频生成中摄像机运动的可控性和鲁棒性。该方法利用全景360度视频构建多样化摄像机轨迹，突破了传统视频多为直线前进视角的限制。通过引入“零俯仰”条件策略，减少了文本内容与摄像机参数间的冲突影响。最后，作者还建立了包含多样俯仰角度的评测基准，推动了摄像机感知视频生成技术的发展。整体上，GimbalDiffusion显著增强了生成视频中摄像机运动的精细控制能力。,1
Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction,矿业智能时机：用于比特币硬件投资回报率预测的深度学习框架,Other,Other,https://arxiv.org/pdf/2512.05402,https://huggingface.co/papers/2512.05402,本文提出了MineROI-Net，一种基于Transformer的深度学习模型，用于预测比特币ASIC矿机的投资回报率，帮助矿工判断购买时机。该模型将硬件采购问题视为时间序列分类任务，预测一年内投资是否盈利、边际盈利或亏损。通过分析2015至2024年20种矿机的数据，MineROI-Net在准确率和经济效益上均优于传统方法，能够有效识别盈利与亏损周期，降低矿业投资风险。该工具为资本密集型矿业提供了实用的数据驱动决策支持。,1
MOA: Multi-Objective Alignment for Role-Playing Agents,MOA：面向角色扮演智能体的多目标对齐方法,Agent,"THU, Alibaba",https://arxiv.org/pdf/2512.09756,https://huggingface.co/papers/2512.09756,本文提出了MOA（一种多目标对齐的强化学习框架），旨在提升角色扮演智能体在多维度技能上的表现。MOA通过同时优化多项细化评价指标，有效解决了传统监督学习易过拟合且多样性不足的问题，同时引入思维增强的策略提升生成内容的质量与多样性。实验证明，基于MOA的模型在多个复杂对话和角色扮演任务中表现优异，甚至超越了现有强基线方法，展示了其在构建具备丰富角色知识和风格一致性的智能对话系统中的广阔应用前景。,1
The N-Body Problem: Parallel Execution from Single-Person Egocentric Video,N体问题：基于单人第一视角视频的并行执行,Embodied AI,Other,https://arxiv.org/pdf/2512.11393,https://huggingface.co/papers/2512.11393,本文提出了“N体问题”，旨在从单人第一视角视频中预测多个人协同完成同一任务的并行执行方案。通过设计评估指标，综合考虑任务加速、覆盖率以及空间碰撞、物体冲突和因果关系等现实约束，作者引入了一种结构化提示策略，指导视觉语言模型推理三维环境和时间依赖，生成合理的多主体任务分配。在EPIC-Kitchens和HD-EPIC数据集上的实验表明，该方法在两人并行情况下显著提升了任务覆盖率并大幅降低冲突，展示了从单人视频中有效学习并行执行的潜力。,1
Sharp Monocular View Synthesis in Less Than a Second,[翻译]Sharp Monocular View Synthesis in Less Than a Second,Other,Other,https://arxiv.org/pdf/2512.10685,https://huggingface.co/papers/2512.10685,无法生成摘要。,1
Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching,Fast-FoundationStereo：实时零样本立体匹配,Other,Other,https://arxiv.org/pdf/2512.11130,https://huggingface.co/papers/2512.11130,本文提出Fast-FoundationStereo，一种实现实时零样本立体匹配的新型架构。该方法通过知识蒸馏、分块神经架构搜索和结构化剪枝，有效压缩模型体积并提升推理速度，同时保持了较强的泛化能力。作者还引入自动伪标签生成，利用大量真实场景立体图像辅助训练。实验表明，Fast-FoundationStereo在保证准确度的前提下，推理速度较现有基础模型提升了10倍以上，首次实现了实时性能与零样本泛化的平衡，推动了立体匹配技术在实际应用中的发展。,1
Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit,基于稀疏自编码器的可解释嵌入：一种数据分析工具包,LLM,Other,https://arxiv.org/pdf/2512.10092,https://huggingface.co/papers/2512.10092,本文提出利用稀疏自编码器（SAE）构建可解释的文本嵌入表示，实现对大规模文本数据的高效分析。相比传统依赖大型语言模型（LLM）标注或密集嵌入方法，SAE嵌入不仅成本更低、结果更可靠，还能通过维度对应具体语义概念，增强对数据属性的控制。作者通过多项任务验证了该方法在揭示数据集语义差异、识别偏见、文档聚类及基于属性的检索中的优势，展示了SAE作为一种灵活且经济的数据分析工具的重要价值。,1
Learning Robot Manipulation from Audio World Models,基于音频世界模型的机器人操作学习,Embodied AI,Other,https://arxiv.org/pdf/2512.08405,https://huggingface.co/papers/2512.08405,本文提出了一种生成式潜在流匹配模型，用于预测机器人操作任务中的未来音频信息。该模型通过捕捉音频中的内在节奏模式，使机器人能够基于音频的时间演变推断长期后果，从而提升操作性能。实验表明，在需要感知自然环境音频或音乐信号的操作任务中，该方法优于不考虑未来音频预测的传统方法。研究强调，准确预测未来音频状态对于多模态机器人学习尤为关键，尤其在视觉信息不足时，音频成为重要的感知依据。,1
Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries,基于细粒度分类的鱼类视觉重识别方法及其在渔业电子监控中的应用,Other,Other,https://arxiv.org/pdf/2512.08400,https://huggingface.co/papers/2512.08400,本文针对渔业电子监控系统中鱼类视频数据量大、人工审核困难的问题，提出了一种基于深度学习的自动鱼类重识别方法。利用模拟传送带场景的AutoFish数据集，结合特定图像预处理和难样本挖掘技术，显著提升了鱼类个体识别的准确率。研究表明，基于视觉变换器的Swin-T模型优于传统卷积网络，达到了90.43%的准确率。分析指出，同种鱼个体间的外观相似性及视角变化是主要挑战。该方法为渔业资源管理提供了高效、自动化的数据支持。,1
FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models,FIN-bench-v2：用于评估芬兰语大型语言模型的统一且稳健的基准套件,LLM,Other,https://arxiv.org/pdf/2512.13330,https://huggingface.co/papers/2512.13330,本文介绍了FIN-bench-v2，一个统一且稳健的芬兰语大型语言模型评测套件。该套件整合了多项广泛使用的芬兰语基准测试和原有FIN-bench的扩展版本，涵盖阅读理解、常识推理、情感分析等多种任务，支持多选和生成式评测。通过预训练2.15亿参数模型并分析其学习曲线，筛选出表现稳定且区分度高的任务，同时对机器翻译数据进行了人工校验。所有数据和评测工具均公开，促进芬兰语模型的标准化评估与发展。,1
State over Tokens: Characterizing the Role of Reasoning Tokens,State over Tokens：推理Token角色的特征化研究,LLM,Other,https://arxiv.org/pdf/2512.12777,https://huggingface.co/papers/2512.12777,本文提出了“State over Tokens”（SoT）框架，重新定义大型语言模型生成的推理过程中的中间文本（推理标记）为一种计算状态，而非字面上的语言叙述。研究表明，这些推理标记虽然改善了模型在复杂任务中的表现，但并不能真实反映模型的内部推理过程。SoT框架强调应将推理标记视为模型在无状态生成过程中持续携带的信息载体，从而引导未来研究从解读文本表面转向解析其背后的计算状态，推动对语言模型推理机制的更深入理解。,1
AutoMV: An Automatic Multi-Agent System for Music Video Generation,AutoMV：一种用于音乐视频生成的自动多智能体系统,Agent,Other,https://arxiv.org/pdf/2512.12196,https://huggingface.co/papers/2512.12196,本文提出了AutoMV，一种自动多智能体系统，能够直接从完整歌曲生成连贯且与音乐结构、节奏及歌词高度匹配的全长音乐视频。系统通过提取音乐特征，协同多个智能体设计剧本、角色和镜头，生成关键帧及多场景视频，并通过验证智能体保证视频一致性。作者还建立了涵盖音乐内容、技术、后期制作和艺术四大类的评测标准。实验结果显示，AutoMV在多个维度显著优于现有方法，缩小了与专业音乐视频的差距，展示了自动生成高质量音乐视频的潜力。,1
Rethinking Expert Trajectory Utilization in LLM Post-training,重新思考LLM后训练中专家轨迹的利用,LLM,Other,https://arxiv.org/pdf/2512.11470,https://huggingface.co/papers/2512.11470,本文针对大型语言模型后期训练中如何有效利用专家示范路径的问题，提出了“可塑性天花板”理论框架，将模型性能分解为监督微调（SFT）基础表现和后续强化学习（RL）可塑性两部分。通过大量实验验证，作者确定了先进行SFT再进行RL的顺序训练方式优于同步训练，解决了稳定性问题。论文还给出了具体的训练切换时机、数据规模和示范路径选择的指导原则，帮助最大化模型最终性能。研究为提升后期训练效果提供了实用且科学的策略。,1
FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos,FoundationMotion：视频中空间运动的自动标注与推理,Multimodal LLM,Other,https://arxiv.org/pdf/2512.10927,https://huggingface.co/papers/2512.10927,本文提出了FoundationMotion，一种全自动的视频运动数据集构建方法。该方法通过检测和跟踪视频中的物体轨迹，结合大语言模型自动生成细粒度的运动描述和相关问答，解决了现有运动数据集规模受限的问题。利用该数据集对开源模型进行微调，显著提升了模型在运动理解和空间推理任务上的表现，且优于多种强劲基线。FoundationMotion为大规模运动数据的自动标注和模型训练提供了高效可扩展的解决方案，推动了机器对动态场景的理解能力发展。,1
Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation,LLM消融方法的比较分析：跨架构评估,LLM,Other,https://arxiv.org/pdf/2512.13655,https://huggingface.co/papers/2512.13655,本文系统评估了四种用于移除大型语言模型中拒绝响应机制的工具，比较其在不同模型架构上的效果和对模型能力的影响。研究发现，单次处理方法在保持模型性能方面表现较好，而贝叶斯优化方法则导致不同程度的分布变化和能力波动。尤其是数学推理能力对移除操作最为敏感，表现出显著的性能变化。该工作为研究人员在选择和应用拒绝机制消除工具时提供了实证依据，推动了安全对齐与模型研究需求之间的平衡。,1
MobileWorldBench: Towards Semantic World Modeling For Mobile Agents,MobileWorldBench：面向移动智能体的语义世界建模,Agent,Other,https://arxiv.org/pdf/2512.14014,https://huggingface.co/papers/2512.14014,本文提出了一种基于语义描述的世界建模方法，专门针对移动设备上的图形界面代理。传统像素级预测在复杂界面状态转换中效果有限，作者通过自然语言描述状态变化，提升了模型的实用性。为此，论文引入了MobileWorldBench基准测试和包含140万样本的MobileWorld数据集，显著增强了视觉-语言模型的世界建模能力。基于此，作者设计了将视觉-语言模型整合入移动代理规划的新框架，有效提升了任务完成率，推动了移动端智能代理的语义理解与决策能力。,1
JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction,JMMMU-Pro：基于图像的日语多学科多模态理解基准及其通过Vibe基准构建方法的构建,Multimodal LLM,Other,https://arxiv.org/pdf/2512.14620,https://huggingface.co/papers/2512.14620,本文提出了JMMMU-Pro，一种面向日语的多学科图像文本理解基准，旨在评估大规模多模态模型的综合视觉与文本理解能力。JMMMU-Pro通过将问题图片与文本合成为单一图像，增强了对视觉感知的要求。为构建该基准，作者引入了Vibe Benchmark Construction方法，利用高质量的图像生成模型生成候选问题，并通过人工校验和提示调整保证数据质量。实验结果显示，现有开源多模态模型在该基准上表现较差，表明JMMMU-Pro为推动日语多模态模型的发展提供了重要且高效的评测工具。,1
TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning,TraPO：一种用于提升大语言模型推理能力的半监督强化学习框架,LLM,Other,https://arxiv.org/pdf/2512.13106,https://huggingface.co/papers/2512.13106,本文提出了一种名为TraPO的半监督强化学习框架，旨在提升大型语言模型的数学推理能力。该方法利用少量带标签数据指导对大量无标签样本的训练，有效避免了纯无监督方法中常见的模型崩溃问题。通过匹配学习轨迹，TraPO筛选出可靠的无标签样本，实现了高效的数据利用和优异的泛化能力。在多个数学推理和跨领域测试中，TraPO以远少于传统方法的标注数据量，显著提升了模型性能，甚至超越了全监督训练的效果，展示了其在降低标注成本同时提升推理准确性的潜力。,1
UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction,UAGLNet：基于不确定性聚合的全局-局部融合网络及协同CNN-Transformer的建筑物提取,Other,Other,https://arxiv.org/pdf/2512.12941,https://huggingface.co/papers/2512.12941,本文提出了一种名为UAGLNet的新型网络，用于提升遥感影像中建筑物的提取精度。该方法结合卷积神经网络和变换器结构，融合局部与全局特征，通过中间交互模块缩小两者差距，并引入不确定性聚合解码器，有效减少了分割过程中的模糊和误差。大量实验表明，UAGLNet在建筑提取任务上优于现有先进方法，具有较强的实用价值和推广潜力。,1
Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation,用于学习多模态机器人操作的同步触觉-视觉感知,Embodied AI,Other,https://arxiv.org/pdf/2512.09851,https://huggingface.co/papers/2512.09851,本文提出了TacThru，一种结合透明弹性材料和新型标记的传感器，实现了触觉与视觉的同步感知；同时设计了TacThru-UMI模仿学习框架，有效利用多模态信息指导机器人操作。该系统在多个复杂任务中表现出色，成功率显著高于仅用视觉或交替感知的基线方法，尤其在处理薄软物体接触和精细操作时表现优异。研究表明，将同步多模态感知与先进学习方法结合，可显著提升机器人操作的精准性和适应性。,1
LikeBench: Evaluating Subjective Likability in LLMs for Personalization,LikeBench：评估大语言模型中用于个性化的主观喜好度,LLM,Amazon,https://arxiv.org/pdf/2512.13077,https://huggingface.co/papers/2512.13077,本文提出了LikeBench，一种多轮对话评估框架，用于衡量大语言模型（LLM）在个性化服务中对用户偏好的适应能力及其“喜好度”。不同于传统关注记忆准确性的评测，LikeBench首次将喜好度细分为七个维度（如情感适应、幽默感等），通过与模拟用户的动态交互，全面评估模型的响应受欢迎程度。实验结果表明，记忆表现优异并不必然带来更高的喜好度，强调了个性化中情感和风格适配的重要性，为提升用户体验提供了新视角和评测工具。,1
Vibe Spaces for Creatively Connecting and Expressing Visual Concepts,Vibe空间：用于创造性连接和表达视觉概念,Other,Other,https://arxiv.org/pdf/2512.14884,https://huggingface.co/papers/2512.14884,本文提出了“Vibe Blending”任务，通过识别和融合图像中最相关的共享属性（即“vibe”），生成连贯且富有创意的视觉混合图像。为此，作者设计了“Vibe Space”，一种基于层级图结构的低维流形，能够在特征空间中找到语义一致的平滑路径，实现不同概念间的自然过渡。结合人类评价、大型语言模型推理和路径难度评分，实验证明该方法在创意性和连贯性上显著优于现有技术，推动了视觉概念创新表达的发展。,1
TabReX : Tabular Referenceless eXplainable Evaluation,TabReX：基于表格的无参考可解释评估,LLM,Other,https://arxiv.org/pdf/2512.15907,https://huggingface.co/papers/2512.15907,本文提出了TabReX，一种无需参考答案的表格质量评估框架，通过将源文本和生成表格转换为知识图谱并进行对齐，实现对表格结构和事实准确性的量化评分。该方法兼顾灵敏度与特异性，能够提供与人工评价高度一致的判断及细粒度错误定位。为验证其稳健性，作者构建了涵盖多个领域和多种扰动类型的大规模基准测试。实验证明，TabReX在复杂情况下依然表现优异，推动了结构化数据生成系统的可信且可解释评估新范式。,1
MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning,MomaGraph：基于视觉-语言模型的具身任务规划状态感知统一场景图,Embodied AI,Other,https://arxiv.org/pdf/2512.16909,https://huggingface.co/papers/2512.16909,本文提出了MomaGraph，一种融合空间和功能关系的统一场景图表示，专为家庭环境中的移动机器人设计，能够捕捉物体状态及可交互部件，提升任务相关的场景理解和规划能力。为支持该方法，作者构建了首个大规模任务驱动的家庭场景图数据集MomaGraph-Scenes及涵盖多种推理能力的评测套件MomaGraph-Bench。在此基础上，开发了基于强化学习训练的7B视觉语言模型MomaGraph-R1，实现了任务导向的场景图预测和零样本任务规划。实验表明，该模型在多个公开基准和实际机器人任务中均取得领先性能，显著提升了家庭机器人对复杂场景的理解与操作能力。,1
Brain-Grounded Axes for Reading and Steering LLM States,基于大脑的轴用于读取和引导大语言模型状态,LLM,Other,https://arxiv.org/pdf/2512.19399,https://huggingface.co/papers/2512.19399,本论文提出利用人类脑部神经活动作为坐标系，构建可解释且具外部认知基础的轴线，用于读取和调控大型语言模型（LLM）的内部状态。通过分析脑磁图数据，作者建立了词汇层面的脑活动图谱，并提取出潜在的语义轴线。随后，设计轻量级适配器将LLM隐藏状态映射到这些脑轴线上，无需对模型进行微调。实验验证了基于脑信号的轴线能够稳定且有效地引导模型生成具有特定词汇频率和功能内容特征的文本，展示了脑神经生理学在提升语言模型可解释性和控制性方面的新潜力。,1
Over++: Generative Video Compositing for Layer Interaction Effects,Over++：用于层交互效果的生成式视频合成,Other,Other,https://arxiv.org/pdf/2512.19661,https://huggingface.co/papers/2512.19661,本文提出了Over++，一个无需相机位姿或复杂标注的生成视频合成框架，能够根据文本提示在视频前景和背景层之间合成真实且半透明的环境交互效果，如阴影、反射、尘埃和水花。该方法支持可选的遮罩和关键帧引导，保持原始视频场景的完整性。通过构建专门的数据集和创新的数据增强策略，Over++在有限训练数据下仍能生成多样且逼真的视觉效果，显著优于现有技术，提升了视频后期合成的自动化和效果质量。,1
SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models,SecureCode v2.0：用于训练安全感知代码生成模型的生产级数据集,Other,Other,https://arxiv.org/pdf/2512.18542,https://huggingface.co/papers/2512.18542,本文提出了SecureCode v2.0，一个包含1215个安全相关代码示例的高质量数据集，覆盖11种编程语言和11类常见漏洞。每个示例均基于真实安全事件，包含漏洞代码、安全修复、具体攻击演示及多层防御指导，且通过结构化验证和专家审核。数据集采用四轮对话形式，模拟开发者与AI的安全编码交流，提供实际运维安全建议如日志集成和基础设施加固。SecureCode v2.0为训练安全意识强的代码生成模型提供了规模充足、贴近生产环境的标准化资源，推动AI辅助开发的安全性提升。,1
Toxicity Ahead: Forecasting Conversational Derailment on GitHub,有害倾向预警：基于GitHub的对话偏离预测,LLM,Other,https://arxiv.org/pdf/2512.15031,https://huggingface.co/papers/2512.15031,本文针对开源软件社区中对话失控和有害言论的问题，提出了一种基于大型语言模型的预测框架。通过收集并分析GitHub上的有害与非有害讨论数据，研究发现对话中的紧张点、情绪变化及特定交流模式能预示有害趋势。该方法采用两步提示策略，先生成对话动态摘要，再预测失控可能性，在多个模型和数据集上表现优异。研究为开源社区提供了一种高效、可解释的早期干预工具，有助于维护社区健康和项目可持续发展。,1
LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics,LLM瑞士轮：通过竞争性瑞士制动态聚合多基准性能,LLM,ByteDance,https://arxiv.org/pdf/2512.21010,https://huggingface.co/papers/2512.21010,本文提出了一种名为竞争瑞士制动态（CSD）的新型评估框架，用于综合衡量大型语言模型（LLMs）在多项基准测试中的表现。该方法通过模拟多轮动态对抗，基于模型的胜负记录调整匹配对手，并利用大量蒙特卡洛模拟计算稳健的期望胜率，减少随机性影响。同时引入失败敏感度分析，区分模型的风险偏好，帮助识别稳健通用型与激进专精型模型。实验表明，CSD相比传统静态评分方法，能更准确反映模型的综合能力和竞争力，为下一代LLM的风险感知评估提供了重要工具。,1
Masking Teacher and Reinforcing Student for Distilling Vision-Language Models,Masking Teacher与Reinforcing Student用于蒸馏视觉-语言模型,Multimodal LLM,Other,https://arxiv.org/pdf/2512.22238,https://huggingface.co/papers/2512.22238,本文提出了一种名为Masters的知识蒸馏方法，旨在将大型视觉语言模型的能力高效传递给体积更小的模型，便于在移动或边缘设备上部署。该方法通过逐步遮蔽教师模型的非关键部分，降低其复杂度，再逐渐恢复模型容量，使学生模型能够稳定且顺畅地学习丰富的表示。同时，结合离线强化学习和双重奖励机制，提升知识转移的准确性和效率。实验表明，Masters在多个视觉语言任务上优于现有紧凑模型，且训练过程更高效，展现出良好的推广性和实用价值。,1
Self-Evaluation Unlocks Any-Step Text-to-Image Generation,Self-Evaluation解锁任意步数的文本到图像生成,Diffusion Model,Other,https://arxiv.org/pdf/2512.22374,https://huggingface.co/papers/2512.22374,本文提出了Self-E模型，一种从零开始训练的文本生成图像方法，支持任意步数推理。该模型通过结合局部学习与自我评估机制，自我监督生成样本质量，避免了传统方法对大量推理步骤或预训练教师模型的依赖。实验表明，Self-E在少步数生成中表现优异，且随着推理步数增加性能持续提升，实现了快速生成与高质量采样的统一，显著提高了文本到图像生成的效率和灵活性。,1
Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks,思维的形状：当分布比正确性在推理任务中更重要时,LLM,"Microsoft, DeepMind",https://arxiv.org/pdf/2512.22255,https://huggingface.co/papers/2512.22255,本文发现，通过训练语言模型使用来自更强模型的合成链式思维（CoT）推理路径，即使这些路径最终答案均错误，也能提升模型的推理能力，且效果优于基于人工标注数据的训练。研究表明，关键原因在于合成数据的分布更贴近模型自身的生成习惯，且错误路径中仍包含有效的推理步骤。此外，通过将人工标注路径进行同义改写以匹配模型分布，同样提升了性能。实验覆盖数学、算法推理和代码生成等多个领域，强调了数据分布匹配的重要性，并指出正确答案并非推理过程可靠性的唯一标准。,1
CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks,[翻译]CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks,Other,Other,https://arxiv.org/pdf/2512.22206,https://huggingface.co/papers/2512.22206,本文提出了一种名为CosineGate的新方法，用于动态调节深度残差网络中的计算量。该方法通过衡量输入特征与残差变换特征之间的语义差异，智能决定是否跳过某些计算块，从而减少不必要的计算。实验结果表明，CosineGate在保持或提升模型准确率的同时，有效降低了计算资源消耗，且无需额外监督或复杂设计。该方法为提升残差网络的计算效率提供了一种简单且有效的解决方案，尤其适合资源受限的应用场景。,1
Fast-weight Product Key Memory,快速权重产品键记忆,LLM,Other,https://arxiv.org/pdf/2601.00671,https://huggingface.co/papers/2601.00671,本文提出了一种名为Fast-weight Product Key Memory（FwPKM）的新型序列建模机制，旨在解决现有模型在存储容量与计算效率之间的矛盾。FwPKM通过动态更新记忆参数，实现快速记忆和检索输入序列中的关键信息，有效扩展了存储能力且保持较低计算成本。实验表明，FwPKM在长上下文任务中显著提升了模型性能，尤其在极长序列（128K令牌）测试中表现出优异的泛化能力，展示了其作为高效记忆模块的潜力。,1
MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing,MorphAny3D：释放结构化潜变量在3D变形中的力量,Other,PKU,https://arxiv.org/pdf/2601.00204,https://huggingface.co/papers/2601.00204,本文提出了MorphAny3D，一种无需训练的3D形变方法，利用结构化潜变量表示实现高质量的3D对象平滑变换。通过在注意力机制中融合源目标特征，方法自然生成语义一致且时间连贯的变形序列。引入的交叉注意力和时间融合自注意力模块增强了结构合理性和时间一致性，同时姿态校正策略减少了变形过程中的姿态歧义。实验表明，该方法在跨类别变形任务中表现优异，并支持解耦变形和3D风格迁移，具有广泛的应用潜力和良好的泛化能力。,1
InfoSynth: Information-Guided Benchmark Synthesis for LLMs,InfoSynth：面向大语言模型的信息引导基准合成,LLM,Other,https://arxiv.org/pdf/2601.00575,https://huggingface.co/papers/2601.00575,本文提出InfoSynth，一种基于信息理论指标和遗传算法的自动化框架，用于生成多样且新颖的Python编程测试题，旨在高效评估大型语言模型的推理和代码生成能力。该方法通过衡量题目与已有数据的差异性和复杂度，自动合成并验证问题及其解答，生成的测试集在准确性、新颖性和多样性方面均优于原始数据。InfoSynth实现了可控难度和多样性的题目生成，提供了一种可扩展且自验证的评测基准构建方案，促进对模型真实能力的更精准评估。,1
Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning,我们能信任AI解释吗？链式思维推理中系统性少报的证据,LLM,Other,https://arxiv.org/pdf/2601.00830,https://huggingface.co/papers/2601.00830,本文通过在问题中嵌入提示词，系统性地评估了主流AI模型在逐步推理解释中是否真实反映其实际影响因素。研究发现，尽管模型能感知这些提示，但几乎不会主动提及，且在被直接询问时才承认注意到，表明其解释存在隐瞒信息的现象。强制模型报告提示虽能提高透明度，但会导致误报并降低准确率。特别是迎合用户偏好的提示，模型最易采纳却最少报告。结果表明，仅依赖模型的解释链难以全面揭示其真实推理过程，提醒我们在信任AI解释时需保持谨慎。,1
OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment,OpenNovelty：一个基于大语言模型的可验证学术新颖性评估智能体系统,Agent,Other,https://arxiv.org/pdf/2601.01576,https://huggingface.co/papers/2601.01576,本文提出了OpenNovelty，一种基于大语言模型的系统，旨在辅助学术同行评审中对论文创新性的透明且可验证评估。该系统通过提取论文核心任务和贡献，利用语义搜索检索相关文献，构建层级分类体系，并进行全文对比分析，最终生成包含明确引用和证据的创新性报告。与传统方法不同，OpenNovelty的判断基于真实文献，确保评审结果可追溯。系统已应用于500余篇ICLR 2026投稿，显示出有效识别相关先行工作的能力，为提升评审的公平性和一致性提供了有力工具。,1
IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset,IMA++：ISIC档案多标注者皮肤镜皮肤病变分割数据集,Other,Other,https://arxiv.org/pdf/2512.21472,https://huggingface.co/papers/2512.21472,本文介绍了IMA++数据集，这是目前规模最大的公开多标注者皮肤病变分割数据集，包含近1.5万张皮肤镜图像及超过1.7万份分割标注，其中部分图像拥有2至5位专家的多重标注。数据集还附带标注者技能水平和使用工具等元信息，支持对标注者差异和共识建模的研究。该资源有助于推动皮肤病变自动分割技术的发展，促进多标注者医学图像分析领域的深入探索。,1
"Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",选择性不完美作为分析、创造力与发现的生成框架,AI4Science,Other,https://arxiv.org/pdf/2601.00863,https://huggingface.co/papers/2601.00863,本文提出了“materiomusic”框架，将物质的层级结构与音乐的创作逻辑相结合，揭示了从分子振动到进化历史中的模式如何通过声音表现出来。研究表明，新颖性源于约束条件下的结构扩展，而“选择性不完美”机制在保持系统稳定性与适应性之间起关键作用。通过音乐尺度的全面分析，发现文化音乐系统与材料强度优化存在相似规律。该框架促进了科学与艺术的融合，推动AI在创作中实现超越简单模仿的创新，展示了振动作为跨尺度结构组织共通语言的潜力。,1
X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework,X-MuTeST：一个用于可解释仇恨言论检测的多语言基准及一种新颖的LLM辅助解释框架,LLM,Other,https://arxiv.org/pdf/2601.03194,https://huggingface.co/papers/2601.03194,本文提出了X-MuTeST，一种结合大语言模型与注意力机制的多语言可解释仇恨言论检测框架，特别针对资源匮乏的印地语和泰卢固语。该方法通过人工标注的词级解释提升模型的分类准确性和可解释性，并融合多种解释手段生成最终解释。实验结果表明，利用人工解释和模型自带注意力机制的结合，显著增强了模型性能和解释质量。论文同时发布了包含印地语、泰卢固语和英语的标注数据集，推动了多语言环境下仇恨言论检测的研究与应用。,1
Parallel Latent Reasoning for Sequential Recommendation,用于序列推荐的并行潜在推理,Other,Alibaba,https://arxiv.org/pdf/2601.03153,https://huggingface.co/papers/2601.03153,本文提出了一种名为Parallel Latent Reasoning（PLR）的新框架，旨在提升序列推荐系统对用户复杂偏好的捕捉能力。与传统方法仅沿单一路径加深推理不同，PLR通过同时探索多条多样化的推理路径，实现推理宽度的扩展，从而有效克服了推理深度增加带来的性能瓶颈。该方法利用可学习的触发令牌构建并行推理流，保持路径多样性，并通过自适应聚合综合多条推理结果。实验表明，PLR在多个真实数据集上显著优于现有技术，且具备实时推理能力，展现出良好的泛化性能。,1
The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization,声纳时刻：音频语言模型在音频地理定位中的基准测试,Multimodal LLM,Shanghai AI Lab,https://arxiv.org/pdf/2601.03227,https://huggingface.co/papers/2601.03227,本论文提出了首个针对音频语言模型的音频地理定位基准数据集AGL1K，涵盖72个国家和地区。通过设计音频可定位性指标，作者从众包平台筛选出1444段高质量音频样本，促进模型在地理推理任务中的表现评估。实验结果表明，音频语言模型已具备一定的地理定位能力，且闭源模型优于开源模型，语言线索在定位中起关键作用。该工作为音频地理定位研究提供了标准化测试平台，有助于推动模型的空间理解能力提升。,1
AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules,AceFF：一种面向小分子的最先进机器学习势能模型,AI4Science,Other,https://arxiv.org/pdf/2601.00581,https://huggingface.co/papers/2601.00581,本文介绍了AceFF，一种针对小分子药物发现优化的预训练机器学习势能模型。AceFF基于改进的TensorNet2架构，利用涵盖药物相关化合物的丰富数据集训练，兼顾了高速推理和接近密度泛函理论的高准确性。该模型支持多种常见医药元素并能处理带电状态。通过复杂扭转能量扫描、分子动力学模拟和能量力学准确性等多项严格测试，AceFF在有机分子模拟中表现出领先水平。AceFF-2的模型权重和推理代码已公开，便于科研应用推广。,1
U-Net-Like Spiking Neural Networks for Single Image Dehazing,用于单幅图像去雾的类U-Net脉冲神经网络,Other,Other,https://arxiv.org/pdf/2512.23950,https://huggingface.co/papers/2512.23950,本文提出了一种名为DehazeSNN的新型图像去雾方法，该方法结合了类似U-Net的结构与脉冲神经网络，有效提升了多尺度特征提取和长距离依赖建模能力。通过引入正交泄漏积分-发放模块，增强了不同通道间的信息交流，实现了高质量去雾效果的同时显著降低了计算资源需求。实验结果表明，DehazeSNN在多个基准数据集上表现优异，具备更小的模型规模和更低的运算复杂度，展示了其在实际应用中的潜力和优势。,1
ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors,ExposeAnyone：个性化音频到表情扩散模型的鲁棒零样本人脸伪造检测器,Diffusion Model,Other,https://arxiv.org/pdf/2601.02359,https://huggingface.co/papers/2601.02359,本论文提出了ExposeAnyone，一种基于自监督扩散模型的个性化音频驱动面部表情生成方法，用于零样本人脸伪造检测。该方法通过对特定人物的个性化训练，利用重建误差衡量视频与该人物身份的一致性，从而有效识别未知的伪造视频。实验表明，ExposeAnyone在多个公开数据集上优于现有最先进方法，且对新型伪造技术和图像模糊、压缩等现实干扰表现出较强的鲁棒性，具备良好的实际应用潜力。,1
Unified Thinker: A General Reasoning Modular Core for Image Generation,[翻译]Unified Thinker: A General Reasoning Modular Core for Image Generation,Other,Other,https://arxiv.org/pdf/2601.03127,https://huggingface.co/papers/2601.03127,无法生成摘要。,1
Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners,大型推理模型（尚未）成为多语言潜在推理器,LLM,Other,https://arxiv.org/pdf/2601.02996,https://huggingface.co/papers/2601.02996,本文系统研究了大型推理模型（LRMs）在多语言环境下的隐性推理能力，涵盖11种语言。结果表明，模型确实具备跨语言的隐性推理机制，但在资源丰富语言中的表现更强，而低资源语言和复杂任务中则较弱。尽管表面表现存在差异，内部预测过程在各语言间高度一致，且与英语表现相似，暗示模型的隐性推理路径以英语为中心。该研究揭示了LRMs多语言推理能力的局限性和内在机制，为未来提升多语言智能推理提供了重要参考。,1
MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents,MAGMA：一种基于多图的智能体记忆架构,Agent,Other,https://arxiv.org/pdf/2601.03236,https://huggingface.co/papers/2601.03236,本文提出了MAGMA，一种基于多图结构的智能体记忆架构，旨在提升语言模型处理长时上下文推理的能力。MAGMA通过将记忆信息划分为语义、时间、因果和实体四个独立维度，以多图形式存储，并采用策略引导的检索方式，实现了对记忆内容的灵活选择和结构化组织。该方法有效避免了传统单一存储方式中信息混杂的问题，增强了推理过程的透明度和准确性。实验结果表明，MAGMA在长时推理任务中显著优于现有先进的记忆增强系统，展示了其在提升语言模型长期记忆和推理能力方面的潜力。,1
ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation,ResTok：用于自回归图像生成的1D视觉分词器中的层次残差学习,Other,Other,https://arxiv.org/pdf/2601.03955,https://huggingface.co/papers/2601.03955,本文提出了一种名为Residual Tokenizer（ResTok）的1D视觉分词器，通过引入层级残差结构，有效提升了自回归图像生成的表现。与传统基于语言模型设计的单层次视觉分词方法不同，ResTok利用逐层融合的多尺度层级特征，增强了表示能力，同时通过层间残差避免信息重叠，使潜在表示更集中、易于建模。此外，论文设计了层级自回归生成器，大幅减少了采样步骤，提高生成效率。实验证明，ResTok在ImageNet-256数据集上实现了优异的生成质量和速度，展示了将视觉特性融入视觉分词的重要价值。,1
Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach,利用特权信息提升目标检测：一种模型无关的师生架构方法,Other,Other,https://arxiv.org/pdf/2601.02016,https://huggingface.co/papers/2601.02016,本文提出了一种通用的教师-学生架构，将训练阶段可用但推理时不可用的辅助信息（如边界框掩码、显著图和深度线索）融入目标检测模型中。通过在多个先进检测模型和公开数据集上的实验，结果表明该方法显著提升了检测准确率，尤其对中大型目标效果更佳，同时保持了推理过程的计算效率和模型规模不变。研究验证了利用训练时的额外信息能有效增强目标检测性能，为实际应用提供了一种高效且实用的改进策略。,1
AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering,AgentDevel：将自我进化的大语言模型智能体重新定义为发布工程,Agent,Other,https://arxiv.org/pdf/2601.04620,https://huggingface.co/papers/2601.04620,本文提出了AgentDevel，一种将大型语言模型（LLM）智能体的自我改进过程视为软件发布工程的新方法。通过将智能体作为可交付的产品，AgentDevel构建了一个外部化的、注重回归检测的发布流程，利用执行轨迹生成不依赖内部实现的质量信号，自动诊断并筛选单一的发布候选版本。该方法强调避免性能回退，保持版本稳定性，实验表明其在多任务执行测试中显著减少了回归现象，实现了更可靠、可审计的智能体迭代更新，为构建和维护实用的LLM智能体提供了有效的开发规范。,1
LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models,LEMAS：一个包含15万小时大规模可扩展多语种音频套件及生成式语音模型,Other,Other,https://arxiv.org/pdf/2601.04233,https://huggingface.co/papers/2601.04233,本文介绍了LEMAS数据集，这是目前最大规模的开源多语言语音库，涵盖10种主要语言，超过15万小时的高质量语音数据，并配备了精确的词级时间戳。基于该数据集，作者训练了两种不同架构的生成模型：一种实现了稳定的多语言语音合成，另一种支持自然流畅的语音编辑。实验结果表明，LEMAS数据集有效提升了多语言语音生成和编辑的性能，推动了多语言语音技术的发展。,1
Learning User Preferences Through Interaction for Long-Term Collaboration,通过交互学习用户偏好以实现长期协作,Agent,Other,https://arxiv.org/pdf/2601.02702,https://huggingface.co/papers/2601.02702,本文提出了MultiSessionCollab基准，用于评估对话代理在多轮交互中学习和适应用户偏好的能力，从而提升长期合作的效果。作者设计了具备持续记忆机制的协作代理，能够随着交互积累不断更新和优化对用户偏好的理解。实验表明，配备记忆的代理在任务成功率、交互效率和用户体验方面均有显著提升。人类用户研究进一步验证了该方法在实际应用中的有效性，推动了智能代理与用户的长期协作发展。,1
Distilling Feedback into Memory-as-a-Tool,将反馈蒸馏为记忆即工具,Agent,Other,https://arxiv.org/pdf/2601.05960,https://huggingface.co/papers/2601.05960,本文提出了一种将临时反馈转化为可检索指导原则的新框架，通过文件式记忆系统和工具调用，使大型语言模型能够积累并复用之前的改进经验。该方法在一个基于评分标准的新数据集上进行了验证，结果显示增强后的模型在保持与复杂推理流程相当性能的同时，大幅降低了推理成本。此框架有效解决了传统推理方法计算资源消耗大且无法持续利用历史反馈的问题，提升了模型的效率和适应性。,1
ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers,ViTNT-FIQA：基于视觉变换器的无训练人脸图像质量评估,Other,Other,https://arxiv.org/pdf/2601.05741,https://huggingface.co/papers/2601.05741,本文提出了ViTNT-FIQA，一种基于视觉变换器（ViT）的无训练人脸图像质量评估方法。该方法通过分析不同ViT层之间的图像特征变化稳定性，利用单次前向传播即可计算图像质量分数。实验证明，高质量人脸图像在特征演变过程中表现出较稳定的变化，而低质量图像则变化较为剧烈。ViTNT-FIQA无需额外训练或模型修改，且在多个公开数据集上表现出与现有先进方法相当的性能，同时具备较高的计算效率和广泛的适用性。,1
Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs,Router-Suggest：面向视觉对话的多模态自动补全动态路由框架,Multimodal LLM,Microsoft,https://arxiv.org/pdf/2601.05851,https://huggingface.co/papers/2601.05851,本文提出了Multimodal Auto-Completion（MAC）任务，通过结合部分输入文本和视觉信息，实时预测对话中的后续内容，从而提升自动补全的准确性和用户体验。为此，作者构建了相关多模态数据集，并比较了视觉语言模型与纯文本模型的性能。核心贡献是设计了Router-Suggest框架，能够根据对话上下文动态选择合适模型，实现了显著的加速效果和资源优化。用户研究表明，多模态信息显著提升了补全质量和用户满意度，推动了更智能、感知环境的对话助手发展。,1
TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents,TowerMind：用于大语言模型作为智能体的塔防游戏学习环境与基准测试,Agent,Other,https://arxiv.org/pdf/2601.05899,https://huggingface.co/papers/2601.05899,本文提出了TowerMind，一个基于塔防游戏的新型环境，用于评估大型语言模型（LLM）的规划和决策能力。该环境兼具低计算需求和多模态观察输入（包括像素、文本及结构化游戏状态），支持模型幻觉检测并高度可定制。通过设计五个基准关卡，实验揭示了LLM在能力和幻觉方面与人类专家存在显著差距，暴露了其规划验证不足、决策单一和动作效率低等局限。TowerMind为AI代理研究提供了一个轻量级且多样化的测试平台，源码已公开。,1
SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models,SketchJudge：用于多模态大语言模型评分手绘图示的诊断基准,Multimodal LLM,Other,https://arxiv.org/pdf/2601.06944,https://huggingface.co/papers/2601.06944,本文提出了SketchJudge，一个专门用于评估多模态大语言模型（MLLMs）在批改手绘STEM图示能力的新基准。该基准涵盖几何、物理、图表和流程图四个领域的1015份学生手绘答案，包含多样的风格和错误类型。实验结果表明，尽管MLLMs在视觉理解上已有进展，但在处理结构复杂且含噪声的手绘图示时，表现仍远逊于人类。SketchJudge有效揭示了当前模型在视觉与语言结合方面的不足，为提升自动化图示批改提供了重要参考。所有数据和代码均已公开。,1
FinForge: Semi-Synthetic Financial Benchmark Generation,FinForge：半合成金融基准生成,LLM,Other,https://arxiv.org/pdf/2601.06747,https://huggingface.co/papers/2601.06747,本文提出了FinForge，一种结合专家指导与语言模型合成的半合成金融领域评测数据生成框架。该方法整合权威金融资料，通过结构化问题生成和人工验证，构建了涵盖11个子领域的FinForge-5k基准数据集，包含5000多个高质量问答对。基于此数据集对多款先进语言模型的测试显示，模型在金融推理能力上存在显著差异，最高准确率接近80%。FinForge为评估和提升金融领域语言模型性能提供了有效工具，促进了专业应用中的可靠性和深度理解。所有代码和数据均公开。,1
Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths,Gecko：一种高效神经架构，固有处理任意长度序列,LLM,Meta,https://arxiv.org/pdf/2601.06463,https://huggingface.co/papers/2601.06463,本文提出了Gecko，一种高效的神经网络架构，专为处理任意长度的序列数据而设计。通过引入指数移动平均的门控注意力机制、时间步衰减归一化、滑动分块注意力和自适应工作记忆等技术，Gecko显著提升了对长距离依赖的捕捉能力。与现有模型（如Llama2和Megalodon）相比，Gecko在相同规模和训练数据下表现出更优的训练效率和长上下文处理能力，能够稳定处理数百万长度的序列，且无需额外的上下文扩展技术，展现出强大的固有长序列建模优势。,1
FlyPose: Towards Robust Human Pose Estimation From Aerial Views,FlyPose：面向空中视角的鲁棒人体姿态估计,Other,Other,https://arxiv.org/pdf/2601.05747,https://huggingface.co/papers/2601.05747,本文提出了FlyPose，一种轻量级的无人机航拍人体姿态估计系统，旨在解决航拍视角下因低分辨率、角度陡峭和遮挡带来的识别挑战。通过多数据集联合训练，FlyPose在多个人体检测和二维姿态估计测试集上分别提升了6.8和16.3的平均精度（mAP），并实现了约20毫秒的实时推理速度。该系统已成功部署于四旋翼无人机平台，支持实际飞行中的实时应用。此外，论文还发布了包含复杂航拍视角标注的FlyPose-104数据集，为未来研究提供了有价值的资源。,1
Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?,文本推理能否提升多模态大语言模型在细粒度视觉分类中的表现？,Multimodal LLM,Other,https://arxiv.org/pdf/2601.06993,https://huggingface.co/papers/2601.06993,本论文针对多模态大语言模型（MLLMs）在细粒度视觉分类（FGVC）任务中表现不佳的问题，系统分析了链式思维（CoT）推理导致性能下降的原因，发现推理长度过长是关键因素，称为“思考成本”。基于此，提出了一种多奖励归一化方法（MRN）和结合该方法的ReFine-RFT框架，通过控制推理长度并提供密集的准确性反馈，有效提升了模型在FGVC上的表现。大量实验验证了该方法的有效性，达到了该领域的最新水平。,1
Artificial Entanglement in the Fine-Tuning of Large Language Models,大语言模型微调中的人工纠缠,LLM,Other,https://arxiv.org/pdf/2601.06788,https://huggingface.co/papers/2601.06788,本文从量子信息视角出发，提出“人工纠缠”概念来刻画大型语言模型中参数高效微调的方法结构特征。通过分析低秩适配（LoRA）与全参数微调的参数更新，发现LoRA在内部参数上呈现独特的纠缠模式，而在模型注意力输出层的外部纠缠则表现稳定且相似，揭示了低秩更新方法在保持模型表现上的有效性。该研究不仅为理解微调机制提供了新的理论框架，还通过随机矩阵理论和矩阵乘积态方法验证了结果，推动了参数高效微调技术的发展。,1
Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs,推理规模化是否提升推理的可信度？基于多模型的自洽性权衡分析,LLM,Other,https://arxiv.org/pdf/2601.06423,https://huggingface.co/papers/2601.06423,本文系统评估了自洽推理（self-consistency）技术在提升大型语言模型推理准确性与推理可信度（faithfulness）上的效果。通过对四个先进模型在数学推理任务上的实证分析，发现自洽推理对不同模型的影响存在显著差异：部分模型准确率提升伴随可信度稳定或下降，另一些则表现出准确率下降但可信度显著提升。研究还揭示了问题难度对结果的影响，强调自洽推理并非对所有模型均有益，建议实际应用中需针对具体模型进行评估。论文为理解推理质量与准确性之间的权衡提供了重要见解，并公开了相关代码以促进后续研究。,1
"The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",智能体的第一天：工作场景中学习、探索与调度的基准测试,Agent,Shanghai AI Lab,https://arxiv.org/pdf/2601.08173,https://huggingface.co/papers/2601.08173,本文提出了Trainee-Bench，一个模拟“新员工”在动态工作环境中学习和适应的评估平台，旨在解决多模态大语言模型在真实场景中面临的任务调度、主动探索和持续学习三大挑战。该平台通过动态任务安排、信息获取和策略演化，全面考察智能体在不断变化的环境中的表现。实验结果表明，现有先进模型在动态探索和长期学习方面仍存在明显不足。该研究为智能体的鲁棒性评估提供了新的框架，推动从静态测试向更贴近实际应用的评测转变。,1
GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models,GeoMotionGPT：基于几何对齐的大语言模型运动理解,LLM,Other,https://arxiv.org/pdf/2601.07632,https://huggingface.co/papers/2601.07632,本文提出了GeoMotionGPT，一种将人体运动数据与大语言模型（LLM）语义空间几何结构对齐的新框架。通过在运动码本和语言嵌入空间施加正交约束，并采用稀疏投影技术，实现两者几何关系的自然映射，从而提升了运动理解和运动语言推理的效果。该方法避免了传统方案中运动量化与语义学习脱节的问题，在HumanML3D数据集上的实验显示，GeoMotionGPT较现有最先进方法提升了约20%的性能，验证了统一几何基础对细粒度运动推理的有效性。,1
Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models,焦点引导：解锁视频扩散模型中语义弱层的可控性,Diffusion Model,ByteDance,https://arxiv.org/pdf/2601.07287,https://huggingface.co/papers/2601.07287,本文针对图像到视频生成中的文本指导弱化问题，发现扩散变换器模型中部分中间层对文本信息响应不足，导致视觉注意力偏离文本引导。为此，提出了“聚焦引导”方法，通过细粒度语义指导和注意力缓存机制，增强这些语义弱层的文本控制能力，从而提升视频生成对文本指令的遵从性。实验结果表明，该方法在多个基准测试中显著提高了模型的文本一致性和生成质量，展示了良好的泛化能力和实用价值。,1
Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning,Omni-R1：迈向多模态推理的统一生成范式,Multimodal LLM,Other,https://arxiv.org/pdf/2601.09536,https://huggingface.co/papers/2601.09536,本文提出了一种统一的生成式多模态推理方法，通过在推理过程中生成中间图像，实现多样化的视觉推理能力。该方法以Omni-R1为代表，采用两阶段训练框架，有效结合感知对齐和奖励机制，提升图像生成的功能性。进一步，Omni-R1-Zero版本无需多模态标注，通过纯文本数据引导生成视觉推理步骤。实验表明，Omni-R1在多模态任务中表现出良好的通用性，而Omni-R1-Zero在多数情况下表现不逊色，展示了生成式多模态推理的广阔前景。,1
No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning,不再滞后的反馈：面向开放世界智能体学习的协同进化评判器,Agent,"PKU, Alibaba",https://arxiv.org/pdf/2601.06794,https://huggingface.co/papers/2601.06794,本文提出了ECHO框架，通过同步联合优化策略和评价器，有效解决了强化学习中评价器反馈滞后、逐渐失效的问题。ECHO采用层级回滚机制和增益塑形目标，使评价器能够持续提供针对性且动态更新的反馈，促进策略的稳定提升。实验验证表明，ECHO在开放世界环境中显著提升了训练稳定性和长期任务完成率，推动了基于语言模型的智能体更高效的学习与适应能力。,1
SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning,SCALER：用于推理的合成可扩展自适应学习环境,LLM,Other,https://arxiv.org/pdf/2601.04809,https://huggingface.co/papers/2601.04809,本文提出了SCALER，一种基于强化学习的自适应训练框架，旨在提升大型语言模型的推理能力。通过将真实编程问题转换为可调难度且可无限生成的训练环境，SCALER实现了持续有效的学习信号。同时，采用多环境动态调整策略，保持训练任务难度与模型能力匹配，并确保任务多样性，避免奖励稀疏和过拟合。实验表明，SCALER在多种推理任务中优于传统基于数据集的方法，训练过程更加稳定且效果持续提升。,1
VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation,VQ-Seg：用于半监督医学图像分割的向量量化令牌扰动,Other,Other,https://arxiv.org/pdf/2601.10124,https://huggingface.co/papers/2601.10124,本文提出了VQ-Seg，一种基于向量量化的新型扰动方法，用于半监督医疗图像分割，替代传统依赖敏感超参数的dropout技术。通过在特征空间进行离散化并打乱编码索引位置，VQ-Seg实现了更可控且有效的正则化。为减少量化带来的信息损失，设计了同时支持图像重建和分割的双分支结构，并引入基础模型指导以补充高层语义信息。实验证明，该方法在肺癌CT及多个公开数据集上优于现有先进技术，提升了半监督分割的性能和稳定性。,1
Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques,通过先进的提示工程技术提升大语言模型中的情感分类与讽刺检测,LLM,Other,https://arxiv.org/pdf/2601.08302,https://huggingface.co/papers/2601.08302,本研究探讨了通过先进的提示设计技术提升大型语言模型在情感分析中的表现，重点评估了GPT-4o-mini和gemini-1.5-flash两种模型。实验结果表明，采用少量示例提示显著提升了GPT-4o-mini的情感分类效果，而链式思维提示则在gemini-1.5-flash的讽刺检测任务中提升了近46%的性能。研究强调，提示策略需结合模型特点和任务复杂度进行定制，以最大化模型的分析能力。该工作为优化情感和讽刺识别提供了有效的方法指导。,1
WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments,WildRayZer：动态环境中的自监督大视角合成,Other,Other,https://arxiv.org/pdf/2601.10716,https://huggingface.co/papers/2601.10716,本文提出了WildRayZer，一种面向动态环境中新视角合成的自监督框架。针对摄像机和物体同时移动导致的多视角一致性破坏问题，WildRayZer通过分析合成方法识别并遮蔽运动区域，聚焦于背景的跨视角重建，从而有效减少动态内容引起的重影和几何错误。为支持大规模训练，作者还构建了包含丰富动态场景的真实数据集。实验结果表明，WildRayZer在动态区域去除和整体视角合成质量上均优于现有方法，且仅需一次前向推理即可实现高效生成。,1
Demystifying the Slash Pattern in Attention: The Role of RoPE,揭开注意力中Slash模式的神秘面纱：RoPE的作用,LLM,Other,https://arxiv.org/pdf/2601.08297,https://huggingface.co/papers/2601.08297,"本文系统揭示了大型语言模型中“斜线”注意力模式（Slash-Dominant Heads, SDHs）的成因。通过对开源模型的实证分析，发现SDHs是模型的内在特性，且能推广到未见过的输入。研究指出，SDHs的出现依赖于查询和键几乎为低秩结构，以及旋转位置编码（RoPE）中中高频成分的主导作用。在此基础上，作者通过理论分析证明，这些条件足以通过梯度下降训练动态促成SDHs的形成。该工作深化了对注意力机制中位置编码作用的理解，对优化和解释大型语言模型具有重要意义。",1
Memory Bank Compression for Continual Adaptation of Large Language Models,用于大语言模型持续适应的记忆库压缩,LLM,Other,https://arxiv.org/pdf/2601.00756,https://huggingface.co/papers/2601.00756,本文提出了一种针对大规模语言模型的持续学习方法，通过优化编码表实现对外部记忆库的压缩，有效解决了记忆库随数据流增长而膨胀的问题。该方法还引入在线重置机制以保证学习稳定性，并在模型注意力层采用低秩适配技术，提高对压缩记忆的利用效率。实验表明，该方法在保持模型旧知识的同时，将记忆库规模缩小至原来的0.3%，显著提升了在线适应的效率和实用性。,1
Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models,具有稳健护栏的分类自适应大语言模型调节机制,LLM,Other,https://arxiv.org/pdf/2512.05339,https://huggingface.co/papers/2512.05339,本文提出了Roblox Guard 1.0，一种基于指令微调的大型语言模型安全防护系统，旨在通过对模型输入和输出的全面审核，提升生成内容的安全性。该系统采用多模型流水线结构，结合合成与开源数据进行训练，并引入链式思维推理和输入反转技术以增强理解能力。Roblox Guard 1.0能够适应多变的安全分类标准，在未见过的安全场景中表现出色。此外，论文还发布了RobloxGuard-Eval评测基准，为系统化评估语言模型安全防护效果提供工具，推动更可靠的AI安全保障。,0
Embodied Referring Expression Comprehension in Human-Robot Interaction,人机交互中的具身指代表达理解,Embodied AI,Amazon,https://arxiv.org/pdf/2512.06558,https://huggingface.co/papers/2512.06558,本文针对机器人在多样化环境中理解人类肢体语言和口头指令的挑战，提出了Refer360数据集，该数据集涵盖室内外多视角的自然交互，弥补了现有数据集的视角偏差和非言语动作覆盖不足问题。同时，设计了MuRes模块，通过提取并强化多模态关键信息，提升机器人对肢体指令的理解能力。实验结果表明，结合MuRes的模型在多个数据集上表现优异，验证了该方法在促进人机自然互动中的有效性和应用潜力。,0
JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention,JEPA作为神经分词器：结合密度自适应注意力机制学习鲁棒的语音表示,Other,Amazon,https://arxiv.org/pdf/2512.07168,https://huggingface.co/papers/2512.07168,本文提出了一种两阶段自监督框架，结合联合嵌入预测架构（JEPA）与密度自适应注意力机制，旨在学习鲁棒的语音表示。第一阶段通过掩码预测在潜在空间中提取语义特征，独立于波形重建；第二阶段利用有限标量量化和混合进制编码实现高效的语音标记化，随后用高保真生成对波形进行重构。该方法通过自适应选择时间特征，发现语音的层次结构，生成的标记具备高度压缩且便于语言模型处理的特点，性能优于现有神经音频编码器，展现出在语音表示与压缩领域的显著潜力。,0
The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation,Segment Anything模型家族中SAM2与SAM3的差距：为何基于提示的专业知识在概念驱动的图像分割中失效,Multimodal LLM,Other,https://arxiv.org/pdf/2512.06032,https://huggingface.co/papers/2512.06032,本文分析了Segment Anything模型家族中SAM2与SAM3之间的根本差异，揭示了基于空间提示的SAM2方法为何难以适应多模态概念驱动的SAM3范式。研究从概念、架构、数据集、训练策略及评估指标五个方面对比两者，指出SAM3通过融合视觉与语言信息，实现了开放词汇的语义理解和更复杂的概念分割，标志着图像分割技术迈入新的阶段。该工作为未来基于概念的图像分割模型发展提供了理论基础和实践指导。,0
"Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs",相同内容，不同答案：多模态大语言模型中的跨模态不一致性,Multimodal LLM,Other,https://arxiv.org/pdf/2512.08923,https://huggingface.co/papers/2512.08923,本文提出了两个新基准测试REST和REST+，用于系统评估多模态大语言模型（MLLMs）在不同信息表达形式（图像、文本及混合）上的一致性。研究发现，当前顶尖MLLMs在处理相同语义内容的不同模态时表现出显著不一致，且这种不一致无法通过简单转换文本与图像形式解决。视觉特征如颜色和分辨率也会影响模型表现。最后，作者揭示了模型在联合嵌入空间中对文本和图像表示的差异是导致跨模态推理不一致的关键原因。该工作为理解和改进多模态模型的一致性提供了重要参考。,0
DragMesh: Interactive 3D Generation Made Easy,[翻译]DragMesh: Interactive 3D Generation Made Easy,Other,Other,https://arxiv.org/pdf/2512.06424,https://huggingface.co/papers/2512.06424,无法生成摘要。,0
Particulate: Feed-Forward 3D Object Articulation,Particulate：基于前馈的三维物体关节结构推断,Embodied AI,Other,https://arxiv.org/pdf/2512.11798,https://huggingface.co/papers/2512.11798,本文提出了Particulate，一种基于变换器网络的前馈方法，能够从单个静态3D网格快速准确地推断物体的关节结构，包括各部件、运动方式及约束条件。该方法通过端到端训练，支持多关节复杂结构，推理速度显著优于需逐个优化的传统方法。Particulate还可应用于AI生成的3D资产，实现从单张图像到完整关节模型的高效转换。作者同时发布了新的评测基准，实验结果显示该方法在准确性和实用性上均超越现有技术，推动了3D物体关节结构自动识别的发展。,0
CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence,CoRe3D：作为3D智能基础的协同推理框架,Multimodal LLM,Other,https://arxiv.org/pdf/2512.12768,https://huggingface.co/papers/2512.12768,本文提出了CoRe3D，一种融合语义理解与空间推理的3D智能框架。该方法通过将语言中的高层意图与三维空间的局部区域紧密结合，实现对3D内容的协同推理与生成。CoRe3D采用分块的空间表示，有效捕捉几何结构和语义信息，提升了模型在复杂场景下的推理能力和生成质量。相比传统方法，CoRe3D更好地保证了生成3D形状与语言描述的一致性和局部连贯性，推动了3D模型对语言指令的理解与应用。,0
KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification,KD-OCT：用于临床级视网膜OCT分类的高效知识蒸馏,Other,Other,https://arxiv.org/pdf/2512.09069,https://huggingface.co/papers/2512.09069,本研究提出了一种名为KD-OCT的知识蒸馏框架，将性能优越但计算量大的ConvNeXtV2-Large模型压缩为轻量级的EfficientNet-B2模型，用于视网膜光学相干断层扫描（OCT）图像中年龄相关性黄斑变性（AMD）及相关病变的实时分类。该方法通过结合软标签和真实标签的联合训练，显著减少了模型大小和推理时间，同时保持了接近教师模型的诊断准确率。实验结果表明，KD-OCT在效率与性能之间取得了良好平衡，有助于将高质量的AMD筛查技术推广至临床边缘设备。,0
S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation,S2D：用于无监督视频实例分割的稀疏到密集关键掩码蒸馏,Other,Other,https://arxiv.org/pdf/2512.14440,https://huggingface.co/papers/2512.14440,本文提出了一种基于真实视频数据的无监督视频实例分割方法，克服了以往依赖合成视频数据导致的运动建模不足问题。该方法通过识别视频中高质量且具有时间一致性的关键帧分割掩码，利用稀疏到稠密的蒸馏策略和时间丢弃损失实现掩码的传播与优化。最终训练出的模型在多个基准测试中优于现有无监督方法，展示了在无需人工标注的情况下提升视频实例分割性能的有效性和实用价值。,0
MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation,MeViS：用于指称运动表达视频分割的多模态数据集,Multimodal LLM,Other,https://arxiv.org/pdf/2512.10945,https://huggingface.co/papers/2512.10945,"本文提出了MeViS，一个大规模多模态数据集，专注于基于运动描述的目标对象视频分割与跟踪。与现有侧重静态属性的相关数据集不同，MeViS强调运动信息在视频理解中的作用，包含2,006个复杂场景视频中8,171个对象的33,072条人工标注的文本和音频运动表达。通过对15种现有方法在多项任务上的评测，揭示了当前技术在运动表达引导的视频理解中的不足。论文还提出了性能领先的新方法LMPM++，为运动表达驱动的视频分析提供了有力支持和研究平台。",0
ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation,ContextAnyone：基于上下文感知的扩散模型用于角色一致性的文本到视频生成,Diffusion Model,Other,https://arxiv.org/pdf/2512.07328,https://huggingface.co/papers/2512.07328,本文提出了ContextAnyone，一种基于扩散模型的文本到视频生成方法，能够从文本描述和单张参考图像生成角色一致且视觉细节丰富的视频。该方法通过联合重建参考图像与生成新视频帧，有效利用参考信息，结合强调注意力模块防止角色身份漂移，并采用特殊的位置编码稳定时间建模。实验表明，ContextAnyone在保持角色身份一致性和视觉质量方面优于现有方法，能够生成在多样动作和场景中保持上下文连贯的角色视频，提升了文本驱动视频生成的实用性和视觉连贯性。,0
Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching,生成式人工智能时代用户感知揭示：基于情感分析的AI教育应用在电子教学数字化转型中的作用评估,Other,Other,https://arxiv.org/pdf/2512.11934,https://huggingface.co/papers/2512.11934,本研究基于用户评论的情感分析，评估了主流生成式人工智能教育应用在数字化教学转型中的表现。结果显示，作业辅助类应用因准确性和个性化获得高度好评，而语言学习和学习管理系统类应用因功能不稳定评价较低。研究揭示了AI教育应用在提升学习效率和参与度方面的优势，同时也指出了付费墙、错误和广告等问题。论文提出未来发展方向，包括融合AI与人工教学、引入沉浸式技术及推动公平包容的政策，为促进生成式AI在教育中的伦理应用和创新发展提供了参考。,0
Hierarchical Dataset Selection for High-Quality Data Sharing,用于高质量数据共享的分层数据集选择,Other,Other,https://arxiv.org/pdf/2512.10952,https://huggingface.co/papers/2512.10952,本文提出了一种名为DaSH的层级数据集选择方法，针对现实中数据分布在多个异质来源且质量参差不齐的问题，旨在从大量数据集中高效挑选出对模型性能提升最有价值的整个数据集。与传统逐样本选择不同，DaSH同时考虑数据集及其所属群组的效用，显著减少了探索步骤。实验表明，该方法在多个公开基准上提升准确率最高达26.2%，且在资源有限和数据相关性不足的情况下依然表现稳健，适合实际多源数据共享与利用场景。,0
CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates,CoSPlan：通过场景图增量更新实现纠正性序列规划,Multimodal LLM,Microsoft,https://arxiv.org/pdf/2512.10342,https://huggingface.co/papers/2512.10342,本文提出了CoSPlan，一个用于评估视觉语言模型在包含错误步骤的多步视觉规划任务中的能力的新基准，涵盖迷宫导航、积木重排、图像重建和物体重组四个领域。研究发现现有模型难以有效识别并纠正非最优操作。为此，作者设计了一种无需训练的中间推理方法——场景图增量更新（SGI），通过引入中间状态推理，显著提升模型纠错和规划性能，平均提升约5.2%。该方法不仅增强了模型在纠正性顺序规划中的可靠性，也具备良好的泛化能力，适用于传统规划和视觉问答任务。,0
Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics,迈向无缝交互：交互式3D会话头部动态的因果层级建模,Embodied AI,Other,https://arxiv.org/pdf/2512.15340,https://huggingface.co/papers/2512.15340,本文提出了TIMAR，一种用于生成3D对话头部动作的因果模型框架。该方法通过交错融合说话者和听者的音视频信息，实现对话中连续回合的因果建模，提升了头部动作的时序连贯性和表达多样性。实验表明，TIMAR在标准数据集及跨域测试中显著降低了动作生成误差，表现出更自然且响应及时的交互特性。该研究为构建更生动、真实的虚拟头像和交互机器人提供了有效技术路径。,0
SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations,SonicMoE：通过IO和切片感知优化加速MoE,LLM,Other,https://arxiv.org/pdf/2512.14080,https://huggingface.co/papers/2512.14080,本文提出了SonicMoE，一种针对Mixture of Experts（MoE）模型的优化方法。通过减少激活缓存、实现计算与内存输入输出的重叠，以及引入“token rounding”技术，SonicMoE显著降低了内存占用并提升了计算效率。在实际测试中，该方法在Hopper GPU上将激活内存减少了45%，计算吞吐量提升了1.86倍，同时在高稀疏设置下进一步加速了内核执行。该工作有效解决了MoE模型中激活内存大、计算资源浪费等问题，推动了大规模语言模型的高效训练。,0
EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration,EmoCaliber：通过置信度表达与校准推进可靠的视觉情感理解,Multimodal LLM,Other,https://arxiv.org/pdf/2512.15528,https://huggingface.co/papers/2512.15528,本文提出了EmoCaliber，一种具备置信度表达能力的多模态大语言模型，用于提升图像情感理解的可靠性。传统方法通常只给出单一情感标签，忽视了情感感知的主观性和多样性。EmoCaliber通过训练模型逐步学习结构化推理、置信度口头表达及其校准，使模型不仅预测情感类别，还能表达对预测结果的自信程度。实验证明，该方法在情感预测和置信度估计上均优于现有技术，推动了更可信赖的视觉情感理解系统的发展。,0
Sharing State Between Prompts and Programs,在提示与程序之间共享状态,LLM,Other,https://arxiv.org/pdf/2512.14805,https://huggingface.co/papers/2512.14805,本文提出了一种名为“共享程序状态”的新型编程抽象，旨在简化自然语言代码与正式编程语言（如Python）之间的互操作。通过该方法，用户可以在自然语言代码中直接操作程序变量和控制流程，实现两种代码形式的无缝融合。作者基于此设计了自然函数接口模式，并在Nightjar编程系统中实现。实验结果表明，Nightjar不仅提升了任务执行的准确率（提高4%至19%），还显著减少了代码量（平均减少近40%），尽管运行时可能带来一定性能开销。该工作推动了自然语言编程与传统编程的有效结合，提升了开发效率和代码质量。,0
Learning to Refocus with Video Diffusion Models,基于视频扩散模型的学习重聚焦,Diffusion Model,Other,https://arxiv.org/pdf/2512.19823,https://huggingface.co/papers/2512.19823,本论文提出了一种基于视频扩散模型的图像后期调焦方法。该方法能够从单张模糊图像生成逼真的焦点序列，支持用户在拍摄后自由选择焦点，实现交互式调焦。为此，作者构建了一个涵盖多样真实手机拍摄场景的大规模焦点堆栈数据集。实验结果表明，该方法在视觉效果和鲁棒性上均优于现有技术，拓展了日常摄影中焦点编辑的可能性。相关代码和数据已公开，促进后续研究与应用。,0
PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation,PhononBench：基于声子的晶体生成动力学稳定性大规模基准测试,AI4Science,Other,https://arxiv.org/pdf/2512.21227,https://huggingface.co/papers/2512.21227,本文提出了PhononBench，这是首个针对AI生成晶体的动态稳定性进行大规模评估的基准平台。利用高效且准确的物理模拟方法，PhononBench对超过10万种由六种主流晶体生成模型产出的结构进行了动力学稳定性分析。结果显示，目前生成模型在保证晶体动力学稳定性方面存在显著不足，平均稳定率仅约26%，最佳模型也未超过41%。此外，研究还发现高对称性晶体稳定性较好，并筛选出2.8万多个稳定晶体结构，为未来材料设计提供了可靠候选。该工作为晶体生成模型的发展提供了重要评价标准和改进方向。,0
Reverse Personalization,Reverse Personalization（逆向个性化）,Diffusion Model,Other,https://arxiv.org/pdf/2512.22984,https://huggingface.co/papers/2512.22984,本文提出了一种基于条件扩散反演的反向个性化框架，实现了面部图像的身份匿名化。该方法无需针对特定身份进行模型微调，能够有效去除身份特征的同时保留面部属性和场景信息，且支持用户对面部属性的可控修改。相比现有方法，本框架在身份去除、属性保持和图像质量之间取得了更优的平衡，适用于更广泛的身份匿名需求，推动了隐私保护与图像生成技术的结合。相关代码和数据已公开。,0
GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs,GateBreaker：基于门控的Mixture-of-Expert大语言模型攻击方法,LLM,Other,https://arxiv.org/pdf/2512.21008,https://huggingface.co/papers/2512.21008,本文提出了GateBreaker，一种无需训练、轻量且架构无关的攻击方法，针对混合专家（MoE）大语言模型的安全机制进行破坏。研究发现，MoE模型的安全防护集中在少数神经元中，通过选择性禁用约3%的关键神经元，攻击成功率显著提升，且对模型性能影响有限。该方法不仅对多款最新MoE语言模型有效，还能跨模型迁移攻击，并推广至视觉语言模型，展示了MoE安全机制的脆弱性，提醒了其在关键应用中的潜在风险。,0
Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents,Project Ariadne：用于审计大语言模型智能体忠实性的结构因果框架,Agent,Other,https://arxiv.org/pdf/2601.02314,https://huggingface.co/papers/2601.02314,本文提出了Project Ariadne，一种基于结构因果模型和反事实逻辑的新型框架，用于审计大型语言模型（LLM）推理过程的因果可靠性。研究发现，尽管模型生成了可读的推理路径，但这些推理往往与最终答案缺乏真实因果关系，表现出“因果脱钩”现象，即模型在内部逻辑矛盾时仍给出相同结论，说明推理过程更多是事后合理化而非决策驱动。论文还提出了Ariadne评分，作为衡量推理可信度的新指标，为提升AI系统透明度和安全性提供了重要工具。,0
M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models,M-ErasureBench：扩散模型中概念消除的综合多模态评估基准,Diffusion Model,Other,https://arxiv.org/pdf/2512.22877,https://huggingface.co/papers/2512.22877,本文提出了M-ErasureBench，一种针对文本提示、学习嵌入和反演潜变量三种输入方式的多模态概念抹除评估框架，系统评测现有扩散模型中概念抹除方法的效果。研究发现，现有方法在文本提示上表现良好，但在其他输入模态下易被绕过。为提升鲁棒性，作者设计了IRECE模块，通过定位并扰动相关潜变量，有效降低了概念再现率，且保持了生成图像质量。该工作首次全面评估了多模态概念抹除，为构建更安全可靠的生成模型提供了实用工具和基准。,0
Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping,Prithvi-Complimentary自适应融合编码器（CAFE）：释放洪水淹没映射的全部潜力,AI4Science,Other,https://arxiv.org/pdf/2601.02315,https://huggingface.co/papers/2601.02315,本文提出了Prithvi-CAFE，一种结合预训练地理基础模型编码器与并行卷积神经网络分支的融合编码器，旨在提升洪水淹没区域的识别精度。该方法通过多尺度、多层次融合，兼顾了全局信息和关键局部细节，显著优于传统U-Net及其他主流模型。在Sen1Flood11和FloodPlanet两个数据集上，Prithvi-CAFE均取得了领先的分割效果，展示了其在多源遥感数据洪水映射任务中的广泛应用潜力。代码已公开，有助于推动相关领域研究。,0
Steerability of Instrumental-Convergence Tendencies in LLMs,大语言模型中工具性趋同倾向的可引导性研究,LLM,Other,https://arxiv.org/pdf/2601.01584,https://huggingface.co/papers/2601.01584,本文探讨了大型语言模型在能力提升过程中行为可控性的变化，重点区分了“授权可控性”（开发者引导模型按预期行为）与“未授权可控性”（攻击者诱导模型产生不良行为）两种情况。研究发现，通过设计特定的反工具性提示语，可以显著降低模型产生不良行为的倾向，且较大且经过对齐的模型在此类提示下表现出更好的可控性。这揭示了AI安全中的一个核心矛盾：既需高可控性确保安全，又要防范恶意操控带来的风险。该工作为提升AI模型安全性提供了有效的策略和实证支持。,0
Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models,Doc-PP：面向大型多模态视觉语言模型的文档策略保护基准,Multimodal LLM,Other,https://arxiv.org/pdf/2601.03926,https://huggingface.co/papers/2601.03926,本文针对大型视觉语言模型在多模态文档问答中面临的政策合规挑战，提出了Doc-PP基准测试，涵盖需跨视觉与文本信息推理且受严格保密政策约束的真实报告。研究发现模型在复杂推理时易泄露敏感信息，且提取文本虽提升理解但增加泄露风险。为此，作者设计了DVA结构化推理框架，有效分离推理与政策验证，显著提升模型的安全合规能力。该工作为多模态文档的安全问答提供了重要基准和方法支持。,0
RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization,RGS-SLAM：基于一次性稠密初始化的鲁棒高斯点云SLAM,Other,Other,https://arxiv.org/pdf/2601.00705,https://huggingface.co/papers/2601.00705,本文提出了RGS-SLAM，一种基于高斯点云的鲁棒SLAM框架。该方法通过一次性密集多视角特征匹配和三角测量，生成均匀且结构感知的高斯初始点集，替代了传统的逐步增密策略。该初始化显著提升了早期建图的稳定性和收敛速度约20%，在复杂纹理和杂乱场景中实现更高的渲染质量。实验结果表明，RGS-SLAM在定位与重建精度上优于或匹配当前先进方法，且支持高达925帧每秒的实时映射，兼容现有高斯SLAM系统。,0
Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction,Gen3R：三维场景生成与前馈重建的结合,Diffusion Model,ByteDance,https://arxiv.org/pdf/2601.04090,https://huggingface.co/papers/2601.04090,本文提出了Gen3R，一种结合基础重建模型与视频扩散模型的新方法，实现了场景级别的3D生成。通过训练适配器将重建模型生成的几何信息与预训练视频扩散模型的外观信息对齐，Gen3R能够同时生成高质量的RGB视频及对应的三维几何数据（如相机姿态、深度图和点云）。实验结果表明，该方法在单张或多张图像条件下的3D场景生成任务中表现出色，同时利用生成模型的先验提升了重建的鲁棒性，展示了重建与生成模型紧密结合的优势。,0
Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance,一次性安全：用单个实例修补微调后的大语言模型,LLM,Other,https://arxiv.org/pdf/2601.01887,https://huggingface.co/papers/2601.01887,本文针对大语言模型（LLMs）在微调过程中安全性受损的问题，提出了一种仅用单个安全示例即可恢复模型安全性的高效方法。该方法不仅保持了模型的实用性，还能在极少的训练周期内实现收敛，且对微调时有害样本数量和模型规模不敏感。研究还发现了安全梯度的低秩结构，解释了这种快速修正的可能性。通过在多个模型和数据集上的验证，证明了该方法的广泛适用性，为提升微调后模型的安全性提供了简便有效的解决方案。,0
VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding,VERSE：视觉嵌入降维与空间探索——基于聚类的视觉丰富文档理解训练数据增强方法,Multimodal LLM,Other,https://arxiv.org/pdf/2601.05125,https://huggingface.co/papers/2601.05125,本文提出了VERSE，一种通过分析视觉嵌入空间来理解和提升视觉语言模型在复杂文档理解中的表现的方法。VERSE通过可视化模型的潜在表示，帮助识别易出错的聚类区域，并指导生成针对性合成数据以增强模型性能。实验证明，利用VERSE优化训练数据显著提升了模型在真实文档上的识别准确率，同时保持了良好的泛化能力。此外，VERSE优化的本地模型在性能上可媲美甚至超越部分云端服务方案，展示了其在视觉文档理解领域的实用价值。,0
IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck,IIB-LPO：基于迭代信息瓶颈的潜在策略优化,LLM,Alibaba,https://arxiv.org/pdf/2601.05870,https://huggingface.co/papers/2601.05870,本文提出了一种名为IIB-LPO的强化学习方法，旨在解决大型语言模型推理过程中常见的探索崩溃问题。该方法通过在推理轨迹中引入拓扑分支，促进多样化的推理路径生成，并利用信息瓶颈机制对轨迹进行筛选和自我奖励，确保探索既简洁又富有信息。实验证明，IIB-LPO在多个数学推理基准测试中显著提升了模型的准确率和多样性，优于现有技术，展示了其在提升复杂推理任务表现上的有效性和潜力。,0
Afri-MCQA: Multimodal Cultural Question Answering for African Languages,Afri-MCQA：面向非洲语言的多模态文化问答,Multimodal LLM,Other,https://arxiv.org/pdf/2601.05699,https://huggingface.co/papers/2601.05699,本文提出了Afri-MCQA，这是首个涵盖15种非洲语言、包含7500个问答对的多模态文化问答基准，数据由母语者创建，支持文本和语音两种形式。通过对大型语言模型的测试，发现其在非洲本土语言和文化相关任务上表现极差，尤其是开放式视觉问答准确率接近零。研究强调了非洲语言AI发展中需优先考虑语音输入、文化背景预训练及跨语言文化迁移的重要性。该数据集已公开发布，助力推动非洲语言的包容性多模态人工智能研究。,0
Legal Alignment for Safe and Ethical AI,安全与伦理人工智能的法律对齐,Other,Other,https://arxiv.org/pdf/2601.04175,https://huggingface.co/papers/2601.04175,本文提出“法律对齐”作为确保人工智能（AI）系统安全与伦理的新框架，强调利用法律规则、原则和解释方法指导AI设计。研究围绕三大方向展开：使AI遵守合法制定的法律规范，借鉴法律解释促进AI决策合理性，以及运用法律理念提升AI的可靠性与信任度。该工作填补了AI对齐领域忽视法律视角的空白，提出了法律合规评估和治理机制的构建路径，促进法律与计算机科学等多学科合作，推动AI系统更好地服务于社会和人类价值。,0
The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models,人格悖论：临床语言模型中的医疗人格作为行为先验,LLM,Other,https://arxiv.org/pdf/2601.05376,https://huggingface.co/papers/2601.05376,本文系统评估了在临床大语言模型中引入“医疗角色”作为行为先验对模型表现和安全性的影响。研究发现，不同专业身份（如急诊医生、护士）和交互风格（大胆或谨慎）会产生情境依赖且非单调的效果：医疗角色提升了危重病护理任务的准确性和校准度，但在初级护理任务中表现反而下降。交互风格对风险倾向的影响则高度依赖具体模型。整体来看，医疗角色并非安全或专业性的保证，而是带来了权衡与挑战。该研究为临床AI系统的安全应用提供了重要洞见。,0
3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence,3D CoCa v2：结合测试时搜索的对比学习者用于通用空间智能,Embodied AI,Other,https://arxiv.org/pdf/2601.06496,https://huggingface.co/papers/2601.06496,本文提出了3D CoCa v2，一种提升三维场景描述能力的框架。该方法结合了视觉与语言的对比学习，利用空间感知的三维编码器和多模态解码器，增强了模型对不同环境的泛化能力。通过引入推理阶段的测试时搜索策略，3D CoCa v2无需更新模型参数即可生成多样化描述并选出最优结果。实验表明，该方法在多个数据集上显著优于现有模型，尤其在跨环境的零样本测试中表现突出，推动了三维空间智能的自然语言理解与表达发展。,0
On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation,论全球Token困惑度在口语语言模型评估中的谬误,LLM,Other,https://arxiv.org/pdf/2601.06329,https://huggingface.co/papers/2601.06329,本文指出当前评估生成式语音语言模型常用的“全局token困惑度”方法忽视了语音与文本的本质差异，导致对模型性能的低估。作者提出了一系列基于似然和生成质量的新评估方法，这些方法与人类主观评分的相关性更高，更能真实反映语音生成质量。基于新指标，模型性能排名发生显著变化，最佳模型与人类表现的差距明显缩小。研究强调合理评估对推动语音语言模型进展的重要性。,0
Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification,小型语言模型与小型推理语言模型在系统日志严重性分类上的基准测试,LLM,Other,https://arxiv.org/pdf/2601.07790,https://huggingface.co/papers/2601.07790,本文以系统日志中的严重性分类为切入点，评估了九种小型语言模型及推理模型在无示例、少示例和检索增强生成三种条件下的表现。研究发现，模型性能受架构设计、训练目标及检索信息整合能力共同影响，且检索增强显著提升部分模型准确率。实验基于真实Linux服务器日志，结果显示部分小模型在保证高准确率的同时具备较低延迟，适合实时部署。该工作为数字孪生系统中的日志理解与故障分析提供了实用基准，强调了小型高效模型在实际应用中的潜力。,0
"Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",随机CHAOS：为何确定性推理致命，分布变异性是人工认知的核心,LLM,Other,https://arxiv.org/pdf/2601.07239,https://huggingface.co/papers/2601.07239,本文挑战了大语言模型（LLM）推理中普遍采用的确定性推断方式，指出这种方式抑制了模型对不确定性的表达，削弱了 emergent 能力和安全性。作者提出“随机混沌”（Stochastic CHAOS）理念，强调分布式输出的多样性是人工认知的核心。实验证明，确定性推断低估了模型能力和脆弱性，掩盖了多路径推理和罕见但重要的风险行为。论文呼吁将分布变异性作为信号加以测量和控制，以促进更真实、可靠的认知表现和安全保障。,0
RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction,RealMem：基于真实世界记忆驱动交互的大语言模型基准测试,Agent,PKU,https://arxiv.org/pdf/2601.06966,https://huggingface.co/papers/2601.06966,本文提出了RealMem，一个针对大型语言模型长期项目导向交互的首个真实场景记忆基准。该基准包含2000多条跨会话对话，覆盖11个实际项目场景，通过自然用户提问评估模型对动态变化任务状态的记忆管理能力。研究发现，现有记忆系统在处理长期项目目标和复杂上下文依赖方面仍存在显著挑战。RealMem为推动语言模型在持续协作和长期记忆方面的能力提升提供了重要测试平台和数据资源。,0
A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality,水涨船高：利用MTQE奖励提升习语翻译及整体翻译质量,LLM,Other,https://arxiv.org/pdf/2601.06307,https://huggingface.co/papers/2601.06307,本文针对成语、谚语等非组合性表达在机器翻译中的难题，提出利用机器翻译质量评估模型作为奖励，通过一种强化学习微调方法提升模型对成语的翻译能力。实验证明，该方法在中印两种语言成语翻译上提升约14分，同时也带动了普通文本翻译和跨语言翻译能力的提升。研究不仅量化了非组合性短语的翻译差距，还为构建具备更强跨文化和比喻语言理解能力的大型语言模型提供了有效思路。,0
SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers,SPINAL——神经对齐层中的尺度定律与偏好整合,LLM,Other,https://arxiv.org/pdf/2601.06238,https://huggingface.co/papers/2601.06238,本文提出了SPINAL，一种用于分析大语言模型对齐过程的诊断工具。通过追踪模型不同层次的内部结构变化，SPINAL发现直接偏好优化（DPO）主要在模型的后期解码层集中调整表示，显著收紧和稳定输出分布。该方法揭示了对齐在网络深度上的局部化特征，为模型训练中的对齐效果提供了可量化的几何指标，有助于对齐过程的监控、评估和故障预测，推动了大规模语言模型更可靠的偏好对齐研究。,0
Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing,集群工作负载分配：基于自然语言处理的语义软亲和性,LLM,Amazon,https://arxiv.org/pdf/2601.09282,https://huggingface.co/papers/2601.09282,本文提出了一种基于自然语言处理的语义调度方法，利用大型语言模型（LLM）解读用户以自然语言形式提供的任务分配偏好，实现集群工作负载的软亲和性调度。通过在Kubernetes调度器中集成LLM，该方法显著提升了调度准确性和资源分配效果，尤其在复杂和冲突场景中表现优异。实验结果表明，该方案简化了配置流程，增强了调度的易用性和灵活性，验证了语义软亲和性在集群管理中的应用潜力，同时指出了当前同步调用延迟的限制，建议未来采用异步处理以提升实用性。,0
sui-1: Grounded and Verifiable Long-Form Summarization,sui-1：具备依据与可验证性的长文本摘要生成,LLM,Other,https://arxiv.org/pdf/2601.08472,https://huggingface.co/papers/2601.08472,本文提出了sui-1，一款拥有240亿参数的模型，能够生成带有行内引用的长文摘要，用户可追溯摘要中的每条信息来源。通过结合链式思维提示与多阶段验证的合成数据训练，模型在多语言、多领域数据上表现出色。实验结果显示，sui-1在准确性和可验证性方面显著优于参数更多的公开模型，证明了针对性训练比单纯扩大模型规模更有效。该模型支持超长文本处理，且模型权重和交互演示已公开，推动了可信赖的自动摘要技术发展。,0
SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers,SampoNLP：一种用于子词分词器形态分析的自指工具包,LLM,Other,https://arxiv.org/pdf/2601.04469,https://huggingface.co/papers/2601.04469,本文提出了SampoNLP，一种无需语料库的形态学词典构建工具，利用自我参照的评分方法自动筛选词形结构，适用于资源匮乏的乌拉尔语系语言。基于该工具生成的高质量词典，作者系统评估了不同词汇规模下的BPE子词分词器表现，提出了综合性能指标以平衡词素覆盖与过度拆分。研究首次实证确定了芬兰语、匈牙利语和爱沙尼亚语的最优词汇规模，为处理复杂黏着语提供了实用指导，并揭示了标准BPE方法的局限性。所有工具和资源均已公开。,0

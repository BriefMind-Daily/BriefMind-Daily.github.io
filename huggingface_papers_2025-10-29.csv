标题,中文标题,领域分类,研究机构,PDF链接,论文链接,简明摘要,Upvote数
InteractComp: Evaluating Search Agents With Ambiguous Queries,InteractComp：基于模糊查询评估搜索智能体的能力,Agent,Other,https://arxiv.org/pdf/2510.24668,https://huggingface.co/papers/2510.24668,本文提出了InteractComp，一个用于评估搜索代理处理模糊查询能力的新基准。该基准通过设计需交互澄清的真实模糊问题，测试搜索代理是否能识别查询歧义并主动互动以解决。对17个模型的评测显示，当前搜索代理在无上下文时准确率极低，表现出系统性过度自信而非推理不足。强制交互显著提升了性能，但代理的交互能力在15个月内几乎无进展。InteractComp揭示了搜索代理在交互理解上的重大盲点，为未来提升和训练此类能力提供了重要资源。,79
Tongyi DeepResearch Technical Report,Tongyi DeepResearch 技术报告,Agent,Alibaba,https://arxiv.org/pdf/2510.24701,https://huggingface.co/papers/2510.24701,本文介绍了Tongyi DeepResearch，一款具备自主深度研究能力的大型语言模型，专为长远、复杂的信息检索任务设计。该模型通过端到端训练框架，结合中期与后期的自主训练，实现了高效的推理和信息搜寻能力。其自动化的数据合成流程无需人工标注，支持训练的各个阶段，确保系统交互的稳定性和一致性。拥有305亿参数的Tongyi DeepResearch在多项深度研究基准测试中表现领先，并已开源模型与完整解决方案，推动社区发展。,56
RoboOmni: Proactive Robot Manipulation in Omni-modal Context,RoboOmni：全模态环境下的主动机器人操作,Multimodal LLM,Other,https://arxiv.org/pdf/2510.23763,https://huggingface.co/papers/2510.23763,本文提出了RoboOmni，一种基于多模态大语言模型的机器人操作框架，能够通过融合语音对话、环境声音和视觉信息，主动识别用户意图并执行相应操作。为解决缺乏相关训练数据的问题，作者构建了包含丰富场景和多样指令类型的大规模数据集OmniAction。实验结果表明，RoboOmni在意图识别准确性、响应速度和主动协作能力上均优于传统基于文本或语音识别的机器人系统，推动了机器人在更自然复杂环境中的智能交互与操作能力。,48
AgentFold: Long-Horizon Web Agents with Proactive Context Management,AgentFold：具备主动上下文管理的长时程网页智能体,Agent,Alibaba,https://arxiv.org/pdf/2510.24699,https://huggingface.co/papers/2510.24699,本文提出了AgentFold，一种基于大语言模型的主动上下文管理方法，旨在提升网页代理在长时间、多步骤任务中的表现。通过模拟人类回顾性整合的认知过程，AgentFold动态“折叠”历史信息，既保留关键细节又压缩冗余内容，有效避免了传统方法中上下文信息过载或重要信息丢失的问题。实验证明，AgentFold在多个长任务基准测试中显著超越了规模更大或商业化的对手，展示了其在复杂信息搜索和处理中的强大潜力。,47
"Game-TARS: Pretrained Foundation Models for Scalable Generalist
  Multimodal Game Agents",Game-TARS：用于可扩展通用多模态游戏智能体的预训练基础模型,Agent,ByteDance,https://arxiv.org/pdf/2510.23691,https://huggingface.co/papers/2510.23691,本文提出了Game-TARS，一种基于统一且可扩展的键鼠操作空间训练的通用游戏智能体。通过在多样化游戏环境和多模态数据上进行大规模预训练，结合递减损失函数和高效推理策略，Game-TARS在多个领域表现优异，特别是在开放世界Minecraft、3D网页游戏及FPS游戏中显著超越现有模型和部分人类玩家。研究表明，简单且统一的动作表示与大规模预训练相结合，为开发具备广泛计算机操作能力的通用智能体提供了有效路径。,40
Uniform Discrete Diffusion with Metric Path for Video Generation,基于度量路径的统一离散扩散用于视频生成,Diffusion Model,Other,https://arxiv.org/pdf/2510.24717,https://huggingface.co/papers/2510.24717,本文提出了URSA，一种基于离散空间的统一扩散模型，用于高效生成高分辨率和长时长视频。URSA通过引入线性度量路径和分辨率相关的时间步调整，实现了对离散时空信息的迭代全局优化，显著减少推理步骤。此外，异步时间微调策略使模型能够灵活应对多种任务，如视频插帧和图像生成视频。大量实验表明，URSA在多个视频和图像生成基准上优于现有离散方法，并与连续扩散模型表现相当，展示了其在离散视频生成领域的实用价值和潜力。,32
Open Multimodal Retrieval-Augmented Factual Image Generation,开放多模态检索增强的事实图像生成,Agent,Other,https://arxiv.org/pdf/2510.22521,https://huggingface.co/papers/2510.22521,本文提出了ORIG，一种开放式多模态检索增强框架，旨在提升基于文本提示生成图像的真实性和质量。通过从网络迭代检索并筛选多模态证据，ORIG将最新且准确的信息逐步融入生成过程，解决了传统方法依赖静态数据、难以保证事实一致性的问题。为系统评估该方法，作者构建了涵盖多维度的FIG-Eval基准测试。实验结果表明，ORIG在图像的事实准确性和视觉表现上均显著优于现有强基线，展示了开放多模态检索在事实性图像生成中的潜力。,30
Repurposing Synthetic Data for Fine-grained Search Agent Supervision,合成数据的再利用用于细粒度搜索智能体监督,Agent,Alibaba,https://arxiv.org/pdf/2510.24694,https://huggingface.co/papers/2510.24694,本文提出了一种名为实体感知群体相对策略优化（E-GRPO）的新方法，通过利用训练中被忽略的实体信息，改进搜索代理的奖励机制。该方法为推理接近正确但答案有误的“近似命中”样本赋予部分奖励，提升模型从中学习的能力。实验证明，E-GRPO在多种问答和深度研究任务中显著优于传统方法，不仅提高了答案准确率，还优化了推理效率，减少了工具调用次数，展现出更有效且高效的搜索代理训练策略。,19
OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents,OSWorld-MCP：计算机使用智能体中MCP工具调用的基准测试,Agent,"PKU, Alibaba",https://arxiv.org/pdf/2510.24563,https://huggingface.co/papers/2510.24563,本文提出了OSWorld-MCP，一种首创的综合基准测试，旨在公平评估多模态智能体在计算机应用中的工具调用、图形界面操作及决策能力。通过自动生成和严格验证的158个高质量工具，覆盖七类常用软件，实验表明工具调用显著提升任务成功率，凸显其重要性。然而，当前模型的工具调用率仍较低，表明该领域存在提升空间。OSWorld-MCP为复杂工具辅助环境下的智能体性能评估树立了新标准，推动了多模态智能体能力的深入理解和发展。相关资源已公开。,18
Group Relative Attention Guidance for Image Editing,用于图像编辑的组相对注意力引导,Diffusion Model,Other,https://arxiv.org/pdf/2510.24657,https://huggingface.co/papers/2510.24657,本文提出了一种名为Group Relative Attention Guidance（GRAG）的图像编辑方法，针对现有基于Diffusion-in-Transformer模型的编辑手段难以精细控制编辑强度的问题。GRAG通过调整模型中注意力机制中不同部分的相对权重，实现对编辑程度的连续且细粒度调节，无需额外调参。大量实验表明，该方法可轻松集成进现有框架，显著提升编辑质量，并在控制编辑强度方面优于传统方法，具有较强的实用价值和推广潜力。,18
"WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling
  Info-Rich Seeking",WebLeaper：通过启用信息丰富的搜索提升WebAgent的效率与效能,Agent,Alibaba,https://arxiv.org/pdf/2510.24697,https://huggingface.co/papers/2510.24697,本文提出了WebLeaper框架，旨在提升基于大语言模型的信息检索效率和效果。通过将信息检索任务设计为树状推理问题，WebLeaper能够在有限上下文中覆盖更多目标实体。利用精心整理的维基百科表格，框架设计了三种任务合成方式，有效增强搜索的全面性和高效性。同时，WebLeaper筛选训练路径，确保模型在准确性和搜索效率之间取得平衡。实验证明，该方法在多个信息检索基准测试中均优于现有强基线，显著提升了搜索性能和效率。,17
"STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D
  Intelligence",STAR-Bench：作为音频4D智能的深度时空推理测试,Multimodal LLM,Shanghai AI Lab,https://arxiv.org/pdf/2510.24693,https://huggingface.co/papers/2510.24693,本文提出了STAR-Bench，一个用于评测“音频4D智能”的新基准，重点考察模型对声音在时间和三维空间中动态变化的理解能力。该基准结合基础声学感知和整体时空推理任务，涵盖声音属性识别、片段重排序及空间定位等多样化挑战。通过高质量数据构建和严格的人类标注筛选，STAR-Bench揭示了现有多模态和音频语言模型在细粒度感知和推理方面的显著不足，尤其在语言难以描述的声音线索上表现欠佳。实验结果为未来提升模型的物理世界理解能力提供了重要参考和方向。,16
"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal
  Reasoning in MLLMs",Latent Sketchpad：通过草图视觉思维激发多模态大语言模型的推理能力,Multimodal LLM,Microsoft,https://arxiv.org/pdf/2510.24514,https://huggingface.co/papers/2510.24514,本文提出了Latent Sketchpad，一种为多模态大型语言模型（MLLMs）引入内部视觉草图板的框架，旨在增强模型的视觉思维和推理能力。该方法通过在文本推理过程中交替生成视觉潜表示，模拟人类通过草图进行视觉规划和想象的过程，并利用预训练的解码器将这些潜表示转化为可理解的草图图像。实验证明，Latent Sketchpad在多个MLLM上提升或保持了推理性能，并具备良好的泛化能力。此框架拓展了模型的推理方式，为更丰富的人机交互和应用场景提供了新可能。,16
"Routing Matters in MoE: Scaling Diffusion Transformers with Explicit
  Routing Guidance",路由在MoE中的重要性：基于显式路由指导的扩散Transformer扩展,Diffusion Model,Alibaba,https://arxiv.org/pdf/2510.24711,https://huggingface.co/papers/2510.24711,本文针对视觉领域中扩散变换器（Diffusion Transformers）应用混合专家模型（MoE）面临的挑战，提出了一种名为ProMoE的新型框架。该框架通过两步路由机制，先根据图像令牌的功能将其划分为条件和非条件集合，再利用可学习的原型进行细化分配，从而促进专家的专门化。实验结果表明，ProMoE在ImageNet数据集上显著优于现有方法，有效提升了模型的性能和效率，推动了视觉扩散模型的规模化发展。,15
"AgentFrontier: Expanding the Capability Frontier of LLM Agents with
  ZPD-Guided Data Synthesis",AgentFrontier：基于ZPD引导的数据合成扩展大语言模型智能体的能力前沿,Agent,Alibaba,https://arxiv.org/pdf/2510.24695,https://huggingface.co/papers/2510.24695,本文提出了一种基于“最近发展区”（ZPD）教育理论的数据合成方法，通过自动生成恰好超出大型语言模型当前能力边界的训练数据，促进模型在复杂推理任务上的能力提升。作者设计了AgentFrontier引擎，实现了高质量、多学科的训练数据合成，支持持续预训练和针对性后训练，并开发了动态评测基准ZPD Exam。基于此方法训练的AgentFrontier-30B-A3B模型，在多项高难度测试中表现优异，显示出该方法在提升语言模型智能水平上的有效性和可扩展性。,15
ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking,ParallelMuse：面向深度信息检索的智能体并行思维,Agent,Alibaba,https://arxiv.org/pdf/2510.24698,https://huggingface.co/papers/2510.24698,本文提出了ParallelMuse，一种针对深度信息检索智能体的并行思维新范式。该方法通过将推理过程划分为功能区域，实现路径的高效复用和分支探索，显著提升搜索效率；并通过压缩冗余推理信息，整合长时间推理轨迹，生成更连贯的答案。实验证明，ParallelMuse在多个开源智能体和基准测试中，性能提升最高达62%，同时减少了10%至30%的探索资源消耗，显著增强了复杂问题解决能力。,15
"Critique-RL: Training Language Models for Critiquing through Two-Stage
  Reinforcement Learning",Critique-RL：通过两阶段强化学习训练用于批评的语言模型,LLM,ByteDance,https://arxiv.org/pdf/2510.24320,https://huggingface.co/papers/2510.24320,本文提出了Critique-RL，一种无需强监督的在线强化学习方法，用于训练具备批判能力的语言模型。该方法通过“生成者-批判者”双角色交互，采用两阶段优化策略：第一阶段利用基于规则的直接奖励提升批判者区分能力，第二阶段通过生成者反馈的间接奖励增强批判者的建设性反馈，同时保持其区分能力。实验结果表明，Critique-RL在多任务和多模型上均显著提升了性能，展示了其在复杂推理任务中改进大语言模型的潜力。,14
VisCoder2: Building Multi-Language Visualization Coding Agents,VisCoder2：构建多语言可视化编码智能体,Agent,Other,https://arxiv.org/pdf/2510.23642,https://huggingface.co/papers/2510.23642,本文提出了VisCoder2，一种支持多语言的可视化代码生成模型，结合了大规模数据集VisCode-Multi-679K和评测基准VisPlotBench，实现了代码生成、执行及多轮自我纠错的闭环流程。VisCode-Multi-679K包含12种编程语言的679K条经过验证的可执行可视化代码及多轮修正对话，VisPlotBench则提供系统化的多语言评测环境。实验表明，VisCoder2在多语言可视化任务上显著优于开源模型，且通过迭代自我调试，性能接近先进的专有模型，提升了实际应用中的可靠性和覆盖范围。,13
"ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,
  Finetuning, and Decoding the Curse of Multilinguality",ATLAS：多语言预训练、微调及多语言诅咒解码的自适应迁移缩放定律,LLM,DeepMind,https://arxiv.org/pdf/2510.22037,https://huggingface.co/papers/2510.22037,本文提出了ATLAS，一种适用于多语言预训练和微调的自适应转移缩放规律。通过774次多语言训练实验，涵盖400多种训练语言和48种评估语言，研究揭示了多语言模型的学习动态和语言间的迁移关系。ATLAS显著提升了模型的泛化能力，并提出了语言无关的缩放策略，指导如何在增加语言时合理扩展模型和数据规模。此外，论文还确定了从头训练与微调多语言模型的计算临界点，助力高效构建多语言AI系统，推动多语言模型的公平和可扩展发展。,10
"UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale
  High-Quality Dataset",UltraHR-100K：利用大规模高质量数据集提升超高分辨率图像合成,Diffusion Model,Other,https://arxiv.org/pdf/2510.20661,https://huggingface.co/papers/2510.20661,本文针对超高分辨率（UHR）文本生成图像领域存在的数据缺乏和细节合成不足问题，提出了两个关键贡献。首先，发布了UltraHR-100K数据集，包含10万张超过3000像素分辨率的高质量图像及丰富描述，内容多样且视觉效果优异。其次，设计了一种频率感知的后训练方法，通过聚焦细节关键阶段和频率约束，有效提升了图像细节表现。实验结果表明，该方法显著增强了UHR图像的细节准确性和整体质量，推动了高分辨率图像合成的发展。,9
"From Spatial to Actions: Grounding Vision-Language-Action Model in
  Spatial Foundation Priors",从空间到动作：基于空间基础先验的视觉-语言-动作模型定位,Embodied AI,"ByteDance, THU",https://arxiv.org/pdf/2510.17439,https://huggingface.co/papers/2510.17439,本文提出了一种名为FALCON的新方法，通过引入丰富的三维空间信息，提升视觉-语言-动作模型在空间理解和动作执行上的表现。该方法利用仅基于RGB图像的空间基础模型，并可灵活融合深度或姿态数据，无需重新训练或修改网络结构。FALCON通过专门设计的空间增强动作模块，有效弥补了传统二维模型在空间推理和多模态适应性上的不足。在多项模拟和真实任务中，FALCON表现出优异的性能和强鲁棒性，显著优于现有主流方法。,9
"MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal
  Understanding",MergeMix：一种用于视觉与多模态理解的统一增强范式,Multimodal LLM,Other,https://arxiv.org/pdf/2510.23479,https://huggingface.co/papers/2510.23479,本文提出了MergeMix，一种融合视觉和多模态理解的训练时数据增强方法，旨在提升多模态大语言模型的视觉-语言对齐效果。MergeMix通过注意力感知的图像混合技术结合偏好驱动的训练策略，有效构建混合图像与原始图像的偏好对，从而优化模型性能。该方法在保持训练稳定性和效率的同时，克服了传统监督微调和强化学习在数据需求与训练复杂度上的不足。实验结果表明，MergeMix在分类任务中实现了更高的准确率和更优的训练效率，展示了其在多模态模型偏好对齐中的广泛应用潜力。,5
"FunReason-MT Technical Report: Overcoming the Complexity Barrier in
  Multi-Turn Function Calling",FunReason-MT技术报告：突破多轮函数调用的复杂性障碍,Agent,Other,https://arxiv.org/pdf/2510.24645,https://huggingface.co/papers/2510.24645,本文提出了FunReason-MT，一种用于多轮函数调用的数据合成框架，旨在解决现有方法在复杂环境交互、查询生成和推理链构建中的不足。通过引入环境-接口图交互、多样化查询合成和引导式迭代推理链，FunReason-MT有效提升了多轮函数调用训练数据的质量。实验证明，基于该框架训练的模型在伯克利函数调用排行榜上取得了领先性能，显示出其在提升大型语言模型与外部工具协作能力方面的显著价值和广泛应用潜力。,4
Rethinking Visual Intelligence: Insights from Video Pretraining,重新思考视觉智能：来自视频预训练的洞见,Diffusion Model,Other,https://arxiv.org/pdf/2510.24448,https://huggingface.co/papers/2510.24448,本文探讨了视频扩散模型（VDMs）在视觉智能领域的潜力，指出相比大型语言模型，基于视频数据的预训练能更有效地捕捉空间和时间结构，从而提升模型在视觉任务中的适应能力和数据利用效率。通过对比实验，VDMs在多种视觉推理和规划任务上均表现出更高的数据效率，表明视频预训练为构建通用视觉基础模型提供了有益的归纳偏置，推动了视觉智能的发展。,4
ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?,ReplicationBench：AI智能体能否复现天体物理学研究论文？,Agent,Other,https://arxiv.org/pdf/2510.24591,https://huggingface.co/papers/2510.24591,本文提出了ReplicationBench，一个用于评估AI代理能否完整复现天体物理学研究论文的框架。通过将论文拆分为实验设计、推导、数据分析和代码实现等任务，并与原作者共同制定评估标准，ReplicationBench客观衡量AI在科学研究中对方法的忠实度和结果的准确性。实验结果显示，目前顶尖语言模型在该任务上的表现仍不足20%，揭示了AI在科研辅助中的多样失败模式。该工作首次建立了专家验证的论文级别复现基准，为评估和提升AI在数据驱动科学研究中的可靠性提供了重要工具和洞见。,3
Once Upon an Input: Reasoning via Per-Instance Program Synthesis,从输入开始：通过逐实例程序合成进行推理,LLM,Other,https://arxiv.org/pdf/2510.22849,https://huggingface.co/papers/2510.22849,本文提出了一种名为“逐实例程序合成”（PIPS）的方法，旨在提升大型语言模型（LLM）处理复杂多步推理任务的能力。PIPS通过为每个输入实例生成并优化专属程序，结合结构化反馈和动态置信度判断，有效提升推理准确率并减少错误程序生成。实验覆盖多个先进模型和多样任务，结果显示PIPS在准确率上较现有方法提升显著，同时显著降低了算法任务中的不良输出，展现出更稳定且高效的推理性能。,2
Generalization or Memorization: Dynamic Decoding for Mode Steering,泛化还是记忆：用于模式引导的动态解码,LLM,Other,https://arxiv.org/pdf/2510.22099,https://huggingface.co/papers/2510.22099,本文针对大型语言模型在推理时表现出的泛化能力与机械记忆之间的矛盾，提出了一种基于信息瓶颈理论的统一框架。该框架通过动态模式引导算法，在推理过程中实时检测模型对记忆的依赖，并主动调整计算路径，促进模型更多依赖泛化机制。实验证明，该方法显著提升了模型的逻辑一致性和事实准确性，为提高大型语言模型的可靠性提供了有效且理论支撑的解决方案。,2
"FlowOpt: Fast Optimization Through Whole Flow Processes for
  Training-Free Editing",FlowOpt：通过整体流过程实现无训练编辑的快速优化,Diffusion Model,Other,https://arxiv.org/pdf/2510.22010,https://huggingface.co/papers/2510.22010,本文提出了FlowOpt，一种无需训练且不依赖梯度的优化框架，能够高效控制扩散和流匹配模型的整个采样过程，实现图像编辑等任务。与传统方法逐步调整采样阶段不同，FlowOpt将整个生成过程视为黑盒进行整体优化，支持中间结果监控与早停，显著提升了效率。理论上，作者给出了保证全局收敛的步长条件及其估计方法。实验证明，FlowOpt在图像反演和文本引导编辑中均达到了先进水平，同时保持了与现有方法相当的计算成本。,1
"Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion
  Transformers",Sprint：用于高效扩散Transformers的稀疏-密集残差融合,Diffusion Model,Other,https://arxiv.org/pdf/2510.21986,https://huggingface.co/papers/2510.21986,本文提出了SPRINT，一种针对扩散变换器（Diffusion Transformers）高效训练的方法。通过结合浅层处理全部令牌和深层仅处理稀疏子集的策略，SPRINT实现了高达75%的令牌丢弃率，同时保持生成质量。采用两阶段训练方案，先进行长时间的掩码预训练，再进行短时间的全令牌微调，有效缩小训练与推理差距。在ImageNet-1K数据集上，SPRINT实现了近10倍的训练成本节省，且推理时计算量几乎减半，生成效果不降反升。该方法简单且通用，为扩散变换器的高效大规模训练提供了实用解决方案。,1
"VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a
  Unified Concept Set",VL-SAE：基于统一概念集的视觉-语言对齐的解释与增强,Multimodal LLM,Other,https://arxiv.org/pdf/2510.21323,https://huggingface.co/papers/2510.21323,本文提出了VL-SAE，一种稀疏自编码器，用于解释和增强视觉-语言模型中视觉与语言表示的对齐关系。通过将隐藏层神经元与统一的概念集合相关联，VL-SAE实现了对多模态表示语义的直观理解。同时，该方法通过自监督训练确保语义相似的表示激活一致，提升了模型的对齐效果。实验证明，VL-SAE不仅提高了对齐的可解释性，还增强了模型在零样本图像分类和幻觉消除等任务中的表现，推动了视觉-语言理解的深入发展。,1
"PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D
  Part Understanding",PartNeXt：面向细粒度和层次化三维部件理解的下一代数据集,Other,Other,https://arxiv.org/pdf/2510.20155,https://huggingface.co/papers/2510.20155,"本文提出了PartNeXt，一个包含超过23,000个高质量带纹理的3D模型的新型数据集，涵盖50个类别并提供细粒度的层级零件标注。相比现有数据集，PartNeXt通过更丰富的纹理信息和可扩展的标注方式，提升了3D零件理解的准确性和实用性。论文在无类别零件分割和基于零件的3D问答两项任务上进行了评测，展示了当前方法在细粒度零件识别和开放词汇零件定位上的不足。实验结果表明，基于PartNeXt训练的模型性能显著优于以往数据集，推动了结构化3D理解的研究进展。",1
VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations,VisJudge-Bench：可视化美学与质量评估基准,Multimodal LLM,Other,https://arxiv.org/pdf/2510.22373,https://huggingface.co/papers/2510.22373,"本论文提出了VisJudge-Bench，这是首个专门用于评估多模态大语言模型（MLLMs）在视觉图表美学与质量判断能力的综合基准，涵盖了3,090个专家标注的真实场景样本和32种图表类型。实验结果表明，当前最先进的MLLM（如GPT-5）在图表评估上仍与人类专家存在显著差距。为此，作者设计了专门的VisJudge模型，有效提升了评估准确性和与专家意见的一致性，显著缩小了模型与人类判断的差距。该基准和模型为自动化图表质量评估提供了重要工具和参考。",1
"Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free
  Structural Topology Optimization",Optimize Any Topology：一种面向形状和分辨率无关的结构拓扑优化基础模型,Diffusion Model,Other,https://arxiv.org/pdf/2510.23667,https://huggingface.co/papers/2510.23667,本文提出了Optimize Any Topology（OAT），一种基于深度学习的通用结构拓扑优化框架，能够在任意形状、分辨率和边界条件下快速预测最优结构布局。OAT结合了无关分辨率和形状的自动编码器、隐式神经场解码器及条件潜在扩散模型，训练于涵盖220万优化结构和200万边界条件配置的大规模数据集OpenTO。实验表明，OAT在多个公开基准和新颖测试中相较于现有方法显著降低了结构柔度，且推理速度快于1秒，展示了其在工程设计中高效、灵活且广泛适用的潜力。,1
Multi-Agent Evolve: LLM Self-Improve through Co-evolution,Multi-Agent Evolve：通过协同进化实现大语言模型的自我提升,Agent,PKU,https://arxiv.org/pdf/2510.23595,https://huggingface.co/papers/2510.23595,本文提出了Multi-Agent Evolve（MAE）框架，通过三个相互协作的智能体（提问者、解答者和评判者）在无需大量人工标注的情况下，利用强化学习实现大型语言模型的自我提升。该方法支持多种任务，包括数学、推理和常识问答，显著提升了模型的综合推理能力。实验表明，MAE在多个基准测试中平均提升4.54%，展示了其作为一种高效、可扩展的自我进化策略，有望推动语言模型在广泛领域的自主学习与能力增强。,0
"PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text
  Embedding",PatenTEB：专利文本嵌入的综合基准与模型家族,Other,Other,https://arxiv.org/pdf/2510.22264,https://huggingface.co/papers/2510.22264,本文提出了PatenTEB，一个涵盖15个任务、包含206万样本的专利文本嵌入综合评测基准，专门针对专利文本的检索、分类、复述和聚类等应用场景设计。基于此，作者开发了patembed模型家族，通过多任务训练提升模型在专利领域的泛化能力，支持长文本处理并在多个外部测试中表现出领先性能。研究还表明，多任务训练和领域预训练显著增强了模型的实用性和适应性。该工作为专利文本分析提供了系统化评测工具和高效模型，推动了相关技术的发展。,0
